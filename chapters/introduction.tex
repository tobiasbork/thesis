% Main chapter title
\chapter{Einführung}

% Chapter label
\label{introduction}

\begin{chapquote}{John Naisbitt}
``We are drowning in information but starved for knowledge.''
\end{chapquote}

Wir befinden uns in einem Zeitalter, in welchem Tag für Tag mehr Daten generiert werden. Im Jahr 2019 wurden pro Minute mehr als 500,000 Tweets auf Twitter gesendet, mehr als 250,000 Stories auf Instagram gepostet und ungefähr 4.5 Millionen Suchanfragen bei Google ausgeführt \cite{domo}. Ein Rückgang dieser Entwicklung ist dabei nicht in Sicht. Im Gegenteil wurden mehr als 90\% der Daten auf der Welt in den letzten zwei Jahren (Stand: 2017) erzeugt \cite{ibm}. Das exponentielle Datenwachstum erfordert die Entwicklung und Analyse von modernen Verfahren, welche mit dieser Masse an Daten umgehen können. \textit{Big Data} ist ohne solche Methoden nutzlos.
 
 
%----------------------------------------------------------------------------------------
%	Motivation
%----------------------------------------------------------------------------------------


\subsubsection{Motivation}

Das Objekt von Interesse ist oft in riesigen Datensätzen verborgen bzw. nur von Teilen davon abhängig. In diesem Zusammenhang können Computer helfen, relevante Informationen zu extrahieren. Dies verlangt eine Identifizierung der signifikanten Variablen und das Verständnis deren Zusammenspiels. Im Bereich des maschinellen Lernens ergeben sich häufig zwei Ziele. Einerseits möchte man einen bestehenden Trainingsdatensatz genauer verstehen, strukturieren und vereinfachen. Andererseits interessiert man sich dafür, die erlernten Gesetzmäßigkeiten des Datensatzes auf neue, ungesehene Testdaten zu verallgemeinern. 

Für derartige Aufgaben sind bereits viele effiziente Algorithmen entwickelt worden. Hochdimensionale Datensätze, in welche eine Vielzahl an Variablen einfließen, können solche Methoden aber häufig vor Probleme stellen \cite{donoho_high, donoho_observed}. Dieses Phänomen ist unter dem sogenannten \textit{Fluch der Dimensionen} bekannt, welcher sich in vielen verschiedenen Facetten niederschlägt \cite{bellman}. Ein offensichtliches Problem ist die Visualisierung von Datensätzen mit mehr als drei Dimensionen. Wir Menschen können Zusammenhänge in einem anschaulichen Kontext viel besser verstehen als in einer Tabelle voller Zahlen. Viele Eigenschaften des zwei- oder dreidimensionalen Raums lassen sich nicht auf höherdimensionale Räume übertragen. Im Zuge einer explorativen Datenanalyse kann sich die Projektion in niedrigdimensionale Räume daher als nützlich erweisen. Ein weiterer Gesichtspunkt beschäftigt sich mit der Verallgemeinerung eines Modells auf neue Daten. Ohne vereinfachende Annahmen muss die Anzahl an Beobachtungen exponentiell im Vergleich zu der Anzahl an Dimensionen wachsen, um weiterhin akkurate Vorhersagen zu treffen. In vielen Situationen verfügen wir zwar über eine Vielzahl an Variablen, aber nicht über genügend Beobachtungen, da die Beschaffung unter Umständen hohe Kosten verursachen kann. 


%----------------------------------------------------------------------------------------
%	Dimensionsreduktionsverfahren
%----------------------------------------------------------------------------------------


\subsubsection{Dimensionsreduktionsverfahren}

Um den Fluch der Dimensionen zu umgehen, können Dimensionsreduktionsverfahren hilfreich sein. Sie zielen darauf ab, Daten in eine einfachere Repräsentation zu übersetzen, welche die Struktur trotz drastischer Reduktion möglichst genau wiedergibt. Dadurch können hochdimensionale Datensätze veranschaulicht und für weitere Analysezwecke verwendet werden. Anwendung finden Dimensionsreduktionsverfahren vor allem in der Bildverarbeitung, der Biologie und der Ingenieurwissenschaft. Einige interessante Beispiele beinhalten die Klassifizierung handgeschriebener Zahlen \cite{hastie_elements}, die Gesichtserkennung \cite{hancock} oder die Genexpressionsanalyse \cite{alter}.

Allgemein fallen Dimensionsreduktionsverfahren in zwei verschiedene Klassen, \textit{feature selection} und \textit{feature extraction}. Erstere erreichen eine Reduktion vor allem durch eine Filterung nach relevanten Variablen. Anstatt unwichtige Variablen zu vernachlässigen beabsichtigen feature extraction Verfahren die Ausgangsvariablen geeignet zusammenzufassen. Sowohl die klassische Hauptkomponentenanalyse als auch die dünnbesetzte Variante, welche im Fokus dieser Arbeit steht, gehören zu dieser Klasse.


%----------------------------------------------------------------------------------------
%	Transparenz des Modells
%----------------------------------------------------------------------------------------


\subsubsection{Transparenz des Modells}

Modelle des maschinellen Lernens können trainiert werden, weitreichende und komplexe Aufgaben auszuführen. Jedoch handelt es sich dabei häufig um einen \textit{Black-Box-Ansatz}, d.h. der Nutzer hat wenig bis keine Einsicht darin, wie Entscheidungen mithilfe dieses Modells getroffen werden. Besonders in sicherheitskritischen Anwendungen sollte man aber nicht blind auf gute Ergebnisse vertrauen, sondern in der Lage sein, gefällte Entscheidungen im Detail nachzuvollziehen. Wirklich wertvoll werden Modelle erst dann, wenn wir sie verstehen und erklären können.

Im Laufe dieser Arbeit werden wir feststellen, dass die klassische Hauptkomponentenanalyse ein solches Verständnis nicht ermöglicht. Häufig ist es unklar, was die neu kombinierten Variablen im Kontext repräsentieren. Die Möglichkeit einer Interpretation ist von wesentlicher Bedeutung bei der Verwendung der dünnbesetzten Hauptkomponentenanalyse. In diesem Rahmen kann es für viele Anwendungen von Vorteil sein, einen Teil der Aussagekraft des Modells für mehr Transparenz einzutauschen.


%----------------------------------------------------------------------------------------
%	Aufbau der Arbeit
%----------------------------------------------------------------------------------------


\subsubsection{Aufbau der Arbeit}

Wir möchten einen kurzen Überblick über den Aufbau dieser Arbeit geben. Ein Großteil der Mathematik der klassischen Hauptkomponentenanalyse beruht auf Methoden der linearen Algebra. In Anhang \ref{linear_algebra} stellen wir daher Grundbegriffe, Matrixzerlegungen sowie ausgewählte Approximationsprobleme zum Nachschlagen bereit. Ein in diesem Bereich bewanderter Leser kann mit Kapitel \ref{fundamentals} starten, welches eine umfassende Einführung in generalisierte lineare Modelle gibt. Für den späteren Verlauf der Arbeit ist das Konzept von Straftermen von wesentlicher Bedeutung. In Kapitel \ref{pca} beschäftigen wir uns mit der weitverbreiteten Hauptkomponentenanalyse. Neben der mathematischen Konstruktion ergänzen wir theoretische Aussagen und zeigen Grenzen der Methode in der Praxis auf. An dieser Stelle wird sich ein Kernproblem herausstellen, welches zur Idee der dünnbesetzten Hauptkomponentenanalyse führt. Dieser ist Kapitel \ref{sparse_pca} gewidmet, welches die klassische Variante durch Inspiration von generalisierten linearen Modellen erweitert. Eine naheliegende mathematische Formulierung des Problems wird sich als NP-schwer erweisen, so dass verschiedene Möglichkeiten der Relaxation in der Literatur vorgeschlagen wurden. Im Rahmen dieser Arbeit führen wir einen dieser Ansätze detailliert aus und zeigen, wie ein geeignetes Modell entwickelt werden kann. Anschließend wenden wir uns in Kapitel \ref{implementation} einer numerischen Lösung des Problems zu, in welchem wir zudem eine eigene Implementierung in Python vorstellen. Mithilfe dieser Implementierung werden wir in Kapitel \ref{application} einen praktischen Teil beisteuern und die dünnbesetzte Hauptkomponentenanalyse auf Frequenzdaten anwenden. In diesem Zusammenhang können wir beachtliche Ergebnisse unter Gewährleistung eines transparenten Modells erzielen. Zum Schluss diskutieren wir Stärken wie Schwächen des Modells und geben einen Ausblick auf mögliche Verbesserungen in Kapitel \ref{conclusion}.


%----------------------------------------------------------------------------------------
%	Eigene Beiträge
%----------------------------------------------------------------------------------------


\subsubsection{Eigene Beiträge}

Ein Großteil der Arbeit zur dünnbesetzten Hauptkomponentenanalyse beruht auf dem von Zou und Haste in \cite{zou_sparsepca} eingeführten Ansatz. Er ist in der Literatur der wohl weitverbreiteste und hat den Grundstein für eine Verwendung der Methode in der Praxis gelegt. Wir möchten an dieser Stelle darauf hinweisen, dass während der Entstehung dieser Arbeit einige Kritikpunkte durch eine Veröffentlichung von Camacho et al. \cite{camacho} geäußert wurden. Wir haben Änderungen wie Korrekturen eingearbeitet und werden Unterschiede im Folgendem deutlich machen. Insgesamt haben wir in dieser Arbeit folgende Eigenbeiträge geleistet.
\begin{itemize}
\item Detaillierte Aufarbeitung der mathematischen Grundlagen
\item Erläuterung der Konstruktion der dünnbesetzten Hauptkomponentenanalyse und\\ Einarbeitung von theoretischen Resultaten
\item Eigene, beschleunigte Implementierung des Algorithmus in Python
\item Anwendung des Verfahrens auf Frequenzdaten
\item Empirische Validierung der von Camacho et al. gewonnenen Kenntnisse
\item Vergleich mit anderen Ansätzen sowie Ausblick auf mögliche Verbesserungen
\end{itemize}