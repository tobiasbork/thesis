% Main chapter title
\chapter{Einführung}

\cite{elad}
\cite{foucart}
\cite{hastie_elements}
\cite{gribonval}
\cite{jenatton}
\cite{johnstone}
\cite{yata}
\cite{mairal}
\cite{tibshirani_lasso}
\cite{tibshirani_uniqueness}
\cite{zou_elasticnet}
\cite{zou_sparsepca}
\cite{zou_overview}
\cite{efron_lars}

% Chapter label
\label{introduction}

\section{Motivation}

\section{Dimensionsreduktionsverfahren}

High dimensionality means that the dataset has a large number of features. The primary problem associated with high-dimensionality in the machine learning field is model overfitting, which reduces the ability to generalize beyond the examples in the training set. Richard Bellman described this phenomenon in 1961 as the Curse of Dimensionality where “Many algorithms that work fine in low dimensions become intractable when the input is high-dimensional. “

\section{Sparse Approximations / Representations}

\section{Interpretierbarkeit}

\section{Compressed Sensing Beispiel}