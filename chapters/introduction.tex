% Main chapter title
\chapter{Einführung}

% Chapter label
\label{introduction}

\begin{chapquote}{John Naisbitt}
``We are drowning in information and starving for knowledge.''
\end{chapquote}

Wir befinden uns in einem Zeitalter, in welchem Tag für Tag mehr Daten generiert werden. Im Jahr 2019 senden Twitter Nutzer mehr als 500,000 Tweets pro Minute, Instagram Nutzer posten mehr als 250,000 stories und Tinder Nutzer haben sich 1,4 Millionen Personen angesehen.
90\% of all data has been created in the last two years.
(Source: IBM). Der exponentielle Datenwachstum, den wir in den letzten Jahren erleben, erfordert die Entwicklung und Analyse von modernen Verfahren, die mit einer solchen Masse an Daten umgehen kann. 

In diesem Zusammenhang können Dimensionsreduktionsverfahren 

In 2019, Twitter users send more than 500,000 tweets every minute. (Source: Domo)
Yes, that’s half a million new sets of data in a single minute! In the same time, Instagram users post over 250,000 stories, Twitch users view 1 million videos, and Tinder users swipe 1.4 million times. A quick look at the social media big data statistics shows the rate at which data is being generated by user activity. And this is not slowing down anytime soon.



Machine Learning, unsupervised, supervised Methoden

Zahlen und Fakten zu Big Data

Ab wann sprechen wir eigentlich von Big Data?

"It should be noted, however, that even when one has an apparently massive data set, the effective number of data points for certain cases of interest might be quite small. In fact, data across a variety of domains exhibits a property known as the long tail, shich means that a few things are very common, but most things are quite rare."
(Murphy, Machine Learning)

\section{Motivation}

So ist man meist besonders an der Bildung sog. Cluster, also Gruppierungen, interessiert. Datenpunkte, die im entstehendem Bild nach Anwendung der Hauptkomponentenanalyse nah beieinander liegen, sind in gewisser Weise ähnlich zueinander während Datenpunkte, die weit von einander entfernt liegen, wenig Ähnlichkeit aufweisen. Abbildung CITE zeigt die Entstehung solcher Cluster auf dem Datensatz. Mit diesem Verfahren lässt sich daher eine Art Struktur in den Daten erkennen, die für weitere Analysezwecke ausgenutzt werden kann.

The goals of PCA are to

    (1)

    extract the most important information from the data table;
    (2)

    compress the size of the data set by keeping only this important information;
    (3)

    simplify the description of the data set; and
    (4)

    analyze the structure of the observations and the variables.


\section{Dimensionsreduktionsverfahren}

CURSE OF DIMENSIONALITY (Bellman 1961)

High dimensionality means that the dataset has a large number of features. The primary problem associated with high-dimensionality in the machine learning field is model overfitting, which reduces the ability to generalize beyond the examples in the training set. Richard Bellman described this phenomenon in 1961 as the Curse of Dimensionality where “Many algorithms that work fine in low dimensions become intractable when the input is high-dimensional. “


If you’ve worked with a lot of variables before, you know this can  There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:
Feature Elimination
Feature Extraction

2. Why is Dimensionality Reduction required?
Here are some of the benefits of applying dimensionality reduction to a dataset:

Space required to store the data is reduced as the number of dimensions comes down
Less dimensions lead to less computation/training time
Some algorithms do not perform well when we have a large dimensions. So reducing these dimensions needs to happen for the algorithm to be useful
It takes care of multicollinearity by removing redundant features. For example, you have two variables – ‘time spent on treadmill in minutes’ and ‘calories burnt’. These variables are highly correlated as the more time you spend running on a treadmill, the more calories you will burn. Hence, there is no point in storing both as just one of them does what you require
It helps in visualizing data. As discussed earlier, it is very difficult to visualize data in higher dimensions so reducing our space to 2D or 3D may allow us to plot and observe patterns more clearly

\section{Sparse Approximations / Representations}

\section{Interpretierbarkeit}

In Zeiten der Datenvielfalt erfährt die Interpretation dieser enorme Wichtigkeit.