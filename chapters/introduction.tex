% Main chapter title
\chapter{Einführung}

% Chapter label
\label{introduction}

\begin{chapquote}{John Naisbitt}
``We are drowning in information and starving for knowledge.''
\end{chapquote}

Wir befinden uns in einem Zeitalter, in welchem Tag für Tag mehr Daten generiert werden. Im Jahr 2019 werden pro Minute mehr als 500,000 Tweets auf Twitter gesendet, mehr als 250,000 Stories auf Instagram gepostet und ungefähr 4.5 Millionen Suchanfragen bei Google gestellt \cite{domo}. Ein Rückgang dieser Entwicklung ist dabei nicht in Sicht. Eher im Gegenteil wurden mehr als 90\% der Daten auf der Welt in den letzten zwei Jahren (Stand: 2017) generiert \cite{ibm}. Das exponentielle Datenwachstum erfordert die Entwicklung und Analyse von modernen Verfahren, welche mit dieser Masse an Daten umgehen kann. \textit{Big Data} ist ohne solcher Methoden nutzlos. (Zeitgleich wächst die Begierde nach Wissen, um Bezug auf Zitat zu nehmen?)
 

\section{Motivation}

Oft ist das Objekt von Interesse in riesigen Datensätzen verborgen bzw. nur von Teilen davon abhängig. In diesem Zusammenhang können Computer helfen, die relevanten Informationen zu extrahieren. Dies erfordert eine Identifizierung der signifikanten Variablen und das Verständnis deren Zusammenspiels. Im Bereich des maschinellen Lernens ergeben sich häufig die folgenden beiden Ziele:
\begin{itemize}
\item Verständnis, Strukturierung und Klassifizierung bestehender Daten mithilfe von Trainingsdaten
\item Schlussfolgerungen und Verallgemeinerung der Erkenntnisse auf neue, ungesehene Daten mithilfe von Testdaten
\end{itemize}
Für dieser Art Aufgaben sind bereits viele effiziente Algorithmen entwickelt worden. Hochdimensionale Datensätze können solche Methoden aber häufig vor Probleme stellen. Diese Phänomen ist unter dem dem sog. \textit{Fluch der Dimensionen} bekannt, welcher sich in vielen verschiedenen Facetten niederschlägt \cite{bellman}. Ein offensichtliches Problem ist die Visualisierung von Datensätzen mit mehr als drei Dimensionen. Wir Menschen können Zusammenhänge in einem anschaulichen Kontext viel besser verstehen als in einer Tabelle voller Zahlen.
Darüber hinaus lassen sich viele Eigenschaften des zwei- oder dreidimensionalen Raums nicht auf höherdimensionale Räume übertragen.
Ohne vereinfachende Annahmen wächst die Anzahl an Beobachtungen exponentiell im Vergleich zu der Anzahl an Dimensionen für das Vorhersagen von Funktionen.


\section{Dimensionsreduktionsverfahren}

Bezüglich der genannten Probleme können Dimensionsreduktionsverfahren helfen. Sie zielen darauf ab, beobachtete Daten in eine einfachere Repräsentation zu übersetzen, welche die Struktur trotz drastischer Reduktion möglichst genau wiedergibt. Dadurch können hochdimensionale Datensätze veranschaulicht, vereinfacht und für weitere Analysezwecke verwendet werden.

Anwendung Bildverarbeitung autonomes Fahren, biologischer Kontxt, 

Allgemein fallen Dimensionsreduktionsverfahren in zwei verschiedene Klassen, \textit{feature selection} und \textit{feature extraction}. Erstere erreichen eine Reduktion vor allem durch eine Filterung nach relevanten Variablen. Anstatt unwichtige Variablen einfach zu vernachlässigen beabsichtigen feature extraction Verfahren, die Information durch Kombination und Transformation der Ausgangsvariablen zu erhalten. Sowohl die klassische Hauptkomponentenanalyse als auch die dünnbesetzte Variante, mit welcher wir uns in dieser Arbeit beschäftigen, fallen unter diese Kategorie.


\section{Interpretierbarkeit}

In Zeiten der Datenvielfalt erfährt die Interpretation dieser enorme Wichtigkeit. Explainable AI wird immer wichtiger. Motivation für dünnbesetzte Hauptachsen.

\subsubsection{Eigene Beiträge}
Ein Großteil der Arbeit zur dünnbesetzten Hauptkomponentenanalyse beruht auf dem von Zou und Haste in \ref{zou_sparsepca} eingeführten Ansatz. Er ist in der Literatur der wohl weitverbreiteste und bietet eine solide Grundlage für Weiterentwicklungen. Wir möchten an dieser Stelle darauf hinweisen, dass während der Entstehung dieser Arbeit, eine Veröffentlichung von Camacho et al. einige Kritikpunkte an diesem Ansatz aufgezeigt hat. Uns ist es gelungen, die 
\begin{itemize}
\item Detaillierte Aufarbeitung der mathematischen Grundlagen für die dünnbesetzte Hauptkomponentenanalyse
\item Erläuterung der Konstruktion und Einarbeitung von theoretischen Resultaten
\item eigene, verschnellerte Implementierung des Algorithmus in Python
\item Anwendung des Verfahrens auf Frequenzdaten
\item Empirische Validierung der von Camacho et al. gemachten Kenntnisse
\item Vergleich mit anderen Ansätzen und Ausblick für mögliche Verbesserungen
\end{itemize}

\subsubsection{Aufbau der Arbeit}