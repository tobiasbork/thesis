% Main chapter title
\chapter{Mathematische Grundlagen}

% Chapter label
\label{fundamentals}

\section{Normen und deren Effekte}

\subsection{l0-Norm}
\subsection{l1-Norm}
\subsection{l2-Norm}

\section{Regression}
Lineare Regression (Least Squares)
\subsection{LASSO}
\subsection{Ridge Regression}

\section{Lineare Algebra}

Ein Großteil der Hauptachsentransformation beruht auf Methoden der linearen Algebra. Daher werden wir im Folgendem die wichtigsten Begriffe einführen. Aufgrund des Anwedungsfalls werden wir uns hier auf reelle Vektorräume beschränken.

\subsection{Orthogonalität}
\begin{defn}[Skalarprodukt \ref{jaenich}]
Sei $V$ ein reeller Vektorraum. Ein \textit{Skalarprodukt} in $V$ ist eine Abbildung $\inner{\cdot}{\cdot}: V \times V \longrightarrow \mathbb{R}$ mit den folgenden Eigenschaften:
\begin{enumerate}[(i)]
\item Für jedes $x \in V$ sind die Abbildungen
\begin{align*}
\inner{\cdot}{x}: V & \longrightarrow \mathbb{R} & \inner{x}{\cdot}: V & \longrightarrow \mathbb{R}\\
v & \longmapsto \inner{v}{x} & v & \longmapsto \inner{x}{v}
\end{align*}
linear. $\quad$ (Bilinearität)
\item $\inner{x}{y} = \inner{y}{x}$ für alle  $x,y \in V \quad$ (Symmetrie)
\item $\inner{x}{x} \geq 0$ für alle $x \neq 0 \quad$ (Positive Definitheit)
\end{enumerate}
\end{defn}

Allgemein versteht man unter einem \textit{euklidischer Vektorraum} ein Paar $(V, \inner{\cdot}{\cdot})$, welches aus einem reellem Vektorraum $V$ und einem Skalarprodukt $\inner{\cdot}{\cdot}$ auf $V$ besteht. Die durch das Skalarprodukt induziert Norm für $v \in V$ wird definiert durch:
$$\norm{v} \defeq \sqrt{\inner{v}{v}}$$
Wir werden uns im Folgenden auf das \textit{Standardskalarprodukt} im $\mathbb{R}^n$ beschränken. Dies ist gegeben durch 
$$\inner{x}{y} = x_1y_1 + \cdots + x_ny_n.$$
Die durch das Standardskalarprodukt induzierte Norm, ist die \textit{euklidische Norm} oder $l_2$-Norm, welche wir bereits zuvor gesehen haben.

\begin{defn}[Orthogonalität \ref{jaenich}]
Zwei Elemente $v,w$ eines euklidischen Vektorraums $V$ heißen \textit{orthogonal} (geschrieben $v \perp w$) wenn ihr Skalarprodukt null ist, d.h.
$$v \perp w \iff \inner{v}{w} = 0.$$
Eine Familie $(v_1, \ldots, v_n)$ in $V$ heißt \textit{orthogonal} oder \textit{Orthogonalsystem}, wenn
$$v_i \perp v_j \quad \text{für alle} \quad i \neq j.$$
Gilt zusätzlich $\inner{v_i}{v_i} = 1$ für alle $1 \leq i \leq n$, so spricht man von einem \textit{Orthonormalsystem}.
\end{defn}

\begin{defn}[Orthonormalbasis \ref{fischer}]
Sei $\inner{\cdot}{\cdot}: V \times V \longrightarrow \mathbb{R}$ ein Skalarprodukt. Ein System von Vektoren $(v_1, \ldots, v_n)$ in $V$ wird als \textit{Orthogonalbasis} (bzw. \textit{Orthonormalbasis}) bezeichnet, wenn folgende Bedingungnen erfüllt sind:
\begin{enumerate}[(i)]
\item $(v_1, \ldots, v_n)$ ist eine Basis von $V$
\item $(v_1, \ldots, v_n)$ ist ein Orthogonalsystem (bzw. Orthonormalsystem)
\end{enumerate}
\end{defn}

\begin{thm}[Existenz einer Orthonormalbasis (Fischer Lineare Algebra)]
Jeder endlichdimensionale euklidische Vektorraum besitzt eine Orthonormalbasis.
\end{thm}

\begin{thm}[Verallgemeinerter Satz des Pythagoras \ref{anton}]
\label{pythagoras}
Für orthogonale Vektoren $u,v$ in einem euklidischem Vektorraum $V$ gilt
$$\norm{u+v}^2 = \norm u^2 + \norm v^2.$$
\end{thm}

Der Begriff der Orthogonalität lässt sich auf lineare Abbildungen und somit auf Matrizen übertragen.

\begin{thm}[Orthogonale Abbildung \ref{jaenich}]
Seien $V,W$ euklidische Vektorräume. Eine lineare Abbildung $f: V \longrightarrow W$ heißt \textit{orthogonal} oder \textit{isometrisch}, wenn
$$\inner{f(v)}{f(w)} = \inner{v}{w} \quad \text{für alle } v,w \in V$$
\end{thm}

\begin{defn}[Orthogonale Matrix \ref{anton}]
Eine Matrix $A \in \mathbb{M}(n \times n, \mathbb{R})$ heißt 		\textit{orthogonal}, falls
$$\mat A^T \mat A = \mathbb{1}_n$$
\end{defn}

\begin{thm}[Bosch]
Für eine Matrix $\mat A \in \mathbb{M}(n \times n, \mathbb{R})$ sind die folgenden Bedingungen äquivalent:
\begin{enumerate}[(i)]
\item $\mat A$ ist orthogonal
\item $\mat A^T \mat A = \mathbb{1}_n$
\item $\mat A \mat A^T = \mathbb{1}_n$
\item Die Spalten von $\mat A$ bilden ein Orthonormalsystem
\item Die Zeilen von $\mat A$ bilden ein Orthonormalsystem
\item $\mat A$ ist invertierbar und $\mat{A}^{-1} = \mat A^T$
\end{enumerate}
\end{thm}

\begin{defn}[Orthogonalprojektion (Wikipedia)]
Eine \textit{Orthogonalprojektion} auf einen Untervektorraum $U$ eines Vektorraums $V$ ist eine lineare Abbildung $P_U \colon V \rightarrow V$, die für alle Vektoren $v\in V$ die beiden Eigenschaften
\begin{enumerate}[(i)]
\item $P_U(v) \in U \quad$   (Projektion)
\item $\langle P_U(v) - v , u \rangle = 0$ für alle $u \in U \quad$ (Orthogonalität)
\end{enumerate}
erfüllt.
\end{defn}

Mithilfe einer Orthogonalbasis für $U$ lässt sich aus dieser Definition eine Lösung für die Orthogonalprojektion $P_U(v)$ herleiten.

\begin{thm}[\ref{Anton}]
Ist $(u_1, \ldots, u_n)$ eine Orthogonalbasis von $U$, so gilt für alle $v \in V$
$$P_{U}(v) = \sum_{i=1}^n \frac{\langle v, u_i \rangle}{\langle u_i, u_i \rangle} u_i$$
\end{thm}

In späteren Kapiteln werden wir die Orthogonalprojektion in einer anderen Form nutzen. Wir können die Projektion auch als Matrix-Vektor-Produkt auffassen. Verwenden wir das Standardskalarprodukt so gilt mit einer Orthogonalbasis $(u_1, \ldots, u_n)$ von $U$:
$$P_U(v) = \sum_{i=1}^n \frac{v^T u_i}{u_i^T u_i} u_i = \sum_{i=1}^n \frac{u_i u_i^T}{u_i^T u_i}v = \mat A \mat A^T v$$
wobei die Spalten von $\mat A$ die normalisierten Vektoren der Orthogonalbasis sind, d.h. $\mat A = \left[ \frac{u_1}{\norm{u_1}} \Big\vert \cdots \Big\vert \frac{u_n}{\norm{u_n}}\right]$. 

Mithilfe von \ref{pythagoras} lässt sich zeigen, dass der orthogonal auf den Unterraum projizierte Vektor den Abstand zwischen dem Ausgansvektor und dem Unterraum minimiert.

\begin{thm}[\ref{anton}]
Sei $U$ ein Unterraum eines euklidischen Vektorraums $V$. Dann ist $P_U(v)$ die beste Näherung von $u$ in $U$, d.h.
$$\norm{P_U(v) - v}^2 \leq \norm{u - v}^2 \quad \text{für alle } u \in U$$
\end{thm}


\subsection{Matrixzerlegungen}

\textbf{Eigenwerte, Eigenvektoren}
\textbf{Singulärwerte}

\textbf{Diagonalisierbarkeit}
\begin{thm}[Spektralsatz / Hauptachsentransformation \ref{beutelspacher}]
Jede symmetrische reelle Matrix ist diagonalisierbar und hat nur reelle Eigenwerte.
\end{thm}
\textbf{Eigenwertzerlegung}
\textbf{Singulärwertzerlegung}

\subsection{Matrixnorm und Rang (Eigenschaften?)}
\begin{defn}[Frobeniusnorm]
$$\norm{\mat X}_F$$
\end{defn}

\textbf{Rang}

\begin{defn}[Rang]
Eine Matrix hat Rang k ... wenn
\end{defn}

\begin{thm}[Wörtlich von Wikipedia, Eckart-Young-Theorem]
The unstructured problem with fit measured by the Frobenius norm, i.e.,
$$\text{minimize} \quad \text{over } \widehat D \quad \|D - \widehat D\|_{\text{F}} \quad\text{subject to}\quad \operatorname{rank}\big(\widehat D\big) \leq r $$
has analytic solution in terms of the singular value decomposition of the data matrix. The result is referred to as the matrix approximation lemma or Eckart–Young–Mirsky theorem.[4] Let

$$D = U\Sigma V^{\top} \in \mathbb{R}^{m\times n}, \quad m \leq n$$

be the singular value decomposition of $D$ and partition $U, \Sigma=:\operatorname{diag}(\sigma_1,\ldots,\sigma_m)$, and $V$ as follows:

$$U =: \begin{bmatrix} U_1 & U_2\end{bmatrix}, \quad 
\Sigma =: \begin{bmatrix} \Sigma_1 & 0 \\ 0 & \Sigma_2 \end{bmatrix}, \quad\text{and}\quad 
V =: \begin{bmatrix} V_1 & V_2 \end{bmatrix},$$

where $U_{1}$ is $m\times r$, $\Sigma _{1}$ is $r\times r$, and $V_{1}$ is $n\times r$. Then the rank-$r$ matrix, obtained from the truncated singular value decomposition

$$\widehat D^* = U_1 \Sigma_1 V_1^{\top},$$

is such that

$$\|D-\widehat D^*\|_{\text{F}} = \min_{\operatorname{rank}(\widehat D) \leq r} \|D-\widehat D\|_{\text{F}} = \sqrt{\sigma^2_{r+1} + \cdots + \sigma^2_m}.$$

The minimizer $\widehat D^*$ is unique if and only if $\sigma_{r+1}\neq\sigma_{r}$.
\end{thm}

\section{Signaltheorie}

\subsection{Fouriertransformation}
\subsection{Nyquist-Shannon Abtasttheorem}

\section{Statistik}
Varianz, Erwartungswert
\subsection{Empirische Kovarianzmatrix}

\section{Mannigfaltigkeit}

\section{Dictionary Learning}