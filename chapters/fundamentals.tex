% Main chapter title
\chapter{Mathematische Grundlagen}

% Chapter label
\label{fundamentals}

\section{Normen und deren Effekte}

\subsection{l0-Norm}
\subsection{l1-Norm}
\subsection{l2-Norm}

\section{Regression}
Lineare Regression (Least Squares)
\subsection{LASSO}
\subsection{Ridge Regression}

\section{Lineare Algebra}

\subsection{Orthogonalprojektion}
\begin{defn}
Zwei Vektoren $\vec a$ und $\vec b$ sind genau dann orthogonal, wenn ihr Skalarprodukt null ist, also
$$\vec a \perp \vec b \iff\vec a \cdot \vec b = 0.$$
\end{defn}

Was sind orthogonale, orthonormale Matrizen, orthogonale, orthonormale Basis?
Skalarprodukt?
Von einem Skalarprodukt induzierte Norm?
Projektionsmatrizen?

Allgemeine orthogonale Projektionsmatrix falls keine ONB gegeben ist.
$$\mat P_{\mat A} = \mat A (\mat A^T \mat A)^{-1} \mat A$$

Von Wikipedia:
\begin{defn}
Eine Orthogonalprojektion auf einen Untervektorraum $U$ eines Vektorraums $V$ ist eine lineare Abbildung $P_U \colon V \rightarrow V$, die für alle Vektoren $v\in V$ die beiden Eigenschaften

\begin{itemize}
\item $P_U(v) \in U \quad$   (Projektion)
\item $\langle P_U(v) - v , u \rangle = 0$ für alle $u \in U \quad$ (Orthogonalität)
\end{itemize}
erfüllt.
\end{defn}

Allgemeine orthogonale Projektion auf einen affinen linearen Unterraum.
$$P_{U_0}(v) = r_0 + \sum_{i=1}^k \frac{\langle v - r_0, w_i \rangle}{\langle w_i, w_i \rangle} w_i$$

WÖRTLICH VON WIKIPEDIA:
Der orthogonal projizierte Vektor minimiert den Abstand zwischen dem Ausgangsvektor und allen Vektoren des Untervektorraums bezüglich der von dem Skalarprodukt abgeleiteten Norm $\norm{\cdot}$, denn es gilt mit dem Satz des Pythagoras für Skalarprodukträume

$$\norm{u - v}^2 = \norm{u - P_U(v)}^2 + \norm{P_U(v) - v}^2 \geq \norm{P_U(v) - v}^2$$

\subsection{Matrixzerlegungen}

Diagonalisierbarkeit?

\subsection{Eigenwertzerlegung}
\subsubsection{Eigenwerte, Eigenvektoren}
\subsection{Singulärwertzerlegung}
\subsubsection{Singulärwerte}

\subsection{Matrixnorm}
\begin{defn}[Frobeniusnorm]
$$\norm{\mat X}_F$$
\end{defn}

\subsection{Rang}

\begin{defn}[Rang]
Eine Matrix hat Rang k ... wenn
\end{defn}

\begin{thm}[Wörtlich von Wikipedia, Eckart-Young-Theorem]
The unstructured problem with fit measured by the Frobenius norm, i.e.,
$$\text{minimize} \quad \text{over } \widehat D \quad \|D - \widehat D\|_{\text{F}} \quad\text{subject to}\quad \operatorname{rank}\big(\widehat D\big) \leq r $$
has analytic solution in terms of the singular value decomposition of the data matrix. The result is referred to as the matrix approximation lemma or Eckart–Young–Mirsky theorem.[4] Let

$$D = U\Sigma V^{\top} \in \mathbb{R}^{m\times n}, \quad m \leq n$$

be the singular value decomposition of $D$ and partition $U, \Sigma=:\operatorname{diag}(\sigma_1,\ldots,\sigma_m)$, and $V$ as follows:

$$U =: \begin{bmatrix} U_1 & U_2\end{bmatrix}, \quad 
\Sigma =: \begin{bmatrix} \Sigma_1 & 0 \\ 0 & \Sigma_2 \end{bmatrix}, \quad\text{and}\quad 
V =: \begin{bmatrix} V_1 & V_2 \end{bmatrix},$$

where $U_{1}$ is $m\times r$, $\Sigma _{1}$ is $r\times r$, and $V_{1}$ is $n\times r$. Then the rank-$r$ matrix, obtained from the truncated singular value decomposition

$$\widehat D^* = U_1 \Sigma_1 V_1^{\top},$$

is such that

$$\|D-\widehat D^*\|_{\text{F}} = \min_{\operatorname{rank}(\widehat D) \leq r} \|D-\widehat D\|_{\text{F}} = \sqrt{\sigma^2_{r+1} + \cdots + \sigma^2_m}.$$

The minimizer $\widehat D^*$ is unique if and only if $\sigma_{r+1}\neq\sigma_{r}$.
\end{thm}

\section{Signaltheorie}

\subsection{Fouriertransformation}
\subsection{Nyquist-Shannon Abtasttheorem}

\section{Statistik}
Varianz, Erwartungswert
\subsection{Empirische Kovarianzmatrix}

\section{Mannigfaltigkeit}

\section{Dictionary Learning}