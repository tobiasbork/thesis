% Main chapter title
\chapter{Generalisierte lineare Modelle}

% Chapter label
\label{fundamentals}

In diesem Kapitel werden wir die wichtigsten mathematischen Grundlagen, welche wir im Rahmen dieser Arbeit benötigen, einführen. Ein Großteil dieses Kapitels ist generalisierten linearen Regressionsmodellen gewidmet, welche sowohl eine sehr enge Verbindung zur klassischen als auch zur dünnbesetzten Hauptkomponentenanalyse aufweisen. Für die Einführung generalisierter linearer Modelle sowie die Angabe von Resultaten richten wir uns vor allem nach dem Buch von Hastie et al. \cite{hastie_elements}. Ab Abschnitt \ref{ridge_regression} werden wir die Modelle zu Zwecken der Regularisierung mit verschiedenen \textit{Straftermen} versehen. Anhand eines Beispiel-Datensatzes, welchen wir in Abschnitt \ref{comparison_linear_models} vorstellen, können wir deren Effekte visuell nachzuvollziehen und miteinander vergleichen. Für den weiteren Verlauf dieser Arbeit ist das Verständnis der Strafterme von entscheidender Bedeutung.


%----------------------------------------------------------------------------------------
%	Norm
%----------------------------------------------------------------------------------------


\section{Norm}

\begin{defn}[\cite{hieber}]
\label{norm}
Eine Abbildung $\norm{\cdot} \colon \mathbb{R}^n \longrightarrow {\mathbb {R} }_{0}^{+}$ heißt \textit{Norm}, falls für alle Vektoren $x,y\in \mathbb{R}^n$ und alle Skalare $\alpha \in \mathbb{R}$ die folgenden drei Axiome gelten:
\begin{enumerate}[(i)]
\item \makebox[4cm][l]{$\|x\|=0\;\Leftrightarrow \;x=0$}(Definitheit)
\item \makebox[4cm][l]{$\|\alpha \cdot x\|=|\alpha |\cdot \|x\|$}(Homogenität)
\item \makebox[4cm][l]{$\|x+y\|\leq \|x\|+\|y\|$}(Subadditivität)
\end{enumerate}
\end{defn}

\begin{defn}[$\ell_p$-Norm \cite{schaback}] 
\label{lp_norm}
Auf dem $\mathbb{R}^n$ sind die $l_p$-Normen für $1 \leq p < \infty$ definiert als
$$\norm{x}_p \defeq \left( \sum_{i=1}^{n} \abs{x_i}^{p} \right) ^{\frac{1}{p}} \quad x \in \mathbb{R}^{n}$$
und für $p = \infty$ als
$$\norm{x}_{\infty} \defeq \max_{1 \leq i \leq n} \abs{x_i} \quad x \in \mathbb{R}^{n}.$$
Im Fall $p = \infty$ spricht man auch von der \textit{Maximumsnorm} und im Fall $p = 2$ von der \textit{euklidischen Norm}.
\end{defn}

Um Verwirrung auszuschließen werden wir im Folgenden von der $\ell_q$-Norm sprechen, da wir mit $p$ die Anzahl an Variablen in einem Modell bezeichnen. Eine weitere wichtige Norm, die wir im Zuge dieser Arbeit verwenden werden ist die $\ell_0$-\q{Norm}. Diese zählt die von Null verschiedenen Einträge eines Vektors und misst, ob ein Vektor dünnbesetzt ist.

\begin{defn}[$\ell_0$-\q{Norm} \cite{foucart}]
Die sogenannte $\ell_0$-\q{Norm} ist definiert durch
$$\norm{x}_{0} \defeq \left|\{i \colon \enspace x_i \neq 0 , \quad 1 \leq i \leq n\}\right|.$$
\end{defn}

Die übliche Schreibweise $\norm{x}_0$ - die Notation $\norm{x}_{0}^{0}$ wäre angemessener - entspringt der Beobachtung, dass 
$$\norm{x}_{q}^{q} = \sum_{i=1}^{n} \abs{x_i}^q \quad \underset{q \rightarrow 0}{\longrightarrow} \quad \sum_{i=1}^{n} \mathbf{1}_{\{x_i \neq 0\}} = \left|\{i \colon \enspace x_i \neq 0\, , \quad 1 \leq i \leq n\}\right|$$
Wir wollen betonen, dass die $\ell_0$-\q{Norm} keine wirkliche Norm ist, da die Abbildung nicht homogen ist. Trotzdem ist diese \q{Norm} in der Theorie der komprimierten Erfassung sehr nützlich. Des Weiteren werden wir die Notation $\norm{\cdot}_q$ gemäß der Definition in \ref{lp_norm} auch für Werte $0 < q < 1$ verwenden, obwohl durch diese Abbildung ebenfalls keine Norm gegeben ist.


%----------------------------------------------------------------------------------------
%	Grundlagen aus der Statistik
%----------------------------------------------------------------------------------------


\section{Grundlagen aus der Statistik}

\begin{defn}[Stichprobenkovarianz \cite{rueschendorf}]
Seien $(x_1, y_1), \ldots (x_n, y_n)$ zweidimensionale Daten. Dann ist die \textit{Stichprobenkovarianz} durch
$$s_{x,y} = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x}_n)(y_i - \bar{y}_n)$$
gegeben, wobei $\bar{x}_n = \frac{1}{n}\sum _{i=1}^{n}x_{i}$ das Strichprobenmittel bezeichnet.
\end{defn}

Der Faktor $\frac{1}{n-1}$ wird als \textit{Bessel-Korrektur} bezeichnet und stellt die Erwartungstreue des Schätzers sicher. Im Folgenden werden wir diesen Faktor allerdings ignorieren, da er für die von uns betrachteten Optimierungsprobleme in Kapitel \ref{pca} und \ref{sparse_pca} aufgrund der Normierung irrelevant ist. Für einen Datensatz $\mat X \in \rnp$ mit $p$ zentrierten Variablen werden wir häufig von der Stichprobenkovarianzmatrix $\mat{\Sigma}$ sprechen, welche nach obiger Definition bis auf die Bessel-Korrektur durch $\mat X^T\mat X$ gegeben ist.

Sei $g\colon \Theta \to \mathbb {R}$ eine zu schätzende reelle Parameterfunktion in einem statistischem Modell $(\mathcal{X},\mathcal {A},\mathcal{P})$, wobei $\Theta \subset \mathbb{R}$ eine Parametermenge und $\mathcal{P} = \{P_{\vartheta } \colon \vartheta \in \Theta \}$ ist. Wir werden nun einige wichtige Grundbegriffe für einen Schätzer $d \colon (\mathcal{X}, \mathcal{A}) \rightarrow (\mathbb{R}, \mathcal{B}(\mathbb{R}))$ in $\mathcal{L}^1(\mathcal{P})$ einführen.

\begin{defn}[Verzerrung \cite{rueschendorf}]
Die \textit{Verzerrung} (Englisch: \textit{Bias}) des Schätzers $d$ bei $\vartheta$ ist definiert durch
$$\operatorname {Bias} _{\vartheta }[d] \defeq \operatorname {E} _{\vartheta }[d]-g(\vartheta ).$$
\end{defn}

%\begin{defn}[Erwartungstreuer Schätzer \cite{rueschendorf}]
%Ein Schätzer $d$ heißt \textit{erwartungstreu} für $g$, falls
%$$\operatorname {Bias} _{\vartheta }(d) = 0 \quad \forall \vartheta \in \Theta.$$
%\end{defn}

Um verschiedene Schätzer miteinander zu vergleichen bedienen wir uns häufig des \textit{mittleren quadratischen Fehlers}. Dieser gibt an, welche Abweichung zwischen dem Schätzer und dem wahren Parameter zu erwarten ist. Damit bietet sich uns eine Möglichkeit, den erwarteten Fehler eines Lernalgorithmus zu analysieren.

\begin{defn}[Mittlerer quadratischer Fehler \cite{kohn}]
Der \textit{mittlere quadratische Fehler} (Englisch: \textit{Mean Squared Error (MSE))} ist definiert durch
$$\operatorname {MSE} (d,\vartheta ) \defeq \operatorname{E} _{\vartheta}\left[\left(d-g(\vartheta )\right)^{2}\right].$$
\end{defn}

\begin{thm}[Verschiebungssatz \cite{kohn}]
Der mittlere quadratische Fehler zerfällt in Varianz und Bias, d.h.
$$\operatorname{MSE} (d,\vartheta) = \operatorname{Var}_{\vartheta}[d]+\left(\operatorname{Bias}_{\vartheta}[d]\right)^{2}$$
\end{thm}

Für die Bewertung eines Schätzers ist also sowohl Verzerrung als auch Varianz zu berücksichtigen. Leider ist in der Praxis selten möglich, beide Fehlerquellen zeitgleich zu minimieren. Im Bereich des überwachten maschinellen Lernens ist das Problem unter dem \textit{Verzerrung-Varianz-Dilemma} (Englisch: \textit{bias-variance tradeoff}) bekannt. Idealerweise versucht man ein Modell zu wählen, welches sowohl die Gesetzmäßigkeiten in den Trainingsdaten genau erfasst, als sich auch auf ungesehene Testdaten generalisieren lässt.
Aufgrund von falschen Annahmen kann es bei einem Lernalgorithmus jedoch zu einer hohen Verzerrung kommen. Beziehungen zwischen Eingabe und Ausgabe können nicht geeignet modelliert werden und es kommt zu einem Fehler zwischen System und Modell. Man spricht in diesem Fall von einer \textit{Unteranpassung} (Englisch: \textit{underfitting}).
Demgegenüber sind Modelle mit hoher Varianz meist komplexer und ermöglichen eine präzise Darstellung der Trainingsdaten. Dadurch läuft man aber Gefahr, sich dem Rauschen der Daten anzupassen und nicht die Gesetzmäßigkeiten der Trainingsdaten zu erkennen. Wir bezeichnen dieses Phänomen als Überanpassung (Englisch: \textit{overfitting}), was in ungenauen Vorhersagen auf Testdaten münden kann. 

Die Frage nach einem günstigen Modell liegt also in der Modellkomplexität und es gilt eine Balance zwischen den beiden beschriebenen Extrema zu finden. Diese Idee haben wir in Abbildung \ref{bias_variance_tradeoff} veranschaulicht.

\begin{figure}
\centering
\includegraphics[width = 0.9\textwidth]{figures/bias_variance_tradeoff_labeled.jpg}
\caption{Das Verzerrung-Varianz-Dilemma beschreibt den Kompromiss zwischen Unter- und Überanpassung an den Trainingsdatensatz, welchen man für ein robustes Modell eingehen muss.}
\label{bias_variance_tradeoff}
\end{figure}


%----------------------------------------------------------------------------------------
%	Lineare Regression
%----------------------------------------------------------------------------------------


\section{Lineare Regression}

Bei der Regressionsanalyse werden Zusammenhänge zwischen
mehreren Merkmalen untersucht. Man versucht eine unabhängige Variable $Y$ durch eine oder mehrere abhängige Variablen $X_1, \ldots, X_p$ zu erklären. Ein lineares Regressionsmodell hat also die Form
\begin{align}
\label{linear_model}
f(X) = \beta_0 + \sum_{j=1}^p X_j\beta_j
\end{align}
wobei $\beta_j$ die Regressionskoeffizienten sind. Bei der Verwendung dieses Modells nehmen wir an, dass die Regressionsfunktion $\text{E}(Y|X)$ linear ist bzw. ein lineares Modell eine geeignete Approximation ist \cite{hastie_elements}.

Typischerweise verfügen wir über eine Menge von Trainingsdaten $(x_1, y_1), \ldots, (x_n, y_n)$. Jedes $x_i = (x_{i1}, \ldots, x_{ip})^{\top}$ stellt eine Beobachtung dar, die wir für die Schätzung der Parameter $\beta_j$ benutzen. Die bekannteste Methode für diesen Zweck ist sicherlich die \textit{Methode der kleinsten Quadrate} (Englisch: \textit{(Ordinary) Least Squares}), in welcher wir $\beta = (\beta_0, \ldots, \beta_p)^{\top}$ wählen, dass die Summe der Residuenquadrate (Englisch: \textit{Residual Sum of Squares (RSS)}) minimiert wird. Wir definieren
\begin{align}
\label{RSS}
\text{RSS}(\beta) & = \sum_{i=1}^{n}(y_i - f(x_i))^2\\
& = \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2\nonumber\\
& = (y - \mat X \beta)^{\top}(y-\mat X \beta)\nonumber\\
& = \norm{y - \mat X \beta}_{2}^{2}\nonumber
\end{align}
und das dazugehörige Minimierungsproblem
\begin{align}
\label{least_squares}
\hat{\beta}^{\text{OLS}} = \argmin_{\beta} \text{RSS}(\beta)
\end{align}
wobei $\mat X \in \mathbb{R}^{n \times (p+1)}$ die Matrix der $x_i$ mit einer 1 an erster Stelle ist und $y = (y_1, \ldots, y_n)^{\top}$. An dieser Stelle möchten wir erwähnen, dass bei Verwendung dieser Methode keine Aussage über die Gültigkeit des Modells getroffen, sondern lediglich die beste lineare Approximation gefunden wird.

Falls $\mat X$ vollen Rang hat zeigt man leicht, dass (\ref{least_squares}) die eindeutige Lösung
\begin{align}
\hat{\beta}^{\text{OLS}} = (\mat X^{\top}\mat X)^{-1}\mat X^{\top} y
\end{align}
besitzt. Die Zielgröße $\hat{y}$ ergibt sich dann durch
\begin{align}
\hat{y} = \mat X \hat{\beta}^{\text{OLS}} = \mat X (\mat X^{\top}\mat X)^{-1}\mat X^{\top} y
\end{align}
Die Matrix $\mat P = \mat X (\mat X^{\top}\mat X)^{-1}\mat X^{\top}$ haben wir bereits in Abschnitt \ref{orthogonality} kennengelernt. Sie projiziert $y$ orthogonal auf den durch die Spalten von $\mat X$ aufgespannten Unterraum. Dies ermöglicht eine geometrische Interpretation der linearen Regression.

Wenn $\mat X$ keinen vollen Rang hat, ist die Lösung von (\ref{least_squares}) nicht mehr eindeutig. Dieser Art Probleme ereignen sich häufig in der Bild- und Signalanalyse, bei welcher wir meist über mehr Variablen als Beobachtungen verfügen. Um ein gewünschtes Verhalten der Regression zu gewährleisten, bestehen verschiedene Möglichkeiten der Filterung oder Regularisierung. In letzterem Fall versehen wir den Regressionsterm mit sog. \textit{Straftermen}, welche eine bedeutende Rolle in den folgenden Kapitel spielen werden. Wir möchten diese mithilfe der linearen Regression einführen.

Die von uns eingeführten Strafterme werden vor allem eine Schrumpfung der Regressionskoeffizienten verursachen. Daher möchten wir zunächst motivieren, in welchen Situationen eine derartige Regularisierung hilfreich sein kann. Falls wir über mehr Variablen als Beobachtungen verfügen, neigen lineare Regressionsmodelle häufig zu einer Überanpassung an die Trainingsdaten. Um die Vorhersagegenauigkeit für ungesehene Testdaten zu verbessern, kann eine Erhöhung des Bias im Sinne des Verzerrung-Varianz-Dilemmas sinnvoll sein. Dies können wir beispielsweise dadurch erreichen, dass wir die Regressionskoeffizienten verkleinern oder sogar auf Null setzen. Des Weiteren erschwert eine hohe Anzahl an Variablen, die in das Modell einfließen, unzweifelhaft eine Interpretation. Daher kann es von Vorteil sein, nur einen Teil der Variablen für das Modell auszuwählen. Optimalerweise selektiert man solche, die eine möglichst genaue Vorhersage ermöglichen.

Ein naheliegender Ansatz zur Lösung dieser Probleme wäre es zu versuchen, eine $k$-Teilmenge der Variablen zu finden, die eine minimale Summe der Residuenquadrate aufweist. In \cite{hastie_elements} werden verschiedene Methoden zur exakten und approximativen Berechnung dieser Teilmenge beschrieben. Nicht immer wird die Genauigkeit der Vorhersagen durch Verwendung dieses Ansatzes besser. Dies liegt vor allem daran, dass Variablen für das Modell entweder ausgewählt oder verworfen werden. Daher beschäftigen wir uns nun mit Methoden, die eine kontinuierliche Schrumpfung der Regressionskoeffizienten erlauben.


%----------------------------------------------------------------------------------------
%	Ridge Regression
%----------------------------------------------------------------------------------------


\section{Ridge Regression}
\label{ridge_regression}

Zu diesem Zweck kann die \textit{Tikhonov Regularisierung}, die auch unter dem Namen \textit{Ridge Regression} bekannt ist, genutzt werden. Mithilfe eines \textit{Ridge-Strafterms} opfern wir einen Teil des Bias für verbesserte Vorhersagen im Sinne des Verzerrung-Varianz-Dilemmas. Wir formulieren das Ridge Regression Problem in der Lagrange Form
\begin{align}
\label{ridge_regression_formulation}
\hat{\beta}^{\text{ridge}} = \argmin_{\beta} \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda_2\sum_{j=1}^p \beta_j^2,
\end{align}
wobei $\lambda_2 \geq 0$ ein Hyperparameter ist, der die Stärke der Schrumpfung der Regressionskoeffizienten kontrolliert. Hyperparameter werden zur Steuerung des Trainingsalgorithmus verwendet und müssen vorab festgelegt werden. Je größer $\lambda_2$, desto stärker ist die Schrumpfung der $\beta_j$. Durch die Einführung des $\ell_2$-Strafterms garantiert (\ref{ridge_regression_formulation}) auch für $p > n$ eine eindeutige Lösung, da $\norm{\cdot}_{2}^{2}$ streng konvex ist. Da $\beta_0$ nicht im Strafterm vorkommt, schätzen wir zunächst $\beta_0 = \bar{y} = \frac{1}{n}\sum_{i=1}^{n} y_i$ und zentrieren die Eingaben $x_{ij} = x_{ij} - \bar{x}_j$. Die eindeutige Lösung der zentrierten Version von (\ref{ridge_regression_formulation}) ist dann durch
\begin{align}
\hat{\beta}^{\text{ridge}}  = (\mat X^{\top} \mat X + \lambda_2\mat{\mathbb{1}})^{-1}\mat{X}^{\top}y
\end{align}
gegeben, wobei $\beta = (\beta_1, \ldots, \beta_p)$ und $\mat{X} \in \mathbb{R}^{n \times p}$ die Matrix der $x_i$. Die durch die Ridge Regression erzeugten Koeffizienten sind also um den Faktor $\frac{1}{1+\lambda_2}$ gegenüber denen der klassischen linearen Regression skaliert. Eine Dünnbesetzung der Koeffizienten wird erst für $\lambda_2 \to \infty$ erreicht.


%----------------------------------------------------------------------------------------
%	Lasso
%----------------------------------------------------------------------------------------


\section{Lasso}
\label{lasso}

Um eine bessere Interpretation des Modells zu ermöglichen, versucht man bei der \textit{Lasso} Regression eine Lösung zu finden, bei welcher viele Koeffizienten gleich Null sind. Das Lasso wurde erstmals von Tibshirani in \cite{tibshirani_lasso} eingeführt und ist in der Signalanalyse unter dem Namen \textit{Basis Pursuit} \cite{chen} bekannt. Mathematisch gesehen erreichen wir eine Dünnbesetzung durch Einbettung eines $\ell_1$-Strafterms
\begin{align}
\label{lasso_formulation}
\hat{\beta}^{\text{lasso}} = \argmin_{\beta} \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda_1\sum_{j=1}^p \left|\beta_j\right|.
\end{align}
Es wird also im Vergleich zu (\ref{ridge_regression_formulation}) lediglich die $\ell_2$-Norm durch eine $\ell_1$-Norm ausgetauscht. Bevor wir uns mit der Lösung dieses Problems beschäftigen, möchten wir erklären, warum die $\ell_1$-Norm eine Dünnbesetzung der Koeffizienten hervorruft. Zunächst geben wir eine geometrische Erklärung, welche in Abbildung \ref{lasso_ridge_regression_figure} zu sehen ist. Dort sind die $\ell_1$- und $\ell_2$-Beschränkungen, sowie die Höhenlinien der RSS-Funktion in zwei Dimensionen aufgezeichnet. Die optimalen Koeffizienten von Ridge und Lasso Regression ergeben sich nun aus dem Schnittpunkt der Höhenlinien mit der Begrenzung der jeweiligen Norm. Wählen wir die $\ell_1$-Norm, ist dieser Schnittpunkt mit einer hohen Wahrscheinlichkeit an eine der Ecken, so dass einer der beiden Koeffizienten auf Null gesetzt wird. Im Gegensatz dazu gibt es bei einer $\ell_2$-Begrenzung keine Ecken, weshalb jeder Randpunkt der Begrenzung als Schnittpunkt in Frage kommt. Dadurch wird keine Dünnbesetzung, sondern lediglich eine kontinuierliche Schrumpfung der Koeffizienten hervorgerufen. Dieser Effekt verstärkt sich in höheren Dimensionen.

\begin{figure}
\centering
\includegraphics[width = 0.9\textwidth]{figures/lasso_ridge_regression.jpg}
\caption{Die Abbildung zeigt die Beschränkungen der $\ell_1$-Norm (links) und der $\ell_2$-Norm (rechts) zusammen mit den Höhenlinien der RSS-Funktion im $\mathbb{R}^2$. Verdeutlicht wird hier die geometrische Findung von $\hat{\beta}^{\text{lasso}}$ (links) und $\hat{\beta}^{\text{ridge}}$ (rechts). (Abbildung basiert auf \cite{hastie_elements})}
\label{lasso_ridge_regression_figure}
\end{figure}

An dieser Stelle kann man auf den Gedanken kommen, andere Strafterme zu verwenden, welche bei geometrischer Betrachtung die Wahrscheinlichkeit erhöhen eine Dünnbesetzung der Koeffizienten hervorzurufen. So kann man zum Beispiel die $\ell_q$-Normen als Strafterm für Werte $q < 1$ in Betracht ziehen. In Abbildung \ref{norm_figure} sind die Begrenzungen der $\ell_q$-Normen für verschiedene Werte von $q$ eingezeichnet. Für $q \rightarrow 0$ entstehen sternförmige Höhenlinien, welche immer weiter zum Ursprung gedrückt werden. Somit wird es immer wahrscheinlicher, dass die Höhenlinien der RSS-Funktion eine Ecke treffen und wir dünnbesetzte Koeffizienten erhalten. Daher kann man folgendes Berechnungsproblem definieren
\begin{align}
\label{sparse_regression}
\hat{\beta}^{\text{sparse}} = \argmin_{\beta} \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda_q\sum_{j=1}^p \left|\beta_j\right|^q.
\end{align}
Leider ist dies nur in der Theorie ein guter Ansatz. Das Problem liegt nicht im Effekt der verschiedenen Strafterme, sondern in der Berechnung. Für $q < 1$ ist (\ref{sparse_regression}) ein nicht-konvexes Optimierungsproblem, da $\norm{\cdot}_q$ keine Norm gemäß Definition \ref{norm} ist. Im Extremfall der $\ell_0$-\q{Norm} wird (\ref{sparse_regression}) sogar NP-schwer \cite{foucart}. Somit besteht keine effiziente Methode zur Berechnung von $\hat{\beta}^{\text{sparse}}$ zur Verfügung. Der Wert $q = 1$ ist eine Art Kompromisslösung, die einerseits effizient zu berechnen ist und andererseits noch immer eine dünnbesetzte Lösung liefert.

\begin{figure}
\centering
	\begin{subfigure}{0.2\textwidth}
	\centering
	\includegraphics[width = \textwidth]{figures/norm_0_5.png}
	\label{norm_0_5_figure}
	\caption*{$q = 0.5$}
	\end{subfigure} \hspace{0.5cm}
	%	
	\begin{subfigure}{0.2\textwidth}
	\centering
	\includegraphics[width = \textwidth]{figures/norm_1.png}
	\label{norm_1_figure}
	\caption*{$q = 1$}
	\end{subfigure}\hspace{0.5cm}
	%
	\begin{subfigure}{0.2\textwidth}
	\centering
	\includegraphics[width = \textwidth]{figures/norm_2.png}
	\label{norm_2_figure}
	\caption*{$q = 2$}
	\end{subfigure}\hspace{0.5cm}
	%
	\begin{subfigure}{0.2\textwidth}
	\centering
	\includegraphics[width = \textwidth]{figures/norm_4.png}
	\label{norm_4_figure}
	\caption*{$q = 4$}
	\end{subfigure}

\caption{Die Abbildung zeigt die Begrenzungen der $\ell_q$-Norm im $\mathbb{R}^2$ für verschiedene Werte von $q$, also die Mengen $\{x \in \mathbb{R}^2 \colon \norm{x}_{q} \leq c \}$. (Abbildung basiert auf \cite{hastie_elements})}
\label{norm_figure}
\end{figure}

Um eine mathematisch gründliche Erklärung für die Dünnbesetzung zu liefern, wenden wir uns der Lösung von (\ref{lasso_formulation}) zu. Falls die Spalten von $\mat X$ orthonormal sind, ergibt sich eine explizite Lösung
\begin{align}
\hat{\beta}_j^{\text{lasso}} = \text{sign}(\hat{\beta}_j^{\text{OLS}}) \left(\left|\hat{\beta}_j^{\text{OLS}}\right| - \frac{\lambda_1}{2}\right)_{+},
\end{align}
wobei $(\cdot)_+ = max(\cdot, 0)$ ist. Der Beweis kann in \cite{murphy} nachgelesen werden. Die Lösung ist also durch den sog. \textit{soft thresholding operator} gegeben, welcher durch
\begin{align}
\text{soft}_{\delta}(x) = \text{sign}(x)(|x| - \delta)_+
\end{align}
mithilfe eines Schwellwerts $\delta$ definiert wird. Dieser steht im Gegensatz zum hard-thresholding Operator und wird in Abbildung \ref{thresholding_figure} dargestellt. Durch den soft-thresholding Operator werden Koeffizienten in $\hat{\beta}^{\text{OLS}}$ entweder auf Null gesetzt oder um $\frac{\lambda_1}{2}$ geschrumpft. Nun sind wir auch in der Lage zu verstehen, warum Tibshirani \cite{tibshirani_lasso} den Begriff Lasso eingeführt hat, welcher für \textit{Least absolute selection and shrinkage operator} steht. 

Für den allgemeinen Fall wird (\ref{lasso_formulation}) mithilfe von Näherungsverfahren gelöst. Da $\norm{\beta}_1$ nicht differenzierbar ist wenn $\beta_j = 0$, sind wir mit einem nicht glattem Optimierungsproblem konfrontiert. Seit der Problemformulierung wurde eine Vielzahl an Algorithmen entwickelt bzw. adaptiert, die eine numerische Lösung liefern. Dazu gehören Least-angle Regression (LARS) \cite{efron_lars}, Koordinaten-Abstiegsverfahren \cite{friedman}, Subdifferential-Methoden und  Näherungs-Gradientenverfahren \cite{yang, vandenberghe}. Letztere sind eine natürliche Erweiterung von Gradientenverfahren falls die Zielfunktion nicht differenzierbar ist. In Abschnitt \ref{elastic_net} werden wir auf ein Koordinaten-Abstiegsverfahren weiter eingehen.

\begin{figure}
\centering
	\begin{subfigure}{0.4\textwidth}
	\centering
	\includegraphics[width = \textwidth]{figures/soft_thresholding.jpg}
	\label{hard_thresholding}
	\end{subfigure}\hspace{1cm}
	%	
	\begin{subfigure}{0.4\textwidth}
	\centering
	\includegraphics[width = \textwidth]{figures/hard_thresholding.jpg}
	\label{soft_thresholding}
	\end{subfigure}
\caption{Gezeigt werden zwei verschiedene Methoden für ein Schwellenwertverfahren. Bei der soft-thresholding-Operation $\operatorname{soft}_{\delta}(x)$ (links) und hard-thresholding-Operation $\operatorname{hard}_{\delta}(x)$ (rechts) werden alle Einträge von $x$ kleiner als $\delta$ auf Null gesetzt. Der Unterschied besteht darin, dass $\operatorname{soft}_{\delta}(x)$ die verbliebenen Einträge um $|\delta|$ schrumpft.}
\label{thresholding_figure}
\end{figure}

Es wurde herausgearbeitet, dass das Lasso zwei wesentliche Nachteile besitzt. Falls es im Datensatz Gruppen stark korrelierter Variablen gibt, tendiert die Methode dazu nur eine Variable je Gruppe statt die Gruppe als Ganzes auszuwählen. Zum Beispiel bei der Suche nach Genen, welche mit einer bestimmten Krankheit verbunden sind, ist man aber daran interessiert, alle assoziierten Koeffizienten zu finden. Darüber hinaus führt dieser Effekt zu verschlechterten Vorhersagen. Um diesem Problem zu begegnen, kann man das sog. \textit{Group Lasso} verwenden \cite{yuan}, bei welcher man Gruppen vorab im Datensatz festlegen kann. Der im Zuge dieser Arbeit aber wichtigere Nachteil ist, dass das Lasso maximal $n$ Variablen selektieren kann, falls $p > n$ ist. In diesem Fall hat $\mat X$ maximal Rang $n$ hat, weshalb wir $y$ mithilfe von $n$ Variablen perfekt vorhersagen können. Das Lasso wählt dann die $n$ Variablen, welche $\lambda_1\norm{\beta}_1$ minimieren. Für moderne Datensätze mit $p \gg n$ ist es aber oft nicht ausreichend, nur $n$ von Null verschiedene Koeffizienten zuzulassen.


%----------------------------------------------------------------------------------------
%	Elastic Net
%----------------------------------------------------------------------------------------


\section{Elastic Net}
\label{elastic_net}

Damit im Fall $p > n$ mehr als $n$ Variablen für das Modell selektiert werden können, betrachten wir eine Kombination von Lasso und Ridge Regression. Durch die Einbettung einer $\ell_1$ und $\ell_2$-Norm erhalten wir das sog. \textit{Elastic Net} \cite{zou_elasticnet}
\begin{align}
\label{elastic_net_formulation}
\hat{\beta}^{\text{en}} = \argmin_{\beta} \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda_1\sum_{j=1}^p \left|\beta_j\right| + \lambda_2\sum_{j=1}^p \beta_j^2.
\end{align}
Wie zuvor wird durch den $\ell_1$-Strafterm ein dünnbesetztes Modell generiert. Dagegen fördert der $\ell_2$-Strafterm den Gruppeneffekt, stabilisiert den Regularisierungspfad und erlaubt eine beliebige Anzahl zu selektierender Variablen. Um eine doppelte Schrumpfung der Koeffizienten zu vermeiden, kann das Elastic Net mit dem Faktor $(1 + \lambda_2)$ korrigiert werden. 

Ähnlich wie bei der Lasso Regression kann nur dann eine explizite Lösung von (\ref{elastic_net_formulation}) angegeben werden, wenn die Spalten von $\mat X$ orthonormal sind. Für eine allgemeine numerische Lösung wurden zum Beispiel LARS-EN \cite{zou_elasticnet}, welches auf dem LARS Algorithmus für das Lasso basiert, und ein Koordinaten-Abstiegsverfahren \cite{friedman} vorgeschlagen. Die Implementierung des Elastic Nets in scikit-learn \cite{scikit_learn} beruht auf letzterem Verfahren mit einer leicht abgeänderten mathematischen Formulierung
\begin{align}
\label{elastic_net_python}
\argmin_{\beta} R_{\lambda}(\beta) = \argmin_{\beta} \frac{1}{2n}  \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \alpha \norm{\beta}_{1} + \frac{\lambda}{2}(1-\alpha) \norm{\beta}_{2}^2,
\end{align}
wobei $\alpha \in [0,1]$ das Verhältnis zwischen $\ell_1$- und $\ell_2$-Norm ist. Während wir mit $\alpha$ das Verhältnis der beiden Regressionen bestimmen, können wir mit $\lambda$ die Stärke der Bestrafung kontrollieren. Setzt man 
\begin{align}
\lambda = \frac{2\lambda_2 + \lambda_1}{2n} \quad \text{und} \quad \alpha = \frac{\lambda_1}{2\lambda_2 + \lambda_1}
\end{align}
entspricht (\ref{elastic_net_python}) unserem ursprünglich formulierten Problem. Durch die Reparametrisierung können wir das Verhältnis der Bestrafungen flexibel anpassen. Mit $\alpha = 0$ reduziert sich (\ref{elastic_net_python}) auf Ridge und für $\alpha = 1$ auf die Lasso Regression.

Oft ist es schwer, alle Koeffizienten gleichzeitig zu optimieren. Daher nutzt man bei einem Koordinaten-Abstiegsverfahren aus, dass eine partielle Optimierung über den $j$-ten Koeffizienten einfach zu berechnen ist, wenn alle anderen fixiert sind. Mithilfe der bisherigen Schätzer $\tilde{\beta}_0$ und $\tilde{\beta}_l$ für $l \neq j$, löst man also
\begin{align}
\label{coordinate_enet_problem}
\tilde{\beta}_j = \argmin_{\beta_j} R_{\lambda}(\tilde{\beta}_0, \ldots, \tilde{\beta}_{j-1}, \beta_j, \tilde{\beta}_{j+1}, \ldots, \tilde{\beta}_p).
\end{align}
Da $R_{\lambda}(\tilde{\beta}_0, \ldots, \tilde{\beta}_{j-1}, \beta_j, \tilde{\beta}_{j+1}, \ldots, \tilde{\beta}_p)$ für $\beta_j = 0$ nicht differenzierbar ist, werden zur Lösung von (\ref{coordinate_enet_problem}) Subdifferentiale benutzt, welche eine Verallgemeinerung des Gradienten auf nicht differenzierbare konvexe Funktionen ist. Wir werden uns auf die Angabe einer Lösung beschränken und verweisen für eine Herleitung dieser auf \cite{donoho}. Der Übersichtlichkeit halber nehmen wir zusätzlich an, dass die Variablen standardisiert wurden, d.h. $\sum_{i=1}^n x_{ij} = 0$ und $\frac{1}{n-1}\sum_{i=1}^n x_{ij}^2 = 1$. Somit ist eine explizite Lösung von (\ref{coordinate_enet_problem}) durch
\begin{align}
\tilde{\beta}_j = \frac{\operatorname{soft}_{\lambda\alpha} (\frac{1}{n}\sum_{i=1}^n x_{ij}(y_i - (\tilde{\beta}_0 + \sum_{l \neq j} x_{il}\tilde{\beta}_l)))}{1 + \lambda (1-\alpha)}
\end{align}
gegeben. Dabei ist $y_i - (\tilde{\beta}_0 + \sum_{l \neq j} x_{il}\tilde{\beta}_l)$ das partielle Residuum der $i$-ten Beobachtung bei Anpassung von $\beta_j$, d.h. das Residuum bei bestmöglicher Vorhersage ohne Einbeziehung von $\beta_j$. Der Term $\frac{1}{n}\sum_{i=1}^n x_{ij}(y_i - (\tilde{\beta}_0 + \sum_{l \neq j} x_{il}\tilde{\beta}_l))$ ist ein Maß dafür, ob eine Einbeziehung von $\beta_j$ in das Modell die Vorhersagen verbessert. Nach Berechnung dieses Werts, wenden wir soft-thresholding gemäß der $\ell_1$-Bestrafung an und schrumpfen die Koeffizienten mit dem Faktor $1+\lambda(1-\alpha)$ für den Beitrag der $\ell_2$-Norm. Wegen der Standardisierung gilt
\begin{align}
\label{cd_fast_explaination}
\frac{1}{n}\sum_{i=1}^n x_{ij}(y_i - (\tilde{\beta}_0 + \sum_{l \neq j} x_{il}\tilde{\beta}_l)) = \frac{1}{n}\sum_{i=1}^n x_{ij}r_i + \tilde{\beta}_j
\end{align}
wobei $r_i$ das aktuelle Residuum der $i$-ten Beobachtung ist. Anhand von (\ref{cd_fast_explaination}) können wir erkennen, warum das Koordinaten-Abstiegsverfahren effizient ist. Viele der Koeffizienten, die zuvor Null waren, bleiben nach der soft-thresholding Operation Null, wenn die Residuen bezüglich $\beta_j$ nicht zu groß sind. Daher muss $\beta_j$ in vielen Fällen nicht aktualisiert werden.

Für die Wahl des Koeffizienten, welcher als nächstes minimiert werden soll, gibt es verschiedene Möglichkeiten. Man kann zyklisch, zufällig oder in die Richtung des steilsten Abstiegs entlang der Koordinaten vorgehen. In Algorithmus \ref{enet_algorithm} haben wir das zyklische Koordinaten-Abstiegsverfahren zusammengefasst.

\begin{algorithm}[tbh]
    \caption{Koordinaten-Abstiegsverfahren für das Elastic Net}
    \label{enet_algorithm}
    \begin{algorithmic}[1]
        \Procedure{elasticnet}{$\mat X, y, \lambda, \alpha$}
        	\State Initialisiere $\beta = (\mat X^T\mat X + \lambda\mat{\mathbb{1}})^{-1}\mat X^{\top}y$
            \While {nicht konvergiert}
                \For {$j = 1, \ldots, p$}
                	\State $c_j \gets \frac{1}{n}\sum_{i=1}^n x_{ij}(y_i - (\beta_0 + \sum_{l \neq j} x_{il}\beta_l))$
                	\State $\beta_j \gets \frac{\operatorname{soft}_{\lambda\alpha}(c_j)}{1 + \lambda (1-\alpha)}$
        		\EndFor
        	\EndWhile
        \EndProcedure
    \end{algorithmic}
\end{algorithm} 

Auch wenn klassische Kriterien für die Konvergenz von Koordinaten-Abstiegsverfahren aufgrund der fehlenden Differenzierbarkeit nicht zutreffen, besitzt (\ref{elastic_net_python}) eine andere charakterisierende Eigenschaft, für welche das Verfahren konvergiert \cite{tseng}.


%----------------------------------------------------------------------------------------
%	Vergleich der Regressionsmethoden
%----------------------------------------------------------------------------------------


\section{Vergleich der Regressionsmethoden}
\label{comparison_linear_models}

Zur Veranschaulichung der oben eingeführten Methoden werden wir diese nun anwenden. Dabei greifen wir auf einen durch scikit-learn \cite{scikit_learn} bereitgestellten Datensatz, der erstmals durch \cite{efron_lars} öffentlich gemacht worden ist, zurück. Ein Ausschnitt des Datensatzes befindet sich in Tabelle \ref{diabetes_data_set}.

\begin{table}
\centering
\begin{tabular}[c]{c|cccccccccc|c}
& \thead{AGE} & \thead{SEX} & \thead{BMI} & \thead{BP} & \multicolumn{6}{c|}{\ldots \thead{Blutproben} \ldots} & \thead{Zielgröße}\\
\thead{Patient} & \thead{x1} & \thead{x2} & \thead{x3} & \thead{x4} & \thead{x5} & \thead{x6} & \thead{x7} & \thead{x8} & \thead{x9} & \thead{x10} & \thead{y}\\
\hline
1 & 59 & 2 & 32.1 & 101 & 157 & 93.2 & 38 & 4 & 4.9 & 87 & 151\\
2 & 48 & 1 & 21.6 & 87 & 183 & 103.2 & 70 & 3 & 3.9 & 69 & 75\\
3 & 72 & 2 & 30.5 & 93 & 156 & 93.6 & 41 & 4 & 4.7 & 85 & 141\\
4 & 24 & 1 & 25.3 & 84 & 198 & 131.4 & 40 & 5 & 4.9 & 89 & 206\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
441 & 36 & 1 & 30.0 & 95 & 201 & 125.2 & 42 & 5 & 5.1 & 85 & 220\\
442 & 36 & 1 & 19.6 & 71 & 250 & 133.2 & 97 & 3 & 4.6 & 92 & 57\\
\end{tabular}
\caption{Überblick über den Diabetes Datensatz aus \cite{diabetes_data}.  In diesem wurden für $n = 442$ Diabetes Patienten $p=10$ verschiedene Variablen gemessen. Dazu gehören Alter (AGE), Geschlecht (SEX), Body Mass Index (BMI), Blutdruck (BP) und verschiedene Blutproben (S1, \ldots, S6). Die Zielgröße $y$ enthält Werte für den Krankheitsfortschritt ein Jahr nach Behandlungsbeginn.}
\label{diabetes_data_set}
\end{table}

In Abbildung \ref{regression_coefficients} sehen wir die Ergebnisse der verschiedenen Regressionsmethoden. Anhand von \ref{regression_coefficients} können wir die Schrumpfung der Regressionskoeffizienten mit Veränderung der Regularisierungsparameter $\lambda_1$, $\lambda_2$ bzw. $\lambda$ visuell nachvollziehen. Deutlich zu erkennen ist, dass bei der Ridge Regression eine kontinuierliche Schrumpfung der Koeffizienten stattfindet, d.h. sie werden erst für $\lambda_2 \to \infty$ auf Null gesetzt. Im Gegensatz dazu erkennt man bei der Lasso Regression, dass schon für kleine $\lambda_1$ einzelne Koeffizienten auf Null gesetzt werden und wir eine Dünnbesetzung erhalten. Das Elastic Net bildet eine natürliche Brücke zwischen Ridge und Lasso Regression. Einerseits können wir sehen, dass einzelne Koeffizienten auf Null gesetzt werden und andererseits eine Stabilisierung der Regularisierungspfade.

Um ein geeignetes Modell zu erhalten, müssen die Hyperparameter $\lambda_1$, $\lambda_2$ bzw. $\lambda$ geeignet gewählt werden. Zu diesem Zweck kann ein $k$-faches Kreuzvalidierungsverfahren eingesetzt werden. Hierbei wird der Datensatz in $k$ möglichst gleich große Teilmengen $T_1, \ldots, T_k$ zerlegt. Anschließend werden $k$ Durchläufe gestartet, wobei die jeweils $i$-te Teilmenge $T_{i}$ als Testmenge und die verbleibenden $k-1$ Teilmengen $\{T_{1},...,T_{k}\}\setminus \{T_{i}\}$ als Trainingsmenge verwendet werden. Mittelt man die mittleren quadratischen Fehler der Testmengen, erhält man ein Maß für dem Gesamtfehler. Man wählt dann den Regularisierungsparameter mit minimalem Gesamtfehler, welcher in Abbildung \ref{regression_coefficients} eingezeichnet ist.

\begin{figure}
\centering
	\begin{subfigure}{0.875\textwidth}
	\centering
	\includegraphics[width = \textwidth]{figures/ridge_regression_coefficients_cv.jpg}
	\label{ridge_regression_coefficients}
	\vspace{-0.5cm}
	\caption{Verhalten der Koeffizienten bei der Ridge Regression.}
	\vspace{0.5cm}
	\end{subfigure}
	%	
	\begin{subfigure}{0.875\textwidth}
	\centering
	\includegraphics[width = \textwidth]{figures/lasso_regression_coefficients_cv.jpg}
	\label{lasso_regression_coefficients}
	\vspace{-0.5cm}
	\caption{Verhalten der Koeffizienten bei der Lasso Regression.}
	\vspace{0.5cm}
	\end{subfigure}
	%
	\begin{subfigure}{0.875\textwidth}
	\centering
	\includegraphics[width = \textwidth]{figures/elastic_net_coefficients_cv.jpg}
	\label{elastic_net_coefficients}
	\vspace{-0.5cm}
	\caption{Verhalten der Koeffizienten bei der Elastic Net Regression mit festem $\alpha = 0.99$.}
	\end{subfigure}
\caption{Jede Linie entspricht dem Wert (y-Achse) des jeweiligen Regressionskoeffizienten in Abhängigkeit der Regularisierungsparameter $\lambda_2$, $\lambda_1$ bzw. $\lambda$ (x-Achse). Zu sehen ist die unterschiedliche Art der Schrumpfung für die verschiedenen Strafterme. Die vertikalen Linien entsprechen dem Wert des jeweiligen Hyperparameters, der durch ein 10-faches Kreuzvalidierungsverfahren bestimmt worden ist.}
\label{regression_coefficients}
\end{figure}