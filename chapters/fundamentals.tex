% Main chapter title
\chapter{Mathematische Grundlagen}

% Chapter label
\label{fundamentals}

\section{Normen und deren Effekte}
\label{norm}

\begin{defn}[$l_p$-Norm \cite{schaback}] 
Auf dem $\mathbb{R}^n$ sind die $l_p$-Normen für $1 \leq p < \infty$ definiert als
$$\norm{x}_p \defeq \left( \sum_{i=1}^{n} \norm{x_i}^{p} \right) ^{\frac{1}{p}} \quad x \in \mathbb{R}^{n}$$
und für $p = \infty$ als
$$\norm{x}_{\infty} \defeq \max_{1 \leq i \leq n} \norm{x_i} \quad x \in \mathbb{R}^{n}.$$
Im Fall $p = \infty$ spricht man auch von der \textit{Maximumsnorm} und im Fall $p = 2$ von der \textit{euklidischen Norm}.
\end{defn}
\subsection{l0-Norm}
\subsection{l1-Norm}
\subsection{l2-Norm}

\section{Regression}
Lineare Regression (Least Squares)
\subsection{LASSO}
\subsection{Ridge Regression}

\section{Lineare Algebra}

Ein Großteil der Mathematik der Hauptkomponentenanalyse beruht auf Methoden der linearen Algebra. Daher werden wir im Folgendem die wichtigsten Begriffe einführen. Aufgrund des Anwedungsfalls werden wir uns hier auf reelle Vektorräume beschränken.
Zunächst Orthogonalität, dann Matrixzerlegungen, dann noch einige weitere Eigenschaften und Theoreme, die wir später benötigen.

\subsection{Orthogonalität}
\begin{defn}[Skalarprodukt \cite{jaenich}]
Sei $V$ ein reeller Vektorraum. Ein \textit{Skalarprodukt} in $V$ ist eine Abbildung $\inner{\cdot}{\cdot}: V \times V \longrightarrow \mathbb{R}$ mit den folgenden Eigenschaften:
\begin{enumerate}[(i)]
\item Für jedes $x \in V$ sind die Abbildungen
\begin{align*}
\inner{\cdot}{x}: V & \longrightarrow \mathbb{R} & \inner{x}{\cdot}: V & \longrightarrow \mathbb{R}\\
v & \longmapsto \inner{v}{x} & v & \longmapsto \inner{x}{v}
\end{align*}
linear. $\quad$ (Bilinearität)
\item $\inner{x}{y} = \inner{y}{x}$ für alle  $x,y \in V \quad$ (Symmetrie)
\item $\inner{x}{x} \geq 0$ für alle $x \neq 0 \quad$ (Positive Definitheit)
\end{enumerate}
\end{defn}

Allgemein versteht man unter einem \textit{euklidischem Vektorraum} ein Paar $(V, \inner{\cdot}{\cdot})$, welches aus einem reellem Vektorraum $V$ und einem Skalarprodukt $\inner{\cdot}{\cdot}$ auf $V$ besteht. Durch das Skalarprodukt wird eine Norm auf $V$ induziert:
$$\norm{v} \defeq \sqrt{\inner{v}{v}}$$
In den folgenden Kapiteln werden wir uns vor allem mit dem \textit{Standardskalarprodukt} in $\mathbb{R}^n$ beschäftigen. Dies ist gegeben durch 
$$\inner{x}{y} = x_1y_1 + \cdots + x_ny_n.$$
Die dazugehörige Norm ist die \textit{euklidische Norm} oder $l_2$-Norm, welche wir bereits zuvor in \ref{norm} gesehen haben.

\begin{defn}[Orthogonalität \cite{jaenich}]
Zwei Elemente $v,w$ eines euklidischen Vektorraums $V$ heißen \textit{orthogonal} (geschrieben $v \perp w$) wenn ihr Skalarprodukt null ist, d.h.
$$v \perp w \iff \inner{v}{w} = 0.$$
Eine Familie $(v_1, \ldots, v_n)$ in $V$ heißt \textit{orthogonal} oder \textit{Orthogonalsystem}, wenn
$$v_i \perp v_j \quad \text{für alle} \quad i \neq j.$$
Gilt zusätzlich $\inner{v_i}{v_i} = 1$ für alle $1 \leq i \leq n$, so spricht man von einem \textit{Orthonormalsystem}.
\end{defn}

\begin{defn}[Orthonormalbasis \cite{fischer}]
Sei $\inner{\cdot}{\cdot}: V \times V \longrightarrow \mathbb{R}$ ein Skalarprodukt. Ein System von Vektoren $(v_1, \ldots, v_n)$ in $V$ wird als \textit{Orthogonalbasis} (bzw. \textit{Orthonormalbasis}) bezeichnet, wenn folgende Bedingungnen erfüllt sind:
\begin{enumerate}[(i)]
\item $(v_1, \ldots, v_n)$ ist eine Basis von $V$
\item $(v_1, \ldots, v_n)$ ist ein Orthogonalsystem (bzw. Orthonormalsystem)
\end{enumerate}
\end{defn}

\begin{thm}[Existenz einer Orthonormalbasis (Fischer Lineare Algebra)]
Jeder endlichdimensionale euklidische Vektorraum besitzt eine Orthonormalbasis.
\end{thm}

\begin{thm}[Verallgemeinerter Satz des Pythagoras \cite{anton}]
\label{pythagoras}
Für orthogonale Vektoren $u,v$ in einem euklidischem Vektorraum $V$ gilt
$$\norm{u+v}^2 = \norm u^2 + \norm v^2.$$
\end{thm}

Der Begriff der Orthogonalität lässt sich auf lineare Abbildungen und somit auf Matrizen übertragen.

\begin{defn}[Orthogonale Abbildung \cite{jaenich}]
Seien $V,W$ euklidische Vektorräume. Eine lineare Abbildung $f: V \longrightarrow W$ heißt \textit{orthogonal} oder \textit{isometrisch}, wenn
$$\inner{f(v)}{f(w)} = \inner{v}{w} \quad \text{für alle } v,w \in V$$
\end{defn}

\begin{defn}[Orthogonale Matrix \cite{anton}]
Eine Matrix $A \in \mathbb{M}(n \times n, \mathbb{R})$ heißt 		\textit{orthogonal}, falls deren Zeilen und Spalten jeweils paarweise orthonormal sind:
$$\mat A^T \mat A = \mathbb{1}_n$$
\end{defn}

\begin{defn}[Orthogonalprojektion (Wikipedia)]
Eine \textit{Orthogonalprojektion} auf einen Untervektorraum $U$ eines Vektorraums $V$ ist eine lineare Abbildung $P_U \colon V \rightarrow V$, die für alle Vektoren $v\in V$ die beiden Eigenschaften
\begin{enumerate}[(i)]
\item $P_U(v) \in U \quad$   (Projektion)
\item $\langle P_U(v) - v , u \rangle = 0$ für alle $u \in U \quad$ (Orthogonalität)
\end{enumerate}
erfüllt.
\end{defn}

Mithilfe einer Orthogonalbasis für $U$ lässt sich aus dieser Definition eine Lösung für die Orthogonalprojektion $P_U(v)$ herleiten.

\begin{thm}[\cite{anton}]
Ist $(u_1, \ldots, u_n)$ eine Orthogonalbasis von $U$, so gilt für alle $v \in V$
$$P_{U}(v) = \sum_{i=1}^n \frac{\langle v, u_i \rangle}{\langle u_i, u_i \rangle} u_i$$
\end{thm}

In späteren Kapiteln werden wir die Orthogonalprojektion in einer anderen Form nutzen. Wir können die Projektion auch als Matrix-Vektor-Produkt auffassen. Verwenden wir das Standardskalarprodukt so gilt mit einer Orthogonalbasis $(u_1, \ldots, u_n)$ von $U$:
$$P_U(v) = \sum_{i=1}^n \frac{v^T u_i}{u_i^T u_i} u_i = \sum_{i=1}^n \frac{u_i u_i^T}{u_i^T u_i}v = \mat A \mat A^T v$$
wobei die Spalten von $\mat A$ die normalisierten Vektoren der Orthogonalbasis sind, d.h. $\mat A = \begin{bmatrix} \frac{u_1}{\norm{u_1}} & \cdots & \frac{u_n}{\norm{u_n}} \end{bmatrix}$

Mithilfe von \ref{pythagoras} lässt sich zeigen, dass der orthogonal auf den Unterraum projizierte Vektor den Abstand zwischen dem Ausgansvektor und dem Unterraum minimiert.

\begin{thm}[\cite{anton}]
Sei $U$ ein Unterraum eines euklidischen Vektorraums $V$. Dann ist $P_U(v)$ die beste Näherung von $u$ in $U$, d.h.
$$\norm{P_U(v) - v}^2 \leq \norm{u - v}^2 \quad \text{für alle } u \in U$$
\end{thm}


\subsection{Matrixzerlegungen}

In diesem Abschnitt werden wir uns mit verschiedenen Matrixzerlegungen beschäftigen. Dazu werden wir zunächst ...

\begin{defn}[Eigenwert, Eigenvektor \cite{anton}]
Sei $\mat{A} \in \rnn$. Ein von Null verschiedener Vektor $x \in \rn$ heißt \textit{Eigenvektor} von $\mat{A}$, wenn
$$\mat{A}x = \lambda x$$
für einen Skalar $\lambda \in \mathbb{R}$. Die Zahl $\lambda$ heißt \textit{Eigenwert} von $\mat{A}$.
\end{defn}

\begin{defn}[Diagonalisierbar \cite{anton}]
Eine quadratische Matrix $\mat A \in \rnn$ heißt \textit{diagonalisierbar}, wenn eine invertierbare Matrix $\mat P$ existiert, so dass $\mat{D} = \mat{P}^{-1}\mat{A}\mat{P}$ Diagonalgestalt hat.
\end{defn}

Es gibt nun viele verschiedene Kriterien, wann eine Matrix diagonalisierbar ist. Für unsere spätere Anwendung interessieren wir uns vor allem für die Frage, ob es zu einer gegebenen Matrix $\mat{A} \in \rnn$ eine orthogonale Matrix $\mat{P}$ gibt, die $\mat{A}$ diagonalisiert. Eine derartige Diagonalisierung wird auch als \textit{Hauptachsentransformation} bezeichnet. Dieser Name stammt ursprünglich aus der Theorie der Kegelschnitte. Hierbei ist eine Hauptachsentransformation eine orthogonale Abbildung, welche die Koordinatenachsen in die Richtungen der beiden \textit{Hauptachsen} überführt. Wir wollen uns aber vorest nicht mit dieser geometrischen Interpretation beschäftigen, sondern mit einem mathematisch äquivalenten, in den Anwendungen aber wichtigeren Problem.

\begin{thm}[Hauptachsentransformation \cite{jaenich}]
Ist $\mat{A} \in \rnn$ eine symmetrische Matrix, so gibt es eine orthogonale Transformation $\mat{P}$, welche $\mat{A}$ in eine Diagonalmatrix $\mat{D} \defeq \mat{P}^{-1}\mat{A}\mat{P}$ der Gestalt
$$\mat{D} = \begin{bmatrix}
    \lambda_{1} & & & & & & \\
    & \ddots & & & & & \\
    & & \lambda_1 & & & & \\
    & & & \ddots & & & \\
    & & & &\lambda_r & & \\
    & & & & & \ddots & \\
    & & & & & & \lambda_{r}
  \end{bmatrix}$$
überführt. Hierbei sind $\lambda_1, \ldots, \lambda_r$ die verschiedenen Eigenwerte von $\mat{A}$.
\end{thm}

Wir können also eine symmetrische Matrix $\mat{A}$ zerlegen in $\mat A = \mat P \mat D \mat P^T$. Man kann $\mat{P}$ so konstruieren, dass die Spalten genau den Eigenvektoren von $\mat{A}$ entsprechen. Wir werden diese Umformung in späteren Kapiteln unter dem Begriff \textit{Eigenwertzerlegung} (Englisch: Eigenvalue Decomposition) verwenden. 

Eine eng verwandte, aber vielseitigere Faktorisierung von Matrizen ist die \textit{Singulärwertzerlegung}. Sie ermöglicht eine Zerlegung auch von nicht quadratischen oder nicht symmetrischen Matrizen.

\begin{thm}[Singulärwertzerlegung \cite{schaback}]
Jede Matrix $\mat{A} \in \rmn$ besitzt eine \textit{Singulärwertzerlegung} 
$$\mat{A} = \mat{U}\mat{D}\mat{V}^{T}$$
mit orthogonalen Matrizen $\mat U \in \mathbb{R}^{m \times m}$ und $\mat V \in \rnn$, sowie der Diagonalmatrix $\mat{D} = (\sigma_j\delta_{ij}) \in \rmn$.
\end{thm}

\begin{defn}[Singulärwert]
Die positiven Diagonaleinträgen $\sigma_{i} > 0$ von $\mat D$ werden \textit{Singulärwerte} genannt.
\end{defn}

Nach Konvention werden die Singulärwerte von $\mat D$ absteigend sortiert, d.h. $\sigma _{1} \geq \cdots \geq \sigma _{r}$. Außerdem sind die Singulärwerte eindeutig bestimmt und stehen durch $\sigma_i = \sqrt{\lambda_i}$ in einer engen Beziehung mit den Eigenwerten $\lambda_i$ von $\mat A$. Geometrisch bedeutet diese Faktorisierung, dass sich die Matrix $A$ in zwei Drehungen $\mat U, \mat V$ und eine Streckung unterteilen lässt. Dabei korrespondieren die Streckungsfaktoren mit den Einträgen der Diagonalmatrix $\mat D$.


\subsection{Matrixnorm und Rang (Eigenschaften?)}

Das Konzept von Normen lässt sich auch Matrizen übertragen. In späteren Kapiteln werden wir vor allem Gebrauch folgender Norm machen.

\begin{defn}[Frobeniusnorm \cite{schaback}]
Für eine Matrix $A \in \rmn$ ist die \textit{Frobeniusnorm} definiert durch
$$\norm{\mat A}_F = \left( \sum_{i=1}^{m} \sum_{j=1}^{n} \lvert a_{ij} \rvert ^{2} \right) ^{\frac{1}{2}}.$$
\end{defn}

Eine weitere wichtige Eigenschaft von Matrizen ist der \textit{Rang}.

\begin{defn}[Rang \cite{anton}]
Die Dimension des Zeilen- und des Spaltenraumes einer Matrix $\mat A$ heißt \textit{Rang} von $\mat A$ und wird mit $\rang{\mat A}$ bezeichnet.
\end{defn}

\begin{thm}[Wörtlich von Wikipedia, Eckart-Young-Theorem]
The unstructured problem with fit measured by the Frobenius norm, i.e.,
$$\text{minimize} \quad \text{over } \widehat D \quad \|D - \widehat D\|_{\text{F}} \quad\text{subject to}\quad \operatorname{rank}\big(\widehat D\big) \leq r $$
has analytic solution in terms of the singular value decomposition of the data matrix. The result is referred to as the matrix approximation lemma or Eckart–Young–Mirsky theorem.[4] Let

$$D = U\Sigma V^{\top} \in \mathbb{R}^{m\times n}, \quad m \leq n$$

be the singular value decomposition of $D$ and partition $U, \Sigma=:\operatorname{diag}(\sigma_1,\ldots,\sigma_m)$, and $V$ as follows:

$$U =: \begin{bmatrix} U_1 & U_2\end{bmatrix}, \quad 
\Sigma =: \begin{bmatrix} \Sigma_1 & 0 \\ 0 & \Sigma_2 \end{bmatrix}, \quad\text{and}\quad 
V =: \begin{bmatrix} V_1 & V_2 \end{bmatrix},$$

where $U_{1}$ is $m\times r$, $\Sigma _{1}$ is $r\times r$, and $V_{1}$ is $n\times r$. Then the rank-$r$ matrix, obtained from the truncated singular value decomposition

$$\widehat D^* = U_1 \Sigma_1 V_1^{\top},$$

is such that

$$\|D-\widehat D^*\|_{\text{F}} = \min_{\operatorname{rank}(\widehat D) \leq r} \|D-\widehat D\|_{\text{F}} = \sqrt{\sigma^2_{r+1} + \cdots + \sigma^2_m}.$$

The minimizer $\widehat D^*$ is unique if and only if $\sigma_{r+1}\neq\sigma_{r}$.
\end{thm}

\section{Signaltheorie}

\subsection{Fouriertransformation}
\subsection{Nyquist-Shannon Abtasttheorem}

\section{Statistik}
Varianz, Erwartungswert
\subsection{Empirische Kovarianzmatrix}

\section{Mannigfaltigkeit}

\section{Dictionary Learning}