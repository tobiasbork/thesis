% Main chapter title
\chapter{Dünnbesetzte Hauptkomponentenanalyse}

% Chapter label
\label{sparse_pca}

Ein wesentlicher Nachteil der Hauptkomponentenanalyse besteht darin, dass sich die neuen Variablen aus einer Linearkombination \textit{aller} bestehenden Variablen zusammensetzt. Dies erschwert besonders für hochdimensionale Daten eine Interpretation der Hauptachsen. Während zuvor jede Variable eine Bedeutung hatte, sind wir nach der Transformation meist nicht in der Lage den Hauptachsen eine Bedeutung im Kontext zuzuweisen. Um zu verstehen, was die Hauptachsen im Modell repräsentieren kann es besonders hilfreich sein, wenn diese \textit{dünnbesetzt} sind, sich also nur aus wenigen Variablen zusammensetzen. Treffen wir irgendwelche Annahmen? Des Weiteren ist nicht jede Variable relevant zur Strukturerkennung. impose extra constraints, which sacrifices some variance in order to improve interpretability. Interpretation ist oberstes Ziel!!!

Zu Anfang dieses Kapitels werden wir eine naheliegende mathematische Formulierung des Problems beschreiben. Leider wird sich diese als NP-vollständig herausstellen, weshalb wir in Abschnitt \ref{relaxation} verschiedene Wege aufzeigen, dass Problem zu relaxieren. In \ref{sparse_pca_construction} möchten wir uns mit einem dieser Ansätze intensiv beschäftigen, welcher den Ausgangspunkt für den weiteren Verlauf dieser Arbeit darstellt. Der Rest dieses Kapitels ist den Details dieses Ansatzes gewidmet.


%----------------------------------------------------------------------------------------
%	Problemformulierung
%----------------------------------------------------------------------------------------


\section{Problemformulierung}
\label{problem_formulation}

Wir möchten nun Hauptachsen eines gegebenen Datensatzes identifizieren mit der Zusatzbedingung, dass diese dünnbesetzt sind. Die wohl einfachste Vorgehensweise ist, zuerst die gewöhnliche Hauptkomponentenanalyse durchzuführen und anschließend ein Schwellwertmethode auf die Hauptachsen anzuwenden. Hierbei vernachlässigt man alle Koeffizienten, die kleiner als ein bestimmter Schwellenwert sind, indem man sie auf 0 setzt. Eine solche Prozedur kann aber in vielen Fällen irreführend sein, unter welcher die Qualität der Ergebnisse leidet \cite{cadima}. Die Wichtigkeit einer Variable in den Hauptachsen wird nicht allein durch den Koeffizient bestimmt. Zu berücksichtigen sind unter anderem sowohl die Standardabweichung als auch die Korrelationen mit anderen Variablen. Bei einer Schwellwertmethode werden diese Faktoren nicht beachtet, weshalb den Ergebnissen im Allgemeinen nicht vertraut werden darf.

Hier Regression on ordinary PCA's mit Zou et al?

Anstelle eines zweischrittigen Ansatzes kann die Dünnbesetzung direkt in die Problemformulierung mit eingebaut werden. Gegeben sei dazu wieder eine Datenmatrix $\mat X \in \rnp$, wobei $n$ die Anzahl an Beobachtungen und $p$ die Anzahl an Variablen ist. Des Weiteren gehen wir davon aus, dass die Matrix $\mat X$ zuvor spaltenweise zentriert wurde. Dann kann die dünnbesetzte Hauptkomponentenanalyse als sukzessives Maximierungsproblem formuliert werden:
\begin{gather}
\label{sparse_pca_np}
\begin{split}
v_{k} = \argmax_{\norm{v}_2 = 1} v^{T}\mat{\Sigma} v\\
\text{unter der Nebendingung, dass für } k\geq 2 \, v_{k}^Tv_{l} = 0 \quad \forall 1 \leq l < k\\
\text{und } \norm{v_{k}}_0 \leq t 
\end{split}
\end{gather}
wobei $\mat{\Sigma} = \frac{\mat X^T \mat X}{n-1}$ die Stichprobenkovarianzmatrix ist. Der einzige Unterschied zur klassischen Hauptkomponentenanalyse, wie wir sie in (\ref{pca_variance_maximization_first}) beschrieben haben, besteht in der Einführung der $\ell_0$-Norm. Somit beschränken wir uns auf die Suche von Hauptachsen, welche höchstens $t$ von Null verschiedene Einträge haben. Wählen wir $t = p$ reduziert sich das Problem auf (\ref{pca_variance_maximization_first}). Während (\ref{sparse_pca_np}) eine sehr schöne und einfache mathematische Formulierung ist, wurde gezeigt, dass dieses Problem NP-vollständig ist \cite{foucart}. Zur Berechnung dünnbesetzter Hauptachsen sind wir also angehalten eine geeignete Relaxation zu finden.


%----------------------------------------------------------------------------------------
%	Relaxation
%----------------------------------------------------------------------------------------


\section{Relaxation}
\label{relaxation}

Es existiert eine Vielfalt an Ansätzen, um das Problem zu relaxieren. Wir wollen zunächst einen kleinen Überblick über die unterschiedlichen Ideen geben und uns anschließend mit einer genauer beschäftigen. Eine selektive Übersicht der verschiedenen Ansätze haben wir hier erstellt.\\

\textbf{SCoTLASS}

Inspiriert von der Lasso Regression \cite{tibshirani_lasso} schlugen Jolliffe et al. \cite{scotlass} vor, die $\ell_1$-Norm anstelle der $\ell_0$-Norm als Strafterm zu verwenden. Wie wir bereits in Abschnitt \ref{lasso} gesehen haben, kann die $\ell_1$-Norm genutzt werden, um dünnbesetzte Vektoren zu erhalten. Somit liegt es nahe das Problem wie folgt zu formulieren.
\begin{gather}
\label{scotlass}
\begin{split}
v_{k} = \argmax_{\norm{v}_2 = 1} v^{T}\mat{\Sigma} v\\
\text{unter der Nebendingung, dass für } k\geq 2 \, v_{k}^Tv_{l} = 0 \quad \forall 1 \leq l < k\\
\text{und } \norm{v_{k}}_1 \leq t 
\end{split}
\end{gather}
Wie in (\ref{sparse_pca_np}) hat man mit der Wahl der Parameters $t$ Einfluss auf die Dünnbesetzung der Hauptachsen. Aufgrund der hohen Berechnungskosten ist SCoTLASS allerdings für hochdimensionale Daten ungeeignet. Diese sind vor allem darauf zurückzuführen, dass (\ref{scotlass}) kein konvexes Optimierungsproblem ist. Des Weiteren ergeben sich Schwierigkeiten bei der Wahl des Hyperparameters $t$. Auch wenn eine passende Wahl eine gewünschte Dünnbesetzung hervorruft, gibt es kaum Orientierungshilfen. Zusätzlich hat ScoTLASS dasselbe grundlegende Problem wie das Lasso. Die Anzahl von null verschiedener Einträge ist durch die Anzahl Beobachtungen im Datensatz limitiert, welches die Brauchbarkeit des Modells deutlich einschränkt. Zusammen mit den hohen Berechnungskosten ist dieser Ansatz in der Praxis daher meist impraktikabel.\\

\textbf{Semidefinite Programmierung}

Konvexe Relaxation ist eine Standard-Technik, um mit schwierigen nichtkonvexen Problemen umzugehen. d'Aspremont et al. \cite{daspremont_semidefinite} entwickeln einen Ansatz, welcher sich als semidefinites Programmierungsproblem ausdrücken lässt. Zunächst werden wir (\ref{sparse_pca_np}) dafür mit Matrizen reformulieren.

Sei $\mat V = v_kv_k^{\top}$. Dann übersetzen sich die Nebenbedingungen 
\begin{align}
\label{semidefinite_programming_naive}
\begin{split}
\argmax_{\mat P} = \spur{\mat \Sigma\mat P}\\
\spur{\mat P} = 1, \quad \norm{\mat P}_0 \leq k^2, \quad \mat P \geq 0, \quad \rang{\mat P} = 1
\end{split}
\end{align}

Diese Formulierung ist noch immer nichtkonvex aufgrund der Rang-Bedingung  und der $\ell_0$-Strafterm. 
Per Definition ist $\mat P$ symmetrisch und $\mat P^2 = \mat P$. Somit ist
$$\norm{\mat P}_{F}^{2} = \spur{\mat P^{\top}\mat P} = \spur{\mat P} = 1$$
und mit der Cauchy-Schwarz-Ungleichung folgt
$$\mat{1}_p^{\top} |\mat P| \mat{1}_p \leq \sqrt{\norm{\mat{P}}_0 \norm{\mat{P}}_F^2} \leq k$$
Ersetzen wir die $\ell_0$-Strafterm und lassen die Rang-Bedingung fallen erhalten wir die DSPCA-Formulierung
\begin{align}
\label{semidefinite_programming}
\begin{split}
\argmax_{\mat P} = \spur{\mat \Sigma\mat P}\\
\spur{\mat P} = 1, \quad \mat{1}_p^{\top} |\mat P| \mat{1}_p \leq k, \quad \mat P \geq 0
\end{split}
\end{align}
Dies stellt ein semidefinites Programmierungsproblem dar, bei welcher die zu optimierenden Variablen symmetrische Matrizen sind unter der Nebenbedingung, dass sie positiv semidefinit sind. Für kleine Probleme kann (\ref{semidefinite_programming}) effizient durch \textit{Innere-Punkte-Verfahren} (English: \textit{interior-point methods}) gelöst werden. SDPT3 \cite{toh}.

In (\ref{semidefinite_programming}) wird allerdings $\mat P$ berechnet und nicht die eigentliche Hauptachse. Hierfür kürzen d'Aspremont et al. die Matrix $\mat P$ und behalten nur den größten Eigenvektor $v_k$. Anschließend erhält man weitere Hauptachsen durch Matrix Deflation, indem wir $\mat \Sigma$ durch
$$\mat \Sigma - (v_k^{\top}\mat \Sigma v_k) v_kv_k^{\top}$$ 
ersetzen. Für größere Probleme wird eine Methode von Nesterov benutzt, um eine Laufzeit von $\mathcal{O}(\frac{p^4\sqrt{\log{p}}}{\epsilon})$ zu erreichen.\\

\textbf{Iterative Schwellenwert-Methode}

Basierend auf der Formulierung (\ref{pca_best_rank_approximation}) der Hauptkomponentenanalyse als beste Rang $k$ Approximation an die Datenmatrix $\mat X$ haben Shen und Huang \cite{shen} das folgende Optimierungsproblem formuliert
\begin{align}
\label{iterative_thresholding}
\begin{split}
(u_1, v_1) = \argmin_{u, v} \norm{\mat X - u v^{\top}}_{F}^{2}  + \lambda \norm{v}_{1}\\
\norm{u}_2 = 1
\end{split}
\end{align}
Somit erhält man mit $\frac{v_1}{\norm{v_1}}$ die erste dünnbesetzte Hauptachse. Auch hier werden die restlichen Hauptachsen sequentiell berechnet durch Ersetzen der Datenmatrix $\mat X_{(k+1)} = \mat X - \sum_{i=1}^k u_iv_i^{\top}$. Jede Iteration kann durch ein alternierendes Minimierungsverfahren gelöst werden. Fixiert man $v$, so ist das optimale $u$ gegeben durch $u = \frac{\mat Xv}{\norm{\mat Xv}}$. Andererseits reduziert sich (\ref{iterative_thresholding}) für festes $u$ auf
$$\argmin_{v} -2\spur{\mat X^{\top}uv^{\top}} + \norm{v}^{2} + \lambda \norm{v}_1.$$
Eine explizite Lösung ist durch den soft-thresholding Operator gegeben
$$v = \operatorname{soft}_{\frac{\lambda}{2}}(\mat X^{\top} U)$$
welcher in Abschnitt \ref{generalized_linear_models} eingeführt worden ist.

Diese Methode ist sehr ähnlich zu der von Zou et al. \cite{zou_sparsepca} , mit welcher wir uns im folgenden Abschnitt beschäftigen werden. Der große Unterschied besteht darin, dass die Hauptachsen dort nicht sequentiell, sondern gleichzeitig berechnet werden. Witten et al. haben in \cite{witten} ebenfalls eine Methode entwickelt, die unter diese Kategorie fällt.\\

\textbf{Weitere Relaxationsideen}

Es gibt noch eine Reihe weiterer Ideen, die in der Literatur betrachtet wurden. Ein interessierter Lesen sei auf die folgenden Ansätze verwiesen.
\begin{itemize}
\item eine verallgemeinerte Potenzmethode \cite{journee}
\item ein alternierendes Maximierungs-Netzwerk \cite{richtarik}
\item Vorwärts und Rückwärts-Greedy-Suche mittels Branch-and-Bound-Verfahren \cite{moghaddam}
\item eine Bayes-Formulierung \cite{guan}
\end{itemize}


%----------------------------------------------------------------------------------------
%	Konstruktion Sparse PCA
%----------------------------------------------------------------------------------------


\section{Konstruktion}
\label{construction}

Wir werden uns nun mit dem von Zou, Hastie und Tibshirani in \cite{zou_sparsepca} eingeführten Ansatz ausführlich beschäftigen. Zou und Hastie führten zuvor in \cite{zou_elasticnet} das sog. \textit{elastic net} ein, welches den Grundstein für die mathematische Formulierung legen wird.

Wie bereits in (\ref{pca_regression_formulation}) beschrieben kann die Hauptkomponentenanalyse als regressionsartiges Problem betrachtet werden. Das folgende Theorem erweitert die bisherige Formulierung, indem nun nicht ausschließlich orthogonale Projektionen erlaubt werden. 
Im Folgenden bezeichnet $k$ die Anzahl an Hauptkomponenten, die wir extrahieren möchten und $x_i$ die $i$-te Zeile von $\mat X$.

\begin{thm} \label{pca_regression_formulation_ridge}
Sei $\mat{A}_{p \times k} = [ \alpha_1, \ldots ,\alpha_k ]$ und $\mat{B}_{p \times k} = [ \beta_1, \ldots ,\beta_k ]$. Für ein $\lambda_2 > 0$ sei
$$(\hat{\mat{A}}, \hat{\mat{B}}) = \argmin_{\mat{A}, \mat{B}} \sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}^2 + \lambda_2 \sum_{j=1}^{k}\norm{\beta_j}^2$$
$$\text{wobei } \mat{A}^T\mat{A} = I_{k \times k}.$$
Dann ist $\hat{\beta}_j \propto v_j$ für $j = 1,2,\ldots,k$. 
\end{thm}

Fordern wir $\mat A =  \mat B$ reduziert sich die Verlustfunktion $\sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}^2$ auf die klassische Hauptkomponentenanalyse (\ref{pca_regression_formulation}). Theorem \ref{pca_regression_formulation_ridge} zeigt, dass wir die Bedingung $\mat A = \mat B$ unter Einführung eines Ridge-Strafterms vernachlässigen können. Mithilfe dieser Verallgemeinerung können wir die Hauptkomponentenanalyse flexibel modifizieren.

Um dünnbesetzte Hauptachsen zu erhalten können wir einen $\ell_1$-Strafterm in die Zielfunktion einbetten. Das ein solcher Strafterm eine gewünschte Dünnbesetzung hervorruft, haben wir in Abschnitt \ref{lasso} beobachten können. Wir definieren daher das \textit{Sparse PCA Kriterium} mit den Hyperparameter $\lambda_{1,j}$ und $\lambda_2$
\begin{align}
\label{spca_criterion}
\begin{split}
(\hat{\mat{A}}, \hat{\mat{B}}) = \argmin_{\mat{A}, \mat{B}} \sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}_2^2 + \lambda_2 \sum_{j=1}^{k}\norm{\beta_j}_2^2 + \sum_{j=1}^k \lambda_{1,j} \norm{\beta_j}_1\\
\text{unter der Nebenbedingung, dass } \mat{A}^T\mat{A} = I_{k \times k}
\end{split}
\end{align}
Die normierten Spalten von $\mat B$ nennen wir dann die \textit{dünnbesetzten Hauptachsen}. Um die Dünnbesetzung für jede Hauptachse unterschiedlich wählen zu können, erlauben wir unterschiedliche Bestrafungen $\lambda_{1,j}$. Dagegen erlauben wir für die Ridge-Bestrafung $\lambda_2$, die im Wesentlichen für die Reduktion von (\ref{spca_criterion}) auf (\ref{pca_regression_formulation}) benötigt wird falls $\lambda_{1,j} = 0$, keine differenzierte Behandlung. Allerdings hat die $\ell_2$-Bestrafung noch einen weiteren Vorteil, welcher in der Praxis relevant ist. Es bewältigt das Lasso-Defizit, so dass auch mehr als $n$ Variablen im Fall $p>n$ ausgewählt werden können.

Wir möchten darauf hinweisen, dass im Gegensatz zu manch anderen Varianten der dünnbesetzten Hauptkomponentenanalyse (\ref{spca_criterion}) eine zeitgleiche anstatt einer sequentiellen Berechnung der Hauptachsen ermöglicht. Dies wird im folgendem Abschnitt von entscheidender Bedeutung sein.



%----------------------------------------------------------------------------------------
%	Anpassung der Transformation, Residuen und Varianzen
%----------------------------------------------------------------------------------------


\section{Anpassung der Transformation, Residuen und Varianzen}

Bei der Verwendung der dünnbesetzten Hauptkomponentenanalyse übertragen sich viele der Eigenschaften der klassischen Variante nicht. \cite{camacho}. Daher gilt es folgende Punkte zu berücksichtigen.\\

\textbf{Korrelation der Hauptkomponenten}

Bei einer klassischen Hauptkomponentenanalyse sind die Hauptkomponenten aufgrund der orthogonalen Hauptachsen unkorreliert. Letztere Eigenschaft fordern wir bei der dünnbesetzten Variante in (\ref{spca_criterion}) nicht, so dass durchaus starke Korrelationen zwischen den Hauptkomponenten auftreten können. Während dies eine flexiblere Modellierung ermöglicht, wird es dadurch schwieriger die Ergebnisse geeignet zu visualisieren. Besonders bei der Verwendung von Streudiagrammen, welche genutzt werden, um den Beitrag der Ausgangsvariablen zu den Hauptachsen zu visualisieren, kann dies zu Problemen führen. Hierbei unterstellt man die Orthogonalität der Hauptachsen, was zu Verzerrungen der Distanzen im Bild führen kann \cite{geladi}. Des Weiteren kann die Berechnung der erfassten Varianz des Datensatzes, welches häufig als Maß für die Qualität eines Modells genutzt wird, nicht analog zur klassischen Variante durchgeführt werden.\\

\textbf{Varianzverlust der Hauptkomponenten}

Der Erfolg der Hauptkomponentenanalyse beruht vor allem darauf, dass die Hauptkomponenten optimal bezüglich erklärter Varianz sind. Oft kann ein Großteil an Information eines Datensatzes durch eine geringe Anzahl an Hauptkomponenten beschrieben werden, welches eine Visualisierung und Interpretation hochdimensionaler Daten ermöglicht. Bei der dünnbesetzten Hauptkomponentenanalyse opfern wir einen Teil der erklärten Varianz für simplere, einfacher zu interpretierende Hauptachsen. Um einen genauso großen Teil an Information des Datensatzes zu erklären, benötigen wir daher eine größere Anzahl an Hauptkomponenten in unserem Modell. Somit können wir die Dimension des Datensatzes unter Umständen nicht all zu stark reduzieren.

In einem erst vor Kurzem erschienen wissenschaftlichen Artikel zeigen Camacho et al. \cite{camacho}, dass viele der Varianten der dünnbesetzten Hauptkomponentenanalyse bezüglich der genannten Punkte Probleme aufweisen. Insbesondere wurde die Berechnung der Hauptkomponenten, Residuen und der erklärten Varianz bislang falsch durchgeführt. Wir möchten an dieser Stelle die Unterschiede detailliert erklären.

Zunächst

\section{Kritik}

results in very large residuals
highly correlated scores
q is no updated due to the normalization step


%----------------------------------------------------------------------------------------
%	Theoretische Aussagen
%----------------------------------------------------------------------------------------


\section{Theoretische Aussagen} 
\label{spca_theorems}


recomputation necessary of principal components when changing number of pc's.

Alexandre d’Aspremont, Optimal Solutions for Sparse Principal Component Analysis
We then use the same relaxation to derive sufficient conditions for global optimality of a
solution, which can be tested in O(n
3
) per pattern. We discuss applications in subset selection and
sparse recovery and show on artificial examples and biological data that our algorithm does provide
globally optimal solutions in many cases.