% Main chapter title
\chapter{Dünnbesetzte Hauptkomponentenanalyse}

% Chapter label
\label{sparse_pca}

Ein Nachteil der Hauptkomponentenanalyse ist, dass sich die neuen Variablen meist aus einer Linearkombination aller bestehenden Variablen zusammensetzt. Dies macht es besonders für hochdimensionale Daten schwierig die Hauptachsen zu interpretieren. Oft können somit nicht die relevanten features/Variablen herausgelesen werden. Es kann durchaus passieren, dass nicht alle Variablen relevant zur Strukturerkennung sind. 

\section{Motivation}

\section{Problemformulierung}
NP-schwere Formulierung

\section{Relaxation / Approximation Ideen}

\section{Konstruktion}

Sparse PCA Kriterium.

$$(\hat{\mat{A}}\hat{\mat{B}}) = \argmin_{\mat{A}, \mat{B}} \sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}^2 + \lambda \sum_{j=1}^{k}\norm{\beta_j}^2 + \sum_{j=1}^k \lambda_{1,j} \norm{\beta_j}_1$$

subject to $\mat{A}^T\mat{A} = I_{k \times k}$
oblique projections AB

\section{Theoretische Aussagen Sparse PCA}
z.B. wie werden neue Varianzen berechnet

Alexandre d’Aspremont, Optimal Solutions for Sparse Principal Component Analysis
We then use the same relaxation to derive sufficient conditions for global optimality of a
solution, which can be tested in O(n
3
) per pattern. We discuss applications in subset selection and
sparse recovery and show on artificial examples and biological data that our algorithm does provide
globally optimal solutions in many cases.