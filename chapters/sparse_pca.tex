% Main chapter title
\chapter{Dünnbesetzte Hauptkomponentenanalyse}

% Chapter label
\label{sparse_pca}

Ein wesentlicher Nachteil der Hauptkomponentenanalyse besteht darin, dass sich die neuen Variablen aus einer Linearkombination \textit{aller} bestehenden Variablen zusammensetzt. Dies erschwert besonders für hochdimensionale Daten eine Interpretation der Hauptachsen. Während zuvor jede Variable eine Bedeutung hatte, sind wir nach der Transformation meist nicht in der Lage den Hauptachsen eine Bedeutung im Kontext zuzuweisen. Um zu verstehen, was die Hauptachsen im Modell repräsentieren kann es besonders hilfreich sein, wenn diese \textit{dünnbesetzt} sind, sich also nur aus wenigen Variablen zusammensetzen. Daher sind wir oft bereit, einen Teil der Varianz für eine vereinfachte Interpretation einzutauschen.

Zu Anfang dieses Kapitels werden wir eine naheliegende mathematische Formulierung des Problems beschreiben. Leider wird sich diese als NP-vollständig herausstellen, weshalb wir in Abschnitt \ref{relaxation} verschiedene Wege aufzeigen, dass Problem zu relaxieren. In \ref{sparse_pca_construction} möchten wir uns dem von Zou und Hastie in \cite{zou_sparsepca} eingeführten Ansatz intensiv beschäftigen. Dieser gilt sicherlich zu den meistverbreiteten Varianten der dünnbesetzten Hauptkomponentenanalyse. In einem erst vor Kurzem erschienen wissenschaftlichen Artikel werden einige Probleme des Ansatzes deutlich. Darum erläutern wir in Abschnitt \ref{adjustment_of_variances} Neuerungen und Korrekturen der Methode durch Camacho et al. \cite{camacho}. Zu Schluss dieses Kapitels werden wir Möglichkeiten für eine automatisierte Wahl der Modellparameter präsentieren und die Konsistenz in hochdimensionalen Fällen darlegen.


%----------------------------------------------------------------------------------------
%	Problemformulierung
%----------------------------------------------------------------------------------------


\section{Problemformulierung}
\label{problem_formulation}

Wir möchten nun Hauptachsen eines gegebenen Datensatzes identifizieren mit der Zusatzbedingung, dass diese dünnbesetzt sind. Die wohl einfachste Vorgehensweise ist, eine Schwellenwertmethode auf die durch die klassische Hauptkomponentenanalyse entstandenen Hauptachsen anzuwenden. Hierbei vernachlässigt man alle Koeffizienten, die kleiner als ein bestimmter Schwellenwert sind, indem man sie auf 0 setzt. Eine solche Prozedur kann aber in vielen Fällen irreführend sein, unter welcher die Qualität der Ergebnisse leidet \cite{cadima}. Die Wichtigkeit einer Variable in den Hauptachsen wird nicht allein durch den Koeffizient bestimmt. Zu berücksichtigen sind unter anderem sowohl die Standardabweichung als auch die Korrelationen mit anderen Variablen. Bei einer Schwellenwertmethode werden diese Faktoren nicht beachtet, weshalb den Ergebnissen im Allgemeinen nicht vertraut werden darf.

Anstelle eines zweischrittigen Ansatzes kann die Dünnbesetzung direkt in die Problemformulierung mit eingebaut werden. Gegeben sei dazu wieder eine Datenmatrix $\mat X \in \rnp$, wobei $n$ die Anzahl an Beobachtungen und $p$ die Anzahl an Variablen ist. Des Weiteren gehen wir davon aus, dass die Matrix $\mat X$ zuvor spaltenweise zentriert wurde. Dann kann die dünnbesetzte Hauptkomponentenanalyse als sukzessives Maximierungsproblem formuliert werden:
\begin{gather}
\label{sparse_pca_np}
\begin{split}
v_{k} = \argmax_{\norm{v}_2 = 1} v^{T}\mat{\Sigma} v\\
\text{unter der Nebendingung, dass } v_{k}^Tv_{l} = 0 \quad \forall 1 \leq l < k \text{ mit } k\geq 2\\
\text{und } \norm{v_{k}}_0 \leq t 
\end{split}
\end{gather}
wobei $\mat{\Sigma} = \mat X^T \mat X$ die Stichprobenkovarianzmatrix ist. Der einzige Unterschied zur klassischen Hauptkomponentenanalyse in (\ref{pca_variance_maximization_first}) und (\ref{pca_variance_maximization_rest}) besteht in der Einführung der $\ell_0$-Norm. Somit beschränken wir uns auf die Suche von Hauptachsen, welche höchstens $t$ von Null verschiedene Einträge haben. Wählen wir $t = p$, reduziert sich das Problem auf (\ref{pca_variance_maximization_first}) und (\ref{pca_variance_maximization_rest}). Während (\ref{sparse_pca_np}) eine sehr schöne und einfache mathematische Formulierung ist, wurde gezeigt, dass dieses Problem NP-vollständig ist \cite{foucart}. Zur Berechnung dünnbesetzter Hauptachsen sind wir also angehalten eine geeignete Relaxation zu finden.


%----------------------------------------------------------------------------------------
%	Relaxation
%----------------------------------------------------------------------------------------


\section{Relaxation}
\label{relaxation}

Es existiert eine Vielfalt an Ansätzen, um das Problem zu relaxieren. Wir wollen zunächst einen kleinen Überblick über die unterschiedlichen Ideen geben und uns anschließend mit einer genauer beschäftigen. 

\subsubsection{SCoTLASS}

Inspiriert von der Lasso Regression \cite{tibshirani_lasso} schlugen Jolliffe et al. \cite{scotlass} vor, die $\ell_1$-Norm anstelle der $\ell_0$-Norm als Strafterm zu verwenden. Wie haben bereits in Abschnitt \ref{lasso} beobachten können, dass die $\ell_1$-Norm genutzt werden kann, um dünnbesetzte Vektoren zu erhalten. Somit liegt es nahe das Problem wie folgt zu formulieren.
\begin{gather}
\label{scotlass}
\begin{split}
v_{k} = \argmax_{\norm{v}_2 = 1} v^{T}\mat{\Sigma} v\\
\text{unter der Nebendingung, dass } v_{k}^Tv_{l} = 0 \quad \forall 1 \leq l < k \text{ mit } k\geq 2\\
\text{und } \norm{v_{k}}_1 \leq t 
\end{split}
\end{gather}
Wie in (\ref{sparse_pca_np}) hat man mit der Wahl der Parameters $t$ Einfluss auf die Dünnbesetzung der Hauptachsen. Aufgrund der hohen Berechnungskosten ist SCoTLASS allerdings für hochdimensionale Daten ungeeignet. Diese sind vor allem darauf zurückzuführen, dass (\ref{scotlass}) kein konvexes Optimierungsproblem ist. Des Weiteren ergeben sich Schwierigkeiten bei der Wahl des Hyperparameters $t$. Auch wenn eine passende Wahl eine gewünschte Dünnbesetzung hervorruft, gibt es kaum Orientierungshilfen. Zusätzlich hat ScoTLASS dasselbe grundlegende Problem wie das Lasso. Die Anzahl von null verschiedener Einträge ist durch die Anzahl Beobachtungen im Datensatz limitiert, welches die Brauchbarkeit des Modells deutlich einschränkt. Zusammen mit den hohen Berechnungskosten ist dieser Ansatz in der Praxis daher meist impraktikabel.

%\subsubsection{Semidefinite Programmierung}
%
%Konvexe Relaxation ist eine Standard-Technik, um mit schwierigen nichtkonvexen Problemen umzugehen. d'Aspremont et al. \cite{daspremont_semidefinite} entwickeln einen Ansatz, welcher sich als semidefinites Programmierungsproblem ausdrücken lässt. Zunächst werden wir (\ref{sparse_pca_np}) dafür mit Matrizen reformulieren.
%
%Sei $\mat V = v_kv_k^{\top}$. Dann übersetzen sich die Nebenbedingungen 
%\begin{align}
%\label{semidefinite_programming_naive}
%\begin{split}
%\argmax_{\mat P} = \spur{\mat \Sigma\mat P}\\
%\spur{\mat P} = 1, \quad \norm{\mat P}_0 \leq k^2, \quad \mat P \geq 0, \quad \rang{\mat P} = 1
%\end{split}
%\end{align}
%
%Diese Formulierung ist noch immer nichtkonvex aufgrund der Rang-Bedingung  und der $\ell_0$-Strafterm. 
%Per Definition ist $\mat P$ symmetrisch und $\mat P^2 = \mat P$. Somit ist
%$$\norm{\mat P}_{F}^{2} = \spur{\mat P^{\top}\mat P} = \spur{\mat P} = 1$$
%und mit der Cauchy-Schwarz-Ungleichung folgt
%$$\mat{1}_p^{\top} |\mat P| \mat{1}_p \leq \sqrt{\norm{\mat{P}}_0 \norm{\mat{P}}_F^2} \leq k$$
%Ersetzen wir die $\ell_0$-Strafterm und lassen die Rang-Bedingung fallen erhalten wir die DSPCA-Formulierung
%\begin{align}
%\label{semidefinite_programming}
%\begin{split}
%\argmax_{\mat P} = \spur{\mat \Sigma\mat P}\\
%\spur{\mat P} = 1, \quad \mat{1}_p^{\top} |\mat P| \mat{1}_p \leq k, \quad \mat P \geq 0
%\end{split}
%\end{align}
%Dies stellt ein semidefinites Programmierungsproblem dar, bei welcher die zu optimierenden Variablen symmetrische Matrizen sind unter der Nebenbedingung, dass sie positiv semidefinit sind. Für kleine Probleme kann (\ref{semidefinite_programming}) effizient durch \textit{Innere-Punkte-Verfahren} (English: \textit{interior-point methods}) gelöst werden. SDPT3 \cite{toh}.
%
%In (\ref{semidefinite_programming}) wird allerdings $\mat P$ berechnet und nicht die eigentliche Hauptachse. Hierfür kürzen d'Aspremont et al. die Matrix $\mat P$ und behalten nur den größten Eigenvektor $v_k$. Anschließend erhält man weitere Hauptachsen durch Matrix Deflation, indem wir $\mat \Sigma$ durch
%$$\mat \Sigma - (v_k^{\top}\mat \Sigma v_k) v_kv_k^{\top}$$ 
%ersetzen. Für größere Probleme wird eine Methode von Nesterov benutzt, um eine Laufzeit von $\mathcal{O}(\frac{p^4\sqrt{\log{p}}}{\epsilon})$ zu erreichen.\\
%
%\textbf{Iterative Schwellenwert-Methode}
%
%Basierend auf der Formulierung (\ref{pca_best_rank_approximation}) der Hauptkomponentenanalyse als beste Rang $k$ Approximation an die Datenmatrix $\mat X$ haben Shen und Huang \cite{shen} das folgende Optimierungsproblem formuliert
%\begin{align}
%\label{iterative_thresholding}
%\begin{split}
%(u_1, v_1) = \argmin_{u, v} \norm{\mat X - u v^{\top}}_{F}^{2}  + \lambda \norm{v}_{1}\\
%\norm{u}_2 = 1
%\end{split}
%\end{align}
%Somit erhält man mit $\frac{v_1}{\norm{v_1}}$ die erste dünnbesetzte Hauptachse. Auch hier werden die restlichen Hauptachsen sequentiell berechnet durch Ersetzen der Datenmatrix $\mat X_{(k+1)} = \mat X - \sum_{i=1}^k u_iv_i^{\top}$. Jede Iteration kann durch ein alternierendes Minimierungsverfahren gelöst werden. Fixiert man $v$, so ist das optimale $u$ gegeben durch $u = \frac{\mat Xv}{\norm{\mat Xv}}$. Andererseits reduziert sich (\ref{iterative_thresholding}) für festes $u$ auf
%$$\argmin_{v} -2\spur{\mat X^{\top}uv^{\top}} + \norm{v}^{2} + \lambda \norm{v}_1.$$
%Eine explizite Lösung ist durch den soft-thresholding Operator gegeben
%$$v = \operatorname{soft}_{\frac{\lambda}{2}}(\mat X^{\top} U)$$
%welcher in Abschnitt \ref{generalized_linear_models} eingeführt worden ist.
%
%Diese Methode ist sehr ähnlich zu der von Zou et al. \cite{zou_sparsepca} , mit welcher wir uns im folgenden Abschnitt beschäftigen werden. Der große Unterschied besteht darin, dass die Hauptachsen dort nicht sequentiell, sondern gleichzeitig berechnet werden. Witten et al. haben in \cite{witten} ebenfalls eine Methode entwickelt, die unter diese Kategorie fällt.\\

\subsubsection{Weitere Relaxationsideen}

Weitere Ideen zur Relaxation von (\ref{sparse_pca_np}) beruhen auf den unterschiedlichen Formulierungen der Hauptkomponentenanalyse. Durch die Einbettung von Straftermen sind die verschiedenen Sichtweisen bei der dünnbesetzten Variante aber nicht mehr äquivalent. Daher haben wir eine selektive Übersicht der verschiedenen Ansätze erstellt. Ein interessierter Lesen sei auf die folgenden Quellen verwiesen.
\begin{itemize}
\item eine iterative Schwellenwertmethode \cite{shen, witten}
\item eine verallgemeinerte Potenzmethode \cite{journee}
\item ein alternierendes Maximierungs-Netzwerk \cite{richtarik}
\item Vorwärts- und Rückwärts-Greedy-Suche mittels Branch-and-Bound-Verfahren \cite{moghaddam}
\item Konvexe Relaxation mittels semidefiniter Programmierung \cite{daspremont_semidefinite}
\item eine Bayes-Formulierung \cite{guan}
\end{itemize}


%----------------------------------------------------------------------------------------
%	Konstruktion Sparse PCA
%----------------------------------------------------------------------------------------


\section{Konstruktion}
\label{construction}

Wir werden uns nun mit dem von Zou, Hastie und Tibshirani in \cite{zou_sparsepca} eingeführten Ansatz ausführlich beschäftigen. Ausgangspunkt ist die Verbindung zur Regression, welche wir in (\ref{pca_regression_formulation}) beschrieben haben. Im Folgenden bezeichnet $k$ die Anzahl an Hauptkomponenten, die wir extrahieren möchten und $x_i$ die $i$-te Zeile von $\mat X$. Mit $\widehat{\mat B}$ werden wir die dünnbesetzten Hauptachsen bezeichnen, um sie von den klassischen Hauptachsen $\widehat{\mat V}$ zu unterscheiden. Das folgende Theorem erweitert die bisherige Formulierung, indem nicht nur ausschließlich orthogonale Projektionen erlaubt werden. 

\begin{thm} \label{pca_regression_formulation_ridge}
Sei $\mat{A}_{p \times k} = [ \alpha_1, \ldots ,\alpha_k ]$ und $\mat{B}_{p \times k} = [ \beta_1, \ldots ,\beta_k ]$. Für ein $\lambda_2 > 0$ sei
$$(\widehat{\mat{A}}, \widehat{\mat{B}}) = \argmin_{\mat{A}, \mat{B}} \sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}^2 + \lambda_2 \sum_{j=1}^{k}\norm{\beta_j}^2$$
$$\text{wobei } \mat{A}^T\mat{A} = I_{k \times k}.$$
Dann ist $\widehat{\beta}_j \propto \widehat{v}_j$ für $j = 1,2,\ldots,k$. 
\end{thm}

Fordern wir $\mat A =  \mat B$ reduziert sich die Verlustfunktion $\sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}^2$ auf die klassische Hauptkomponentenanalyse in (\ref{pca_regression_formulation}). Theorem \ref{pca_regression_formulation_ridge} zeigt, dass wir die Bedingung $\mat A = \mat B$ unter Einführung eines Ridge-Strafterms vernachlässigen können. Mithilfe dieser Verallgemeinerung können wir die Hauptkomponentenanalyse flexibel modifizieren.

Um dünnbesetzte Hauptachsen zu erhalten, können wir einen $\ell_1$-Strafterm in die Zielfunktion einbetten. Dafür definieren wir das \textit{Sparse PCA Kriterium} mit den Hyperparametern $\lambda_{1,j}$ und $\lambda_2$
\begin{align}
\label{spca_criterion}
\begin{split}
(\widehat{\mat{A}}, \widehat{\mat{B}}) = \argmin_{\mat{A}, \mat{B}} \sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}_2^2 + \lambda_2 \sum_{j=1}^{k}\norm{\beta_j}_2^2 + \sum_{j=1}^k \lambda_{1,j} \norm{\beta_j}_1\\
\text{unter der Nebenbedingung, dass } \mat{A}^T\mat{A} = I_{k \times k}.
\end{split}
\end{align}
Die normierten Spalten von $\mat B$ nennen wir die \textit{dünnbesetzten Hauptachsen}. Um die Dünnbesetzung für jede Hauptachse unterschiedlich wählen zu können, erlauben wir unterschiedliche Bestrafungen $\lambda_{1,j}$. Im Gegensatz dazu lassen wir für den Ridge-Strafterm keine differenzierte Behandlung zu, der für die Reduktion von (\ref{spca_criterion}) auf (\ref{pca_regression_formulation}) benötigt wird falls $\lambda_{1,j} = 0$. Allerdings hat die $\ell_2$-Bestrafung noch einen weiteren Vorteil, welcher in der Praxis relevant ist. Es bewältigt das Lasso-Defizit, so dass auch mehr als $n$ Variablen im Fall $p>n$ ausgewählt werden können.

Wir möchten darauf hinweisen, dass im Gegensatz zu manch anderen Varianten der dünnbesetzten Hauptkomponentenanalyse (\ref{spca_criterion}) eine zeitgleiche anstatt einer sequentiellen Berechnung der Hauptachsen ermöglicht. Dies wird im folgendem Abschnitt von entscheidender Bedeutung sein.



%----------------------------------------------------------------------------------------
%	Anpassung der Transformation, Residuen und Varianzen
%----------------------------------------------------------------------------------------


\section{Anpassung der Transformation, Residuen und Varianzen}
\label{adjustment_of_variances}

Bei der Verwendung der dünnbesetzten Hauptkomponentenanalyse übertragen sich viele der Eigenschaften der klassischen Variante nicht \cite{camacho}. Daher gilt es folgende Punkte zu berücksichtigen.

\subsubsection{Korrelation der Hauptkomponenten}

Bei einer klassischen Hauptkomponentenanalyse sind die Hauptkomponenten aufgrund der orthogonalen Hauptachsen unkorreliert. Letztere Eigenschaft fordern wir bei der dünnbesetzten Variante in (\ref{spca_criterion}) nicht, so dass durchaus starke Korrelation zwischen den Hauptkomponenten auftreten kann. Während dies eine flexiblere Modellierung ermöglicht, wird eine geeignete Visualisierung schwieriger. Besonders bei der Verwendung von Streudiagrammen, welche genutzt werden, um den Beitrag der Ausgangsvariablen zu den Hauptachsen zu visualisieren, kann dies zu Problemen führen. Hierbei unterstellt man die Orthogonalität der Hauptachsen, was zu Verzerrungen der Distanzen im Bild führen kann \cite{geladi}. Des Weiteren kann die Berechnung der erfassten Varianz des Datensatzes, welches häufig als Maß für die Qualität eines Modells genutzt wird, nicht analog zur klassischen Variante durchgeführt werden.

\subsubsection{Varianzverlust der Hauptkomponenten}

Der Erfolg der Hauptkomponentenanalyse beruht vor allem darauf, dass die Hauptkomponenten optimal bezüglich der erklärten Varianz ist. Oft kann ein Großteil an Information eines Datensatzes durch eine geringe Anzahl an Hauptkomponenten beschrieben werden, welches die Komplexität hochdimensionaler Daten verringert. Bei der dünnbesetzten Hauptkomponentenanalyse opfern wir einen Teil der erklärten Varianz für simplere, einfacher zu interpretierende Hauptachsen. Um einen genauso großen Teil an Information des Datensatzes zu erklären, benötigen wir daher eine größere Anzahl an Hauptkomponenten in unserem Modell. Somit können wir die Dimension des Datensatzes unter Umständen nicht all zu stark reduzieren.

Camacho et al. zeigen, dass viele der Varianten der dünnbesetzten Hauptkomponentenanalyse bezüglich dieser Aspekte Probleme aufweisen. Insbesondere wurde die Berechnung der Hauptkomponenten, Residuen und der erklärten Varianz bislang falsch durchgeführt. Wir möchten an dieser Stelle die Unterschiede detailliert erklären.

Typischerweise wurden die Hauptkomponenten $\widehat{\mat Z}$ bislang wie bei der klassischen Hauptkomponentenanalyse berechnet, indem man $\widehat{\mat Z} = \mat X \widehat{\mat B}$ setzt \cite{zou_sparsepca}. Allerdings vernachlässigt man in diesem Fall, dass die Hauptachsen nicht orthogonal zueinander sind. Dies wird deutlich, wenn wir die Hauptkomponentenanalyse wie in (\ref{pca_minimize_reconstruction_error}) als eine bestmögliche Rekonstruktion der Datenmatrix $\mat X$ auffassen.
\begin{align}
\label{pca_scores_loadings_model}
\mat X = \widehat{\mat Z} \widehat{\mat B}^T + \widehat{\mat E},
\end{align}
wobei $\widehat{\mat E}$ die Matrix der Residuen ist. Ist uns eine volle Rang Approximation $\mat X = \widehat{\mat Z} \widehat{\mat B}^T$ gegeben, können wir beide Seiten mit $\widehat{\mat B}$ multiplizieren, um die Hauptkomponenten zu erhalten $\mat X \widehat{\mat B} = \widehat{\mat Z} \widehat{\mat B}^T \widehat{\mat B} = \widehat{\mat Z}$. Letzterer Schritt ist aber nur gültig, falls die Hauptachsen orthogonal zueinander sind. Daher muss bei der dünnbesetzten Variante mit der Moore-Penrose-Inversen $(\widehat{\mat B}^T \widehat{\mat B})^+$ korrigiert werden. Demnach sollten die Hauptkomponenten durch
\begin{align}
\label{corrected_scores_sparse_pca}
\widehat{\mat Z} = \mat X \widehat{\mat B}^T (\widehat{\mat B}^T \widehat{\mat B})^+
\end{align}
berechnet werden. Wir möchten an dieser Stelle anmerken, dass durch (\ref{corrected_scores_sparse_pca}) keine sequentielle Berechnung der Hauptkomponenten mehr möglich ist. Es können also nicht ohne weiteres mehr Hauptkomponenten zum Modell hinzugefügt werden, da jede Hauptkomponente von allen Hauptachsen abhängt und somit eine Neuberechnung erfordert. Im Gegenzug zeigen Camacho et al. empirisch, dass die Korrelation zwischen den Hauptkomponenten durch (\ref{corrected_scores_sparse_pca}) deutlich sinkt, was für viele Anwendungen von Vorteil sein kann.

Für die Modellbewertung wird oft der Anteil erklärter Varianz des Datensatzes herangezogen, wie in Abschnitt \ref{selection_principal_components} beschrieben. Zou et al. erkennen, dass aufgrund der Korrelation der Hauptkomponenten die Varianzen nicht wie gewohnt errechnet werden können und schlagen folgende Methode vor. Die erklärte Varianz für die ersten $j+1$ Hauptkomponenten sollte sich aus der Summe der ersten $j$ und der erklärten Varianz der $j+1$-ten Hauptkomponente $\widehat{Z}_{j+1}$ ergeben. Aufgrund der Korrelation erhält die Varianz von $\widehat{Z}_{j+1}$ aber Beiträge anderer Hauptkomponenten. Um nur die zusätzlich durch $\widehat{Z}_{j+1}$ erhaltene Varianz zu erhalten und lineare Abhängigkeiten zu entfernen, nutzen Zou et al. eine Projektion
\begin{align}
\label{zou_orthogonal_projection}
\widehat{Z}_{j\cdot 1, \ldots, j-1} = \widehat{Z}_j - \mat P_{1, \ldots, j-1}\widehat{Z}_j
\end{align}
wobei $\mat P_{1, \ldots, j-1}$ die orthogonale Projektionsmatrix auf $\{\widehat{Z}_i\}_1^{j-1}$ ist. Mit $\widehat{Z}_{j\cdot 1, \ldots, j-1}$ bezeichnen wir also die Residuen nach Anpassung von $\widehat{Z}_j$ durch $\widehat{Z}_1, \ldots, \widehat{Z}_{j-1}$. Man beachte, dass (\ref{zou_orthogonal_projection}) von der Reihenfolge der $\widehat{Z}_i$ abhängt. Aufgrund der natürlichen Ordnung bei der Hauptkomponentenanalyse stellt dies aber kein Problem dar.
Somit ergibt sich die Gesamtvarianz der ersten $k$ Hauptkomponenten durch 
\begin{align}
\label{zou_explained_variance}
\sum_{j=1}^k \norm{\widehat{Z}_{j\cdot 1, \ldots, j-1}}^2.
\end{align}
Mithilfe einer QR-Zerlegung von $\widehat{\mat Z} = \mat Q \mat R$, wobei $\mat Q$ orthonormal und $\mat R$ eine recht obere Dreiecksmatrix ist, können wir (\ref{zou_explained_variance}) schnell berechnen, denn $\norm{\widehat{Z}_{j\cdot 1, \ldots, j-1}}^2 = R_{jj}^2$. Auch wenn dieser Ansatz zunächst sinnvoll scheinen mag, werden wir in Kapitel \ref{application} anhand unseres Datensatzes zeigen, dass durch (\ref{zou_explained_variance}) keine korrekte Berechnung der erklärten Varianz erfolgt. Das Problem des Ansatzes liegt darin, dass der Bezug zum Rekonstruktionsfehler unklar ist. Anders als bei der klassischen Hauptkomponentenanalyse stimmen erklärte Varianz und Rekonstruktionsfehler bei der dünnbesetzten Variante nicht mehr überein.

Eine korrekte Methode wird von Camacho et al. eingeführt. Hierbei zerlegen wir die Varianz des Modells $\mat X = \widehat{\mat Z} \widehat{\mat B}^T + \widehat{\mat{E}}$ in zwei Teile.
\begin{align}
\label{camacho_explained_variance}
\norm{\mat X}_F^2 & = \norm{\widehat{\mat{Z}} \widehat{\mat{B}}^T + \widehat{\mat{E}}}_{F}^{2} \nonumber\\
& = \spur{\widehat{\mat{B}} \widehat{\mat{Z}}^T \widehat{\mat{Z}} \widehat{\mat{B}}^T} + \spur{\widehat{\mat{B}} \widehat{\mat{Z}}^T \widehat{\mat{E}}} + \spur{\widehat{\mat{E}}^T\widehat{\mat{Z}} \widehat{\mat{B}}^T} + \spur{\widehat{\mat{E}}^T \widehat{\mat{E}}} \nonumber\\
& = \spur{\widehat{\mat{B}} \widehat{\mat{Z}}^T \widehat{\mat{Z}} \widehat{\mat{B}}^T} + \spur{\widehat{\mat{E}}^T \widehat{\mat{E}}} \nonumber\\
& = \norm{\widehat{\mat{Z}} \widehat{\mat{B}}^T}_{F}^{2} + \norm{\widehat{\mat{E}}}_{F}^{2}
\end{align}
Damit kann die Varianz eines Datensatzes in Rekonstruktion und Residuum aufgeteilt werden. Ersterer Teil entspricht der erklärten Varianz unseres Modells und wird daher mit $\norm{\widehat{\mat{Z}} \widehat{\mat{B}}^T}_{F}^{2}$ berechnet. 


%----------------------------------------------------------------------------------------
%	Wahl der Hyperparameter
%----------------------------------------------------------------------------------------

\section{Wahl der Hyperparameter}
\label{choice_of_tuning_parameters}

Bei der dünnbesetzten Hauptkomponentenanalyse sind mehrere Hyperparameter zu wählen. Dazu gehören die Anzahl an Hauptkomponenten $k$ und die Regularisierungsparameter $\lambda_{1,j}$ und $\lambda_2$. Im Folgenden möchten wir verschiedene Vorgehensweise näher erläutern und eine Übersicht über mögliche Verfahren geben. Bevor man die Regularisierungsparameter festlegt, ist es sinnvoll zunächst die Anzahl an Hauptkomponenten für das Modell zu bestimmen, da sich bei einer Änderung von $k$ alle Hauptkomponenten verändern können. Hierbei kann man analog zur klassischen Variante wie in Abschnitt \ref{selection_principal_components} vorgehen. Aufgrund der Recheneffizienz bestimmen wir $k$ aber meist durch das Ergebnis der klassischen Variante.

Empirische Studien zeigen, dass sich die Ergebnisse bei Veränderung von $\lambda_2$ kaum ändern. Ist $n > p$ für unseren Datensatz, können wir den Parameter auf Null setzen, da das Lasso-Defizit in diesem Fall nicht auftritt. In der Praxis wird $\lambda_2$ auf eine kleine positive Zahl in der Größenordnung $10^{-6}$ gesetzt, um mögliche Kollinearitätsprobleme zu vermeiden \cite{zou_sparsepca}. Falls $p \gg n$ werden wir eine spezielle Wahl von $\lambda_2$ in Kapitel \ref{implementation} treffen.

Komplizierter gestaltet sich eine Wahl von $\lambda_{1,j}$, welche die Modellkomplexität wesentlich beeinflusst. Im Prinzip könnte man die $\lambda_{1,j}$ durch ein Kreuzvalidierungsverfahren bestimmen. Je nach Größe des Datensatzes kann dies aber sehr rechenintensiv sein, weshalb wir hier einen alternativen Ansatz beschreiben möchten. In der Literatur wird meist ein Bayes-Informationskriterium (BIC) angegeben, welches aber je nach Anwendung und Generalisierung verschieden formuliert wird. Folgende Variante beruht auf \cite{hubert, allen}
\begin{align}
\label{bic_reconstruction}
\operatorname{BIC}(\lambda_{1,j}) = \log\left(\frac{\norm{X - Z_j\beta_j^T}_{F}^{2}}{np}\right) + \operatorname{df}(\lambda_{1,j}) \frac{\log(np)}{np},
\end{align}
wobei $\operatorname{df}(\lambda_{1,j}) = \norm{\beta_j}_0$ die Anzahl von null verschiedener Einträge sind. Dabei steht $\operatorname{df}$ für \textit{degrees of freedom}, welches die Anzahl freier Parameter darstellt \cite{hastie_elements}. Klar erkennbar in (\ref{bic_reconstruction}) ist der Kompromiss zwischen Rekonstruktionsfehler der $j$-ten Hauptkomponente und der Dünnbesetzung durch $\operatorname{df}(\lambda_{1,j})$. Mit steigendem $\lambda_{1,j}$ wird der Rekonstruktionsfehler größer und die Anzahl freier Parameter geringer. Wir sind angehalten $\lambda_{1,j}$ zu finden, welche eine Balance zwischen den beiden Termen ermöglichen.

Um nicht über $k$ Parameter optimieren zu müssen, kann man $\lambda_{1,j} = \lambda_1$ für alle $1 \leq j \leq k$ setzen. Eine weitere Möglichkeit wird in \cite{croux} beschrieben. Für $j>1$ sei $\mat{\widehat{B}}_{j-1}^{\perp}$ die Matrix, deren Spalten eine orthonormale Basis für das orthogonale Komplement für den durch $\widehat{\beta}_1, \ldots, \widehat{\beta}_{j-1}$ aufgespannten Raum sind. Wir berechnen $x_i^{(j-1)} = (\mat{\widehat{B}}_{j-1}^{\perp})^T x_i$ für $i = 1, \ldots, n$ und setzen $\lambda_{1,j} = \lambda_1\operatorname{Var}[\mat X^{(j)}]$, wobei $\mat X^{(j)}$ aus den auf das orthogonale Komplement der ersten $j-1$ Hauptachsen projizierten Daten $x_i^{(j-1)}$ besteht. Mithilfe dieses Ansatzes können wir eine vergleichbare Dünnbesetzung für alle Hauptachsen erreichen. Nun muss lediglich $\lambda_1$ gewählt werden. Hierfür wird von \cite{croux, guo} ein ähnliches BIC-Kriterium vorgeschlagen
\begin{align}
\label{bic_ratio}
\operatorname{BIC}(\lambda_1) = \frac{\norm{\mat X - \mat X \widehat{\mat{B}} \widehat{\mat A}^T}_{F}^{2}}{\norm{\mat X - \mat X \widehat{\mat V} \widehat{\mat V}^T}_{F}^{2}} + \operatorname{df}(\lambda_1) \frac{\log(n)}{n}.
\end{align}
Für die log-likelihood-Funktion wird in (\ref{bic_ratio}) das Verhältnis des Rekonstruktionsfehler zwischen dünnbesetzter und der klassischer Hauptkomponentenanalyse gewählt. Welche der beiden BIC-Kriterien genutzt werden sollte, kommt auf den Anwendungsfall an.

Typischerweise wird für die Minimierung der BIC-Kriterien eine Rastersuche für $\lambda_1$ im Wertebereich $[0, \lambda_1^{max}]$ durchgeführt, wobei eine Wahl von $\lambda_1^{max}$ in Hauptachsen mit nur einem von Null verschiedenem Eintrag resultieren. Andere Suchverfahren wie die Zufallssuche, Bayessche oder gradienbasierte Optimierung sind an dieser Stelle denkbar. 


%----------------------------------------------------------------------------------------
%	Theoretische Aussagen
%----------------------------------------------------------------------------------------


\section{Theoretische Aussagen} 
\label{spca_theorems}


Zu Abschluss dieses Kapitels möchten wir uns mit theoretischen Aussagen zur dünnbesetzten Hauptkomponentenanalyse auseinandersetzen. Von wesentlicher Bedeutung ist die Konsistenz der Methode im Vergleich zur klassischen Variante für hochdimensionale Datensätze.

In Kapitel \ref{implementation} werden wir sehen, dass das Sparse PCA Kriterium nur von der Kovarianzmatrix abhängt. Um eine Populationsversion der dünnbesetzen Hauptkomponete zu erhalten, ersetzen wir wie zuvor $\mat X^T \mat X$ durch die Kovarianzmatrix $\mat \Sigma$. Wir wenden uns nun wieder einem \textit{single component model} zu
\begin{align}
\label{single_component_model_spca}
x_i = l_j v_1 + \sigma r_i \quad i = 1, \ldots n,
\end{align}
in welchem $v_1 \in \mathbb{R}^p$ der zu schätzende Eigenvektor ist. Dabei sind $l_i \sim N(0,1)$ unabhängig gleichverteilte Gaußsche Zufallseffekte und $r_i \sim N_p(0, \mat I)$ unabhängige Rauschvektoren. Johnstone und Lu \cite{johnstone} zeigen ein Resultat für die Konsistenz der Hauptkomponentenanalyse, falls diese auf eine Teilmenge der Variablen angewendet wird. Dies liefert eine theoretische Rechtfertigung der dünnbesetzten Variante.

Wir bezeichnen mit $\widehat{I} = \{j \, \colon \, \widehat{\sigma}_j^2 \geq \sigma^2 (1 + \alpha_n)\}$ die Indexmenge der zu selektierenden Variablen, wobei $\widehat{\sigma}_j^2$ die Stichprobenvarianz der $j$-ten Variable und $\alpha_n = \alpha (n^{-1}\log(\max \{n,p\}))^{\frac{1}{2}}$ für ein hinreichend großes $\alpha$ ist. Da die großen Einträge von $\widehat{v}_1$ mit Variablen großer Varianz korrespondieren, wählen wir 
$$\widehat{v}_{1j}^I = \begin{cases} 
      \widehat{v}_{1j} & j \in \widehat{I}\\
      0 & j \not\in \widehat{I}
   \end{cases}$$
Mit dieser Auswahl erhalten wir ein Konsistenz-Theorem unter geeigneten Modellannahmen.

\begin{thm}[Konsistenz der dünnbesetzten Hauptkomponentenanalyse bei konstantem asymptotischem Verhältnis von $n$ und $p$ \cite{johnstone}]
Für $n \to \infty$ sei die Signalstärke $\norm{v_1}^2 \to \rho$ asymptotisch stabil und $n^{-1}\log(\max \{n,p\}) \to 0$. Bezeichne mit $v_i^{(r)}$ den $r$-größten Eintrag von $v_i$. Unter der Annahme, dass die Einträge der Hauptachsen für alle $n$ schnell abfallen, d.h.
$$|v_1^{(r)}| \leq Cr^{\frac{-1}{q}} \quad \text{für } q \in (0,2) \text{ und } c < \infty$$
ist die dünnbesetzte Hauptkomponentenanalyse fast sicher konsistent
$$\mathbb{P}\left[\lim_{n \to \infty} \operatorname{angle}(\widehat{v}_1^I, v_1) = 0\right] = 1.$$ 
\end{thm}

In einer \textit{high dimension low sample size (HDLSS)} Situation zeigen Shen et al. mithilfe eines spiked covariance models die Konsistenz einer ähnlichen Variante von Sparse PCA \cite{shen}. Der einzige Unterschied zu dem Ansatz von Zou et al. ist, dass die Hauptachsen dort sequentiell statt simultan berechnet werden. Für Details bezüglich der speziellen Bedingungen für eine Konsistenz verweisen wir auf \cite{shen_consistency}.