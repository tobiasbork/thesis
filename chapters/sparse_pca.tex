% Main chapter title
\chapter{Dünnbesetzte Hauptkomponentenanalyse}

% Chapter label
\label{sparse_pca}

Ein wesentlicher Nachteil der Hauptkomponentenanalyse besteht darin, dass sich die neuen Variablen aus einer Linearkombination \textit{aller} bestehenden Variablen zusammensetzt. Dies erschwert besonders für hochdimensionale Daten eine Interpretation der Hauptachsen. Während zuvor jede Variable eine Bedeutung hatte, sind wir nach der Transformation meist nicht in der Lage den Hauptachsen eine Bedeutung im Kontext zuzuweisen. Um zu verstehen, was die Hauptachsen im Modell repräsentieren kann es besonders hilfreich sein, wenn diese \textit{dünnbesetzt} sind, sich also nur aus wenigen Variablen zusammensetzen. Treffen wir irgendwelche Annahmen? Des Weiteren ist nicht jede Variable relevant zur Strukturerkennung. impose extra constraints, which sacrifices some variance in order to improve interpretability. Interpretation ist oberstes Ziel!!!

shape/image analysis, ecological studys und neuroscience data application.

Zu Anfang dieses Kapitels werden wir eine naheliegende mathematische Formulierung des Problems beschreiben. Leider wird sich diese als NP-vollständig herausstellen, weshalb wir in Abschnitt \ref{relaxation} verschiedene Wege aufzeigen, dass Problem zu relaxieren. In \ref{sparse_pca_construction} möchten wir uns mit einem dieser Ansätze intensiv beschäftigen, welcher den Ausgangspunkt für den weiteren Verlauf dieser Arbeit darstellt. Wir haben uns dazu entschieden, den von Zou, Hastie und Tibshirani in \cite{zou_sparsepca} eingeführten Ansatz für diesen Zweck zu verfolgen. Dieser gilt sicherlich zu der am meisten verbreiteten Variante der dünnbesetzten Hauptkomponentenanalyse. Kürzlich erschienenes Paper, Neuerungen. Der Rest dieses Kapitels ist den Details dieses Ansatzes gewidmet.


%----------------------------------------------------------------------------------------
%	Problemformulierung
%----------------------------------------------------------------------------------------


\section{Problemformulierung}
\label{problem_formulation}

Wir möchten nun Hauptachsen eines gegebenen Datensatzes identifizieren mit der Zusatzbedingung, dass diese dünnbesetzt sind. Die wohl einfachste Vorgehensweise ist, zuerst die gewöhnliche Hauptkomponentenanalyse durchzuführen und anschließend ein Schwellwertmethode auf die Hauptachsen anzuwenden. Hierbei vernachlässigt man alle Koeffizienten, die kleiner als ein bestimmter Schwellenwert sind, indem man sie auf 0 setzt. Eine solche Prozedur kann aber in vielen Fällen irreführend sein, unter welcher die Qualität der Ergebnisse leidet \cite{cadima}. Die Wichtigkeit einer Variable in den Hauptachsen wird nicht allein durch den Koeffizient bestimmt. Zu berücksichtigen sind unter anderem sowohl die Standardabweichung als auch die Korrelationen mit anderen Variablen. Bei einer Schwellwertmethode werden diese Faktoren nicht beachtet, weshalb den Ergebnissen im Allgemeinen nicht vertraut werden darf.

Hier Regression on ordinary PCA's mit Zou et al?

Anstelle eines zweischrittigen Ansatzes kann die Dünnbesetzung direkt in die Problemformulierung mit eingebaut werden. Gegeben sei dazu wieder eine Datenmatrix $\mat X \in \rnp$, wobei $n$ die Anzahl an Beobachtungen und $p$ die Anzahl an Variablen ist. Des Weiteren gehen wir davon aus, dass die Matrix $\mat X$ zuvor spaltenweise zentriert wurde. Dann kann die dünnbesetzte Hauptkomponentenanalyse als sukzessives Maximierungsproblem formuliert werden:
\begin{gather}
\label{sparse_pca_np}
\begin{split}
v_{k} = \argmax_{\norm{v}_2 = 1} v^{T}\mat{\Sigma} v\\
\text{unter der Nebendingung, dass für } k\geq 2 \, v_{k}^Tv_{l} = 0 \quad \forall 1 \leq l < k\\
\text{und } \norm{v_{k}}_0 \leq t 
\end{split}
\end{gather}
wobei $\mat{\Sigma} = \frac{\mat X^T \mat X}{n-1}$ die Stichprobenkovarianzmatrix ist. Der einzige Unterschied zur klassischen Hauptkomponentenanalyse, wie wir sie in (\ref{pca_variance_maximization_first}) beschrieben haben, besteht in der Einführung der $\ell_0$-Norm. Somit beschränken wir uns auf die Suche von Hauptachsen, welche höchstens $t$ von Null verschiedene Einträge haben. Wählen wir $t = p$ reduziert sich das Problem auf (\ref{pca_variance_maximization_first}). Während (\ref{sparse_pca_np}) eine sehr schöne und einfache mathematische Formulierung ist, wurde gezeigt, dass dieses Problem NP-vollständig ist \cite{foucart}. Zur Berechnung dünnbesetzter Hauptachsen sind wir also angehalten eine geeignete Relaxation zu finden.


%----------------------------------------------------------------------------------------
%	Relaxation
%----------------------------------------------------------------------------------------


\section{Relaxation}
\label{relaxation}

Es existiert eine Vielfalt an Ansätzen, um das Problem zu relaxieren. Wir wollen zunächst einen kleinen Überblick über die unterschiedlichen Ideen geben und uns anschließend mit einer genauer beschäftigen. Eine selektive Übersicht der verschiedenen Ansätze haben wir hier erstellt.\\

\textbf{SCoTLASS}

Inspiriert von der Lasso Regression \cite{tibshirani_lasso} schlugen Jolliffe et al. \cite{scotlass} vor, die $\ell_1$-Norm anstelle der $\ell_0$-Norm als Strafterm zu verwenden. Wie wir bereits in Abschnitt \ref{lasso} gesehen haben, kann die $\ell_1$-Norm genutzt werden, um dünnbesetzte Vektoren zu erhalten. Somit liegt es nahe das Problem wie folgt zu formulieren.
\begin{gather}
\label{scotlass}
\begin{split}
v_{k} = \argmax_{\norm{v}_2 = 1} v^{T}\mat{\Sigma} v\\
\text{unter der Nebendingung, dass für } k\geq 2 \, v_{k}^Tv_{l} = 0 \quad \forall 1 \leq l < k\\
\text{und } \norm{v_{k}}_1 \leq t 
\end{split}
\end{gather}
Wie in (\ref{sparse_pca_np}) hat man mit der Wahl der Parameters $t$ Einfluss auf die Dünnbesetzung der Hauptachsen. Aufgrund der hohen Berechnungskosten ist SCoTLASS allerdings für hochdimensionale Daten ungeeignet. Diese sind vor allem darauf zurückzuführen, dass (\ref{scotlass}) kein konvexes Optimierungsproblem ist. Des Weiteren ergeben sich Schwierigkeiten bei der Wahl des Hyperparameters $t$. Auch wenn eine passende Wahl eine gewünschte Dünnbesetzung hervorruft, gibt es kaum Orientierungshilfen. Zusätzlich hat ScoTLASS dasselbe grundlegende Problem wie das Lasso. Die Anzahl von null verschiedener Einträge ist durch die Anzahl Beobachtungen im Datensatz limitiert, welches die Brauchbarkeit des Modells deutlich einschränkt. Zusammen mit den hohen Berechnungskosten ist dieser Ansatz in der Praxis daher meist impraktikabel.\\

\textbf{Semidefinite Programmierung}

Konvexe Relaxation ist eine Standard-Technik, um mit schwierigen nichtkonvexen Problemen umzugehen. d'Aspremont et al. \cite{daspremont_semidefinite} entwickeln einen Ansatz, welcher sich als semidefinites Programmierungsproblem ausdrücken lässt. Zunächst werden wir (\ref{sparse_pca_np}) dafür mit Matrizen reformulieren.

Sei $\mat V = v_kv_k^{\top}$. Dann übersetzen sich die Nebenbedingungen 
\begin{align}
\label{semidefinite_programming_naive}
\begin{split}
\argmax_{\mat P} = \spur{\mat \Sigma\mat P}\\
\spur{\mat P} = 1, \quad \norm{\mat P}_0 \leq k^2, \quad \mat P \geq 0, \quad \rang{\mat P} = 1
\end{split}
\end{align}

Diese Formulierung ist noch immer nichtkonvex aufgrund der Rang-Bedingung  und der $\ell_0$-Strafterm. 
Per Definition ist $\mat P$ symmetrisch und $\mat P^2 = \mat P$. Somit ist
$$\norm{\mat P}_{F}^{2} = \spur{\mat P^{\top}\mat P} = \spur{\mat P} = 1$$
und mit der Cauchy-Schwarz-Ungleichung folgt
$$\mat{1}_p^{\top} |\mat P| \mat{1}_p \leq \sqrt{\norm{\mat{P}}_0 \norm{\mat{P}}_F^2} \leq k$$
Ersetzen wir die $\ell_0$-Strafterm und lassen die Rang-Bedingung fallen erhalten wir die DSPCA-Formulierung
\begin{align}
\label{semidefinite_programming}
\begin{split}
\argmax_{\mat P} = \spur{\mat \Sigma\mat P}\\
\spur{\mat P} = 1, \quad \mat{1}_p^{\top} |\mat P| \mat{1}_p \leq k, \quad \mat P \geq 0
\end{split}
\end{align}
Dies stellt ein semidefinites Programmierungsproblem dar, bei welcher die zu optimierenden Variablen symmetrische Matrizen sind unter der Nebenbedingung, dass sie positiv semidefinit sind. Für kleine Probleme kann (\ref{semidefinite_programming}) effizient durch \textit{Innere-Punkte-Verfahren} (English: \textit{interior-point methods}) gelöst werden. SDPT3 \cite{toh}.

In (\ref{semidefinite_programming}) wird allerdings $\mat P$ berechnet und nicht die eigentliche Hauptachse. Hierfür kürzen d'Aspremont et al. die Matrix $\mat P$ und behalten nur den größten Eigenvektor $v_k$. Anschließend erhält man weitere Hauptachsen durch Matrix Deflation, indem wir $\mat \Sigma$ durch
$$\mat \Sigma - (v_k^{\top}\mat \Sigma v_k) v_kv_k^{\top}$$ 
ersetzen. Für größere Probleme wird eine Methode von Nesterov benutzt, um eine Laufzeit von $\mathcal{O}(\frac{p^4\sqrt{\log{p}}}{\epsilon})$ zu erreichen.\\

\textbf{Iterative Schwellenwert-Methode}

Basierend auf der Formulierung (\ref{pca_best_rank_approximation}) der Hauptkomponentenanalyse als beste Rang $k$ Approximation an die Datenmatrix $\mat X$ haben Shen und Huang \cite{shen} das folgende Optimierungsproblem formuliert
\begin{align}
\label{iterative_thresholding}
\begin{split}
(u_1, v_1) = \argmin_{u, v} \norm{\mat X - u v^{\top}}_{F}^{2}  + \lambda \norm{v}_{1}\\
\norm{u}_2 = 1
\end{split}
\end{align}
Somit erhält man mit $\frac{v_1}{\norm{v_1}}$ die erste dünnbesetzte Hauptachse. Auch hier werden die restlichen Hauptachsen sequentiell berechnet durch Ersetzen der Datenmatrix $\mat X_{(k+1)} = \mat X - \sum_{i=1}^k u_iv_i^{\top}$. Jede Iteration kann durch ein alternierendes Minimierungsverfahren gelöst werden. Fixiert man $v$, so ist das optimale $u$ gegeben durch $u = \frac{\mat Xv}{\norm{\mat Xv}}$. Andererseits reduziert sich (\ref{iterative_thresholding}) für festes $u$ auf
$$\argmin_{v} -2\spur{\mat X^{\top}uv^{\top}} + \norm{v}^{2} + \lambda \norm{v}_1.$$
Eine explizite Lösung ist durch den soft-thresholding Operator gegeben
$$v = \operatorname{soft}_{\frac{\lambda}{2}}(\mat X^{\top} U)$$
welcher in Abschnitt \ref{generalized_linear_models} eingeführt worden ist.

Diese Methode ist sehr ähnlich zu der von Zou et al. \cite{zou_sparsepca} , mit welcher wir uns im folgenden Abschnitt beschäftigen werden. Der große Unterschied besteht darin, dass die Hauptachsen dort nicht sequentiell, sondern gleichzeitig berechnet werden. Witten et al. haben in \cite{witten} ebenfalls eine Methode entwickelt, die unter diese Kategorie fällt.\\

\textbf{Weitere Relaxationsideen}

Es gibt noch eine Reihe weiterer Ideen, die in der Literatur betrachtet wurden. Ein interessierter Lesen sei auf die folgenden Ansätze verwiesen.
\begin{itemize}
\item eine verallgemeinerte Potenzmethode \cite{journee}
\item ein alternierendes Maximierungs-Netzwerk \cite{richtarik}
\item Vorwärts und Rückwärts-Greedy-Suche mittels Branch-and-Bound-Verfahren \cite{moghaddam}
\item eine Bayes-Formulierung \cite{guan}
\end{itemize}


%----------------------------------------------------------------------------------------
%	Konstruktion Sparse PCA
%----------------------------------------------------------------------------------------


\section{Konstruktion}
\label{construction}

Wir werden uns nun mit dem von Zou, Hastie und Tibshirani in \cite{zou_sparsepca} eingeführten Ansatz ausführlich beschäftigen. Zou und Hastie führten zuvor in \cite{zou_elasticnet} das sog. \textit{elastic net} ein, welches den Grundstein für die mathematische Formulierung legen wird.

Wie bereits in (\ref{pca_regression_formulation}) beschrieben kann die Hauptkomponentenanalyse als regressionsartiges Problem betrachtet werden. Das folgende Theorem erweitert die bisherige Formulierung, indem nun nicht ausschließlich orthogonale Projektionen erlaubt werden. 
Im Folgenden bezeichnet $k$ die Anzahl an Hauptkomponenten, die wir extrahieren möchten und $x_i$ die $i$-te Zeile von $\mat X$. Wir bezeichnen mit $\mat B$ im Folgenden die dünnbesetzten Hauptachsen, um sie von den klassischen Hauptachsen $\mat V$ zu unterscheiden.

\begin{thm} \label{pca_regression_formulation_ridge}
Sei $\mat{A}_{p \times k} = [ \alpha_1, \ldots ,\alpha_k ]$ und $\mat{B}_{p \times k} = [ \beta_1, \ldots ,\beta_k ]$. Für ein $\lambda_2 > 0$ sei
$$(\hat{\mat{A}}, \hat{\mat{B}}) = \argmin_{\mat{A}, \mat{B}} \sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}^2 + \lambda_2 \sum_{j=1}^{k}\norm{\beta_j}^2$$
$$\text{wobei } \mat{A}^T\mat{A} = I_{k \times k}.$$
Dann ist $\hat{\beta}_j \propto v_j$ für $j = 1,2,\ldots,k$. 
\end{thm}

Fordern wir $\mat A =  \mat B$ reduziert sich die Verlustfunktion $\sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}^2$ auf die klassische Hauptkomponentenanalyse (\ref{pca_regression_formulation}). Theorem \ref{pca_regression_formulation_ridge} zeigt, dass wir die Bedingung $\mat A = \mat B$ unter Einführung eines Ridge-Strafterms vernachlässigen können. Mithilfe dieser Verallgemeinerung können wir die Hauptkomponentenanalyse flexibel modifizieren.

Um dünnbesetzte Hauptachsen zu erhalten können wir einen $\ell_1$-Strafterm in die Zielfunktion einbetten. Das ein solcher Strafterm eine gewünschte Dünnbesetzung hervorruft, haben wir in Abschnitt \ref{lasso} beobachten können. Wir definieren daher das \textit{Sparse PCA Kriterium} mit den Hyperparameter $\lambda_{1,j}$ und $\lambda_2$
\begin{align}
\label{spca_criterion}
\begin{split}
(\hat{\mat{A}}, \hat{\mat{B}}) = \argmin_{\mat{A}, \mat{B}} \sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}_2^2 + \lambda_2 \sum_{j=1}^{k}\norm{\beta_j}_2^2 + \sum_{j=1}^k \lambda_{1,j} \norm{\beta_j}_1\\
\text{unter der Nebenbedingung, dass } \mat{A}^T\mat{A} = I_{k \times k}
\end{split}
\end{align}
Die normierten Spalten von $\mat B$ nennen wir dann die \textit{dünnbesetzten Hauptachsen}. Um die Dünnbesetzung für jede Hauptachse unterschiedlich wählen zu können, erlauben wir unterschiedliche Bestrafungen $\lambda_{1,j}$. Dagegen erlauben wir für die Ridge-Bestrafung $\lambda_2$, die im Wesentlichen für die Reduktion von (\ref{spca_criterion}) auf (\ref{pca_regression_formulation}) benötigt wird falls $\lambda_{1,j} = 0$, keine differenzierte Behandlung. Allerdings hat die $\ell_2$-Bestrafung noch einen weiteren Vorteil, welcher in der Praxis relevant ist. Es bewältigt das Lasso-Defizit, so dass auch mehr als $n$ Variablen im Fall $p>n$ ausgewählt werden können.

Wir möchten darauf hinweisen, dass im Gegensatz zu manch anderen Varianten der dünnbesetzten Hauptkomponentenanalyse (\ref{spca_criterion}) eine zeitgleiche anstatt einer sequentiellen Berechnung der Hauptachsen ermöglicht. Dies wird im folgendem Abschnitt von entscheidender Bedeutung sein.



%----------------------------------------------------------------------------------------
%	Anpassung der Transformation, Residuen und Varianzen
%----------------------------------------------------------------------------------------


\section{Anpassung der Transformation, Residuen und Varianzen}

Bei der Verwendung der dünnbesetzten Hauptkomponentenanalyse übertragen sich viele der Eigenschaften der klassischen Variante nicht. \cite{camacho}. Daher gilt es folgende Punkte zu berücksichtigen.\\

\textbf{Korrelation der Hauptkomponenten}

Bei einer klassischen Hauptkomponentenanalyse sind die Hauptkomponenten aufgrund der orthogonalen Hauptachsen unkorreliert. Letztere Eigenschaft fordern wir bei der dünnbesetzten Variante in (\ref{spca_criterion}) nicht, so dass durchaus starke Korrelationen zwischen den Hauptkomponenten auftreten können. Während dies eine flexiblere Modellierung ermöglicht, wird es dadurch schwieriger die Ergebnisse geeignet zu visualisieren. Besonders bei der Verwendung von Streudiagrammen, welche genutzt werden, um den Beitrag der Ausgangsvariablen zu den Hauptachsen zu visualisieren, kann dies zu Problemen führen. Hierbei unterstellt man die Orthogonalität der Hauptachsen, was zu Verzerrungen der Distanzen im Bild führen kann \cite{geladi}. Des Weiteren kann die Berechnung der erfassten Varianz des Datensatzes, welches häufig als Maß für die Qualität eines Modells genutzt wird, nicht analog zur klassischen Variante durchgeführt werden.\\

\textbf{Varianzverlust der Hauptkomponenten}

Der Erfolg der Hauptkomponentenanalyse beruht vor allem darauf, dass die Hauptkomponenten optimal bezüglich erklärter Varianz sind. Oft kann ein Großteil an Information eines Datensatzes durch eine geringe Anzahl an Hauptkomponenten beschrieben werden, welches eine Visualisierung und Interpretation hochdimensionaler Daten ermöglicht. Bei der dünnbesetzten Hauptkomponentenanalyse opfern wir einen Teil der erklärten Varianz für simplere, einfacher zu interpretierende Hauptachsen. Um einen genauso großen Teil an Information des Datensatzes zu erklären, benötigen wir daher eine größere Anzahl an Hauptkomponenten in unserem Modell. Somit können wir die Dimension des Datensatzes unter Umständen nicht all zu stark reduzieren.

In einem erst vor Kurzem erschienen wissenschaftlichen Artikel zeigen Camacho et al. \cite{camacho}, dass viele der Varianten der dünnbesetzten Hauptkomponentenanalyse bezüglich der genannten Punkte Probleme aufweisen. Insbesondere wurde die Berechnung der Hauptkomponenten, Residuen und der erklärten Varianz bislang falsch durchgeführt. Wir möchten an dieser Stelle die Unterschiede detailliert erklären.

Typischerweise wurden bislang die Hauptkomponenten $\mat Z$ wie bei der klassischen Hauptkomponentenanalyse berechnet, indem man $\mat Z = \mat X \mat B$ setzt, wobei $\mat B$ die Matrix der dünnbesetzten Hauptachsen ist \cite{zou_sparsepca}. Allerdings vernachlässigt man in diesem Fall, dass die Hauptachsen nicht orthogonal zueinander sind. Dies wird deutlich, wenn wir die Hauptkomponentenanalyse wie in (\ref{pca_minimize_reconstruction_error}) als eine bestmögliche Rekonstruktion der Datenmatrix $\mat X$ auffassen.
\begin{align}
\label{pca_scores_loadings_model}
\mat X = \mat Z \mat B^T + \mat E,
\end{align}
wobei $\mat E$ die Matrix der Residuen ist, welche uns erst nach der Berechnung von $\mat Z$ und $\mat B$ zur Verfügung stehen. Im Folgenden sei durch $\mat X = \mat Z \mat B^T$ eine volle Rang Approximation gegeben. Bei der klassischen Hauptkomponentenanalyse multipliziert man beide Seiten mit $\mat B$, um die Hauptkomponenten $\mat X \mat B = \mat Z \mat B^T \mat B = \mat Z$ zu erhalten. Falls die Hauptachsen nicht orthogonal zueinander sind, ist der letzte Schritt nicht mehr gültig. Daher korrigiert man bei der dünnbesetzten Variante mit der Moore-Penrose-Inversen $(\mat B^T \mat B)^+$ ähnlich zu der Methode der kleinsten Quadrate. Demnach sollten die Hauptkomponenten durch
\begin{align}
\label{corrected_scores_sparse_pca}
\mat Z = \mat X \mat B^T (\mat B^T \mat B)^+
\end{align}
berechnet werden. Falls $\mat B$ orthogonal ist, reduziert sich (\ref{corrected_scores_sparse_pca}) wie gehabt auf $\mat Z = \mat X \mat B$. Ein weiterer Unterschied ist, dass nun keine sequentielle Berechnung der Hauptkomponenten mehr möglich ist. Jede Hauptkomponente hängt durch (\ref{corrected_scores_sparse_pca}) von allen Hauptachsen ab, weshalb sich die Werte bei Hinzunahme weiterer Hauptachsen zum Modell jedes mal ändern können. Camacho et al. zeigen empirisch, dass die Korrelation zwischen den Hauptkomponenten für der von uns betrachteten Variante der dünnbesetzten Hauptkomponentenanalyse deutlich sinkt, was eine Interpretation weiter verbessert.

Für die Modellbewertung wird oft die erklärte Varianz des Datensatzes herangezogen, wie in Abschnitt \ref{selection_principal_components} beschrieben. Zou et al. erkennen, dass aufgrund der Korrelation der Hauptkomponenten die Varianzen nicht wie gewohnt errechnet werden können und schlagen in \cite{zou_sparsepca} folgende Methode vor. Die erklärte Varianz für die ersten $j+1$ Hauptkomponenten sollte sich aus der Summe der ersten $j$ zusammen mit der erklärten Varianz der $k+1$-ten Hauptkomponente $Z_{j+1}$ ergeben. Aufgrund der Korrelation der Hauptkomponenten erhält die Varianz von $Z_{j+1}$ aber Beiträge anderer Hauptkomponenten. Um nur die zusätzlich durch $Z_{j+1}$ erhaltene Varianz zu erhalten und lineare Abhängigkeiten zu entfernen, nutzen Zou et al. eine Projektion
\begin{align}
\label{zou_orthogonal_projection}
Z_{j\cdot 1, \ldots, j-1} = Z_j - \mat P_{1, \ldots, j-1}Z_j
\end{align}
wobei $\mat P_{1, \ldots, j-1}$ die orthogonale Projektionsmatrix auf $\{Z_i\}_1^{j-1}$ sei. Mit $Z_{j\cdot 1, \ldots, j-1}$ bezeichnen wir also die Residuen nach Anpassung von $Z_j$ durch $Z_1, \ldots, Z_{j-1}$. Man beachte, dass (\ref{zou_orthogonal_projection}) von der Reihenfolge der $Z_i$ abhängt. Aufgrund der natürlichen Ordnung bei der Hauptkomponentenanalyse stellt dies aber kein Problem dar.
Somit ergibt sich die Gesamtvarianz der ersten $k$ Hauptkomponenten durch 
\begin{align}
\label{zou_explained_variance}
\sum_{j=1}^k \norm{Z_{j\cdot 1, \ldots, j-1}}^2.
\end{align}
Mithilfe einer QR-Zerlegung von $\mat Z = \mat Q \mat R$, wobei $\mat Q$ orthonormal und $\mat R$ eine recht obere Dreiecksmatrix ist, können wir (\ref{zou_explained_variance}) schnell berechnen, denn $\norm{Z_{j\cdot 1, \ldots, j-1}}^2 = R_{jj}^2$. Auch wenn dieser Ansatz zunächst sinnvoll scheinen mag, werden wir in Kapitel \ref{application} anhand unseres Datensatzes zeigen, dass durch (\ref{zou_explained_variance}) keine korrekte Berechnung der erklärten Varianz erfolgt. Das Problem des Ansatzes liegt daran, dass der Zusammenhang zum Rekonstruktionsfehler nicht klar ist. Bei der klassischen Hauptkomponentenanalyse haben wir die erklärte Varianz mithilfe des Rekonstruktionsfehlers angeben können, was hier nicht mehr der Fall sein muss. 

Eine korrekte Methode wird von Camacho et al. eingeführt. Hierbei zerlegen wir die Varianz des Modells $\mat X = \mat Z \mat B^T + \mat E$ in zwei Teile.
\begin{align}
\label{camacho_explained_variance}
\norm{\mat X}_F^2 & = \norm{\mat Z \mat B^T + \mat E}_{F}^{2} \nonumber\\
& = \spur{\mat B \mat Z^T \mat Z \mat B^T} + \spur{\mat B \mat Z^T \mat E} + \spur{\mat E^T\mat Z \mat B^T} + \spur{\mat E^T \mat E} \nonumber\\
& = \spur{\mat B \mat Z^T \mat Z \mat B^T} + \spur{\mat E^T \mat E} \nonumber\\
& = \norm{\mat Z \mat B^T}_{F}^{2} + \norm{\mat E}_{F}^{2}
\end{align}
If deflation is performed in the score space $EP = P^T E^T = 0$.
Damit kann die Varianz eines Datensatzes in die Varianz der Rekonstruktion und der Residuen aufgeteilt werden. Ersterer Teil entspricht der erklärten Varianz unseres Modells und wird daher mit $\norm{\mat Z \mat B^T}_{F}^{2}$ berechnet.


%----------------------------------------------------------------------------------------
%	Wahl der Hyperparameter
%----------------------------------------------------------------------------------------

\section{Wahl der Hyperparameter}
\label{choice_of_tuning_parameters}

Bei der dünnbesetzten Hauptkomponentenanalyse sind mehrere Hyperparameter zu wählen. Dazu gehören die Anzahl an Hauptkomponenten $k$ und die Regularisierungsparameter $\lambda_{1,j}$ und $\lambda_2$. Im Folgenden möchte wir mögliche Vorgehensweise näher erläutern und eine Übersicht über mögliche Verfahren geben. 

Bevor man die Regularisierungsparameter festlegt ist es sinnvoll zunächst die Anzahl an Hauptkomponenten für das Modell zu bestimmen, da sich bei einer Änderung von $k$ alle Hauptkomponenten verändern können. Hierbei kann man analog zur klassischen Variante wie in Abschnitt \ref{selection_principal_components} beschrieben vorgehen. Man nutzt also zunächst die klassische Variante, um $k$ zu bestimmen.

Empirische Ergebnisse zeigen, dass die Ergebnisse sich mit Veränderung von $\lambda_2$ kaum ändern. Für einen $n > p$ Datensatz kann der Parameter auf Null gesetzt werden, da das Lasso-Defizit in diesem Fall nicht auftritt. In der Praxis wird $\lambda_2$ auf eine kleine positive Zahl in der Größenordnung $10^{-6}$ gesetzt, um mögliche Kollinearitätsprobleme zu vermeiden \cite{zou_sparsepca}. Falls $p \gg n$ werden wir eine Standardwahl von $\lambda_2$ in Kapitel \ref{implementation} treffen.

Komplizierter gestaltet sich eine Wahl von $\lambda_{1,j}$, welche die Balance zwischen Dünnbesetzung und Rekonstruktionsfehler regelt. Es wird keine Sturres Ausprobieren in Zou et al. Alternativ wird vorgeschlagen, den zweischrittigen Prozess aus REF zu verwenden, bei welcher man einen gesamten Lösungspfad für die $\lambda_{i,j}$ erhält. Bei der Verwendung dieses Ansatzes kommen allerdings unterschiedliche Lösungne nrua.s

Im Prinzip könnte man die $\lambda_{1,j}$ durch ein Kreuzvalidierungsverfahren bestimmen. Je nach Größe des Datensatzes kann dies aber sehr rechenintensiv sein, weshalb wir hier einen alternativen Ansatz beschreiben möchten. In der Literatur wird meist ein Bayes-Informationskriterium (BIC) angegeben, welches aber je nach Anwendung und Generalisierung verschieden formuliert wird. Zwei Varianten, welche auf \cite{hubert, allen} (\ref{bic_reconstruction}) und \cite{croux, guo} (\ref{bic_ratio}) zurückgehen, werden wir hier betrachten.
\begin{align}
\label{bic_reconstruction}
\operatorname{BIC}(\lambda_{1,j}) = \log\left(\frac{\norm{X - Z_j\beta_j^T}_{F}^{2}}{np}\right) + \operatorname{df}(\lambda_{1,j}) \frac{\log(np)}{np}
\end{align}
wobei $\operatorname{df}(\lambda_{1,j}) = \norm{\beta_j}_0$ die Anzahl von null verschiedener Einträge sind. Dabei steht $\operatorname{df}$ für \textit{degrees of freedom}, welches die Anzahl freier Parameter bzw. den Freiheitsgrad darstellt \cite{hastie_elements}. Klar erkennbar in (\ref{bic_reconstruction}) ist der Kompromiss zwischen Rekonstruktionsfehler der $j$-ten Hauptkomponente und der Dünnbesetzung durch $\operatorname{df}(\lambda_{1,j})$. Mit steigendem $\lambda_{1,j}$ wird der Rekonstruktionsfehler größer und die Anzahl freier Parameter geringer. Wir sind angehalten $\lambda_{1,j}$ zu finden, die eine Balance zwischen den beiden Termen ermöglicht.

Um nicht über $k$ Parameter optimieren zu müssen, kann man beispielsweise $\lambda_{1,j} = \lambda_1$ für alle $1 \leq j \leq k$ setzen. Eine weitere Möglichkeit wird in \cite{croux} beschrieben. Für $j>1$ sei $\mat{\tilde{B}}_{j-1}^{\perp}$ die Matrix, deren Spalten eine orthonormale Basis für das orthogonale Komplement für den durch $\beta_1, \ldots, \beta_{j-1}$ aufgespannten Raum sind. Wir bezeichnen $x_i^{(j-1)} = (\mat{\tilde{B}}_{j-1}^{\perp})^T x_i$ für $i = 1, \ldots, n$ und setzen $\lambda_{1,j} = \lambda_1\operatorname{Var}[\mat X^{(j)}]$, wobei $\mat X^{(j)}$ aus den auf das orthogonale Komplement der ersten $j-1$ Hauptachsen projizierten Daten $x_i^{(j-1)}$ besteht. Mithilfe dieses Ansatzes können wir eine vergleichbare Dünnbesetzung für alle Hauptachsen erhalten. Für die Wahl von $\lambda_1$ wird ein ähnliches BIC-Kriterium vorgeschlagen.
\begin{align}
\label{bic_ratio}
\operatorname{BIC}(\lambda_1) = \frac{\norm{\mat X - \mat X \mat B \mat A^T}_{F}^{2}}{\norm{\mat X - \mat X \mat V \mat V^T}_{F}^{2}} + \operatorname{df}(\lambda_1) \frac{\log(n)}{n}
\end{align}
Für die log-likelihood-Funktion wird in (\ref{bic_ratio}) das Verhältnis zwischen Rekonstruktionsfehler der dünnbesetzten und der klassischen Hauptkomponentenanalyse gewählt. Welche der beiden BIC-Kriterien genutzt werden sollte, kommt auf den Anwendungsfall und den Rechenaufwand an, welchen man bereit ist in Kauf zu nehmen.

Typischerweise wird für die Minimierung der BIC-Kriterien eine Rastersuche für $\lambda_1$ im Wertebereich $[0, \lambda_1^{max}]$ durchgeführt, wobei eine Wahl von $\lambda_1^{max}$ in Hauptachsen mit nur einem von Null verschiedenem Eintrag resultieren. Andere Verfahren wie die Zufallssuche, Bayessche oder gradienbasierte Optimierung sind an dieser Stelle denkbar, um ein passendes $\lambda_1$ zu finden. 

Wichtig zu erwähnen ist, dass BIC-Kriterien sich in $p \gg n$-Situationen schlecht verhalten.
The BIC suffers from two main limitations[5]
the above approximation is only valid for sample size n much larger than the number k of parameters in the model.
the BIC cannot handle complex collections of models as in the variable selection (or feature selection) problem in high-dimension. 


%----------------------------------------------------------------------------------------
%	Theoretische Aussagen
%----------------------------------------------------------------------------------------


\section{Theoretische Aussagen} 
\label{spca_theorems}


Zu Abschluss dieses Kapitels möchten wir uns mit theoretischen Aussagen zur dünnbesetzten Hauptkomponentenanalyse auseinandersetzen.  Von wesentlicher Bedeutung ist die Konsistenz der Methode im Vergleich zur klassischen Variante.

Durch ref werden wir sehen, dass das Sparse PCA Kriterium nur von der Kovarianzmatrix abhängt. Um eine Populationsversion der dünnbesetzen Hauptkomponete zu erhalten ersetzen wir $\mat X^T \mat X$ schlicht durch die Kovarianzmatrix $\mat \Sigma$.





recomputation necessary of principal components when changing number of pc's.

Alexandre d’Aspremont, Optimal Solutions for Sparse Principal Component Analysis
We then use the same relaxation to derive sufficient conditions for global optimality of a
solution, which can be tested in O(n
3
) per pattern. We discuss applications in subset selection and
sparse recovery and show on artificial examples and biological data that our algorithm does provide
globally optimal solutions in many cases.