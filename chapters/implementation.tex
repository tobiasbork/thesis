% Main chapter title
\chapter{Implementierung}

% Chapter label
\label{implementation}

In diesem Kapitel werden wir einen Algorithmus beschreiben, der das Sparse PCA Kriterium (\ref{spca_criterion}) minimiert. Dazu werden wir uns zunächst mit dem allgemeinen Fall beschäftigen bevor wir den Fall $n \ll p$ genauer betrachten, um eine effiziente Berechnung zu garantieren. Zu Schluss dieses Kapitels werden wir uns genauer mit der Laufzeit des Algorithmus auseinandersetzen. 

\section{Numerische Lösung}
\label{spca_numerical_solution}

Zunächst möchten wir das Sparse PCA Kriterium erneut formulieren. Sei $\mat{A}_{p \times k} = [ \alpha_1, \ldots ,\alpha_k ]$ und $\mat{B}_{p \times k} = [ \beta_1, \ldots ,\beta_k ]$. Wie zuvor bezeichnen wir mit $x_i$ die $i$-te Zeile von $\mat X$. Dann lautet das Sparse PCA Kriterium

$$(\hat{\mat{A}}, \hat{\mat{B}}) = \argmin_{\mat{A}, \mat{B}} \sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}_2^2 + \lambda \sum_{j=1}^{k}\norm{\beta_j}_2^2 + \sum_{j=1}^k \lambda_{1,j} \norm{\beta_j}_1$$

$$\text{unter der Nebenbedingung, dass } \mat{A}^T\mat{A} = I_{k \times k}$$

Durch die Einführung der Matrix $\mat B$ in das Kriterium in Kapitel \ref{sparse_pca} wird über die beiden Matrizen $\mat A$ und $\mat B$ minimiert. Man kann zeigen, dass es sich bei (...) um ein nicht-konvexes Optimierungsproblem handelt. (CITE) Allerdings stellt sich heraus, dass das Problem konvex für festes $\mat A$ bzw. festes $\mat B$ ist. (Überprüfung!!!) Daher liegt es nahe einen alternierenden Ansatz zu wählen, um das Problem numerisch zu lösen. Wir betrachten im Folgenden also zwei Optimierungsprobleme.

$\mat B$ \textbf{gegeben} $\mat A$:

Wir wenden uns zunächst der Verlustfunktion zu. Hierfür sei $\mat A_{\perp} \in \mathbb{R}^{p \times (p-k)}$ eine orthonormale Matrix, so dass $[\mat A ; \mat A_{\perp}]$ $p \times p$ orthonormal ist. Dann gilt

\begin{align*}
\sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}_2^2 & = \norm{\mat X - \mat X \mat B \mat A^T}_F^{2}\\
& = \norm{\mat X \mat A_{\perp}}_F^2 + \norm{\mat X \mat A - \mat X \mat B}_F^2\\
& = \norm{\mat X \mat A_{\perp}}_F^2 + \sum_{j=1}^{k}\norm{\mat X \alpha_j - \mat X\beta_j}_2^2
\end{align*}

Wir setzen $Y_j^* = \mat X \alpha_j$ für alle $1 \leq j \leq n$. Somit reduziert sich (...) für fixes $\mat A$ auf das Lösen von $k$ elastic net Problemen
\begin{align}
\label{sub_problem_enet}
\hat{\beta}_j = \argmin_{\beta_j} \norm{Y_j^* - \mat X \beta_j}_2^2 + \lambda \norm{\beta_j}^2 + \lambda_{1,j}\norm{\beta_j}_1
\end{align}
In Kapitel \ref{fundamentals} haben wir uns bereits mit elastic nets beschäftigt und einen effizienten Algorithmus zur Lösung dieser präsentiert.

$\mat A$ \textbf{gegeben} $\mat B$:

Fixieren wir die Matrix $\mat B$, so können wir uns auf das Minimieren der Zielfunktion $\sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}_2^2 = \norm{\mat X - \mat X \mat B \mat A^T}_F^{2}$ beschränken, da die Bedingungen an $\beta_j$ nicht von Relevanz beim Optimieren über $\mat A$ sind. Somit ergibt sich 
\begin{align}
\label{sub_problem_procrustes}
\begin{split}
\hat{\mat A} = \argmin_{\mat A}\norm{\mat X - \mat X \mat B \mat A^T}_F^2\\
\text{unter der Nebenbedingung, dass } \mat A \mat A^T = \mat I_{k \times k}
\end{split}
\end{align}

Für dieses Optimierungsproblem lässt sich eine explizite Lösung angeben, da es eine Form von Procrustes Rotationsproblem ist, welches wir in Theorem REF beschrieben haben. Sei daher 
$$(\mat X^T \mat X) \mat B = \mat U \mat D \mat V^T$$ eine Singulärwertzerlegung. Dann ist $\hat{\mat A} = \mat U \mat V^T$.

Es ist sinnvoll zu erwähnen, dass für die Lösung beider Teilprobleme nur die Gram-Matrix $\mat X^T \mat X$ bekannt sein muss. Dies erleichtert die Berechnung.

\section{Algorithmus}

Durch die Vorarbeit im vorangegangenem Abschnitt können wir einen effizienten Algorithmus zur Lösung des Sparse PCA Kriteriums angeben. Zuerst initialisieren wir $\mat A$ mit den ersten $k$ Hauptachsen. Anschließend minimieren wir abwechselnd über $\mat B$ gegeben $\mat A$ und $\mat A$ gegeben $\mat B$ solange bis ein geeignetes Konvergenzkriterium erfüllt ist oder wir eine maximale Anzahl an Iterationen erreicht haben. Durch abschließende Normalisierung der Spalten von $\mat B$ erhalten wir die dünnbesetzten Hauptachsen. Eine Übersicht haben wir in Algorithmus \ref{spca_algorithm} erstellt.

\begin{algorithm}[tbh]
    \caption{Sparse Principal Component Analysis}
    \label{spca_algorithm}
    \begin{algorithmic}[1]
        \Procedure{SPCA}{$\mat A, \mat B$}
        	\State $\mat A \gets \mat V[,1 \colon k]$, die ersten $k$ Hauptachsen
            \While{nicht konvergiert}
                \State Gegeben festes $\mat A = [\alpha_1, \ldots, \alpha_k]$, löse das elastic net Problem
                $$\hat{\beta}_j = \argmin_{\beta_j} \norm{\mat X \alpha_j - \mat X \beta_j}^{2} + \lambda \norm{\beta_j}^2 + \lambda_{1,j}\norm{\beta_j}_{1} \quad \text{für } j = 1, \ldots, k$$
                \State Gegeben festes $\mat B = [\beta_1, \ldots, \beta_k]$, berechne die Singulärwerzerlegung von $$\mat X^T \mat X \mat B = \mat U \mat D \mat V^T$$
                $$\mat A \gets \mat U \mat V^T$$
            \EndWhile
            \State $\hat{V}_j \gets \frac{\beta_j}{\norm{\beta_j}}$ for $j = 1, \ldots, k$
        \EndProcedure
    \end{algorithmic}
\end{algorithm} 

Es stellt sich nun die Frage nach einem passendem Abbruchkriterium. Da für uns am Schluss des Algorithmus nur die dünnbesetzten Hauptachsen relevant sind, liegt es nahe ein Konvergenzkriterium für $\mat B$ zu wählen. Zou et al. brechen in ihrer Implementierung die Iteration ab, falls
$$\max_{\substack{1 \leq i \leq p \\ 1 \leq j \leq k}} \left| \frac{beta_{ij}^{(l+1)}}{\norm{\beta_i}} - \frac{beta_{ij}^{(l)}}{\norm{\beta_i}} \right| < \epsilon$$
wobei $\beta_{ij}^{(l)}$ der $j$-te Eintrag der dünnbesetzten Hauptachse $\beta_i$ in der $l$-ten Iteration ist. Sobald also die Änderung in $\mat B^{(l)}$ klein genug ist, kann die while-Schleife abgebrochen werden. Um die Laufzeit des Algorithmus zu beschränken, ist es sinnvoll ein zusätzliches Abbruchkriterium zu definieren. So werden wir bei Anwendung des Algorithmus eine maximale Anzahl an Iterationen $l_{max}$ festlegen, die nicht überschritten werden darf.

\section{Numerische Lösung im Fall p >> n}

\section{Laufzeitvergleich}

\section{Eigene Implementierung in Python}

Momentan existieren Implementierungen der dünnbesetzten Hauptkomponentenanalyse in R und Python. Zou et al. stellen das elasticnet package mit einer spca-Funktion, welche auf ihrem Ansatz beruht in der Programmiersprache R zur Verfügung. Für die Lösung des Subproblems (\ref{sub_problem_enet}) wird der LARS-EN Algorithmus gewählt, welche eine Erweiterung des LARS-Algorithmus für Elastic Nets ist \cite{zou_elasticnet}. Dagegen bietet scikit-learn eine SparsePCA-Variante in Python, welche auf Jennaton et al. in \cite{jennaton} zurückgeht und einen leicht anderen Ansatz verfolgt.

Um ein genaues Verständnis der Ergebnisse zu garantieren, haben wir uns dazu entschieden, eine eigene Implementierung in Python vorzunehmen, die auf dem Ansatz von Zou et al. beruht. Es wurde kritisch überprüft, dass der von uns implementierte Code dieselben Ergebnissen wie der spca-Algorithmus aus dem elasticnet package liefert. Dazu haben wir den Pitprops Datensatz aus \cite{zou_sparsepca}, welcher oft als Benchmark genutzt wird, und zusätzlich den eigenen Datensatz, welchen wir in Kapitel \ref{application} beschreiben, verwendet. Gegenüber der Implementierung im elasticnet package haben wir zwei entscheidende Änderungen vorgenommen, welche die Laufzeit in der Praxis deutlich verkürzen. Statt das Subproblem (\ref{sub_problem_enet}) mit LARS-EN zu lösen, wählen wir ein Koordinaten-Abstiegsverfahren.
Des Weiteren wird in der Implementierung von Zou et al. die Gram-Matrix $\mat X^T \mat X$ vorab berechnet, um für das Subproblem (\ref{sub_problem_procrustes}) nur eine Multiplikation pro Iteration $(\mat X^T \mat X) \mat B$ durchführen zu müssen. Da die Gram-Matrix aber in $\mathbb{R}^{p \times p}$ liegt, ist es für den Fall $p \gg n$ sinnvoller, $\mat X^T (\mat X \mat B)$ naiv zu berechnen, damit keine $p \times p$-Matrix zwischengespeichert werden muss. Dies ermöglicht für unseren hochdimensionalen Datensatz eine Laufzeit, die etwa um den Faktor $4$ besser ist.

Eine weitere kleine Änderung haben wir in der Aufrufstruktur der Methode vorgenommen, um die Notation mit der ElasticNet-Klasse in scikit-learn zu vereinheitlichen. Ähnlich wie in Abschnitt \ref{comparison_linear_models} haben wir eine Reparametrisierung vorgenommen
\begin{align}
 \alpha = \frac{2\lambda_2 + \lambda_1}{2n} \quad \text{und} \quad \gamma = \frac{\lambda_1}{2\lambda_2 + \lambda_1}
\end{align}
wobei $\alpha$ die Stärke der Bestrafung und $\gamma$ das Verhältnis der beiden Strafterme beschreibt.