% Main chapter title
\chapter{Implementierung}

% Chapter label
\label{implementation}

In diesem Kapitel werden wir einen Algorithmus beschreiben, der das Sparse PCA Kriterium (\ref{spca_criterion}) minimiert. Dabei sehen wir uns mit einem nicht-konvexem Optimierungsproblem konfrontiert. Allerdings können wir ausnutzen, dass (\ref{spca_criterion}) konvex bezüglich $\hat{\mat A}$ bzw. $\hat{\mat B}$ ist, falls wir eine der beiden Matrizen fixieren. Im Folgenden werden wir zunächst eine allgemeine numerische Lösung präsentieren und die Komplexität des assoziierten Algorithmus bestimmen. In einer HDLSS-Situation werden wir diesen in \ref{numerical_solution_p_greater_n} leicht abändern, um eine effiziente Berechnung zu garantieren. Zu Schluss dieses Kapitels diskutieren wir Details einer eigenen Implementierung in Python.

\section{Numerische Lösung}
\label{spca_numerical_solution}

Für eine einfache Übersicht werden wir das Sparse PCA Kriterium hier wiederholen.
\begin{align}
\label{spca_criterion_restatement}
\begin{split}
(\hat{\mat{A}}, \hat{\mat{B}}) = \argmin_{\mat{A}, \mat{B}} \sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}_2^2 + \lambda_2 \sum_{j=1}^{k}\norm{\beta_j}_2^2 + \sum_{j=1}^k \lambda_{1,j} \norm{\beta_j}_1\\
\text{unter der Nebenbedingung, dass } \mat{A}^T\mat{A} = I_{k \times k}.
\end{split}
\end{align}
Wie beschrieben handelt es sich bei (\ref{spca_criterion_restatement}) um ein bikonvexes Optimierungsproblem. Daher liegt es nahe einen alternierenden Ansatz zu wählen, um das Problem numerisch zu lösen. Somit fixieren wir im Folgenden eine der beiden Matrizen.

\subsubsection{$\mat B$ gegeben $\mat A$:}

Wir wenden uns zunächst der Verlustfunktion zu. Hierfür sei $\mat A_{\perp} \in \mathbb{R}^{p \times (p-k)}$ eine orthonormale Matrix, so dass $[\mat A ; \mat A_{\perp}]$ $p \times p$ orthonormal ist. Dann gilt

\begin{align*}
\sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^Tx_i}_2^2 & = \norm{\mat X - \mat X \mat B \mat A^T}_F^{2}\\
& = \norm{\mat X \mat A_{\perp}}_F^2 + \norm{\mat X \mat A - \mat X \mat B}_F^2\\
& = \norm{\mat X \mat A_{\perp}}_F^2 + \sum_{j=1}^{k}\norm{\mat X \alpha_j - \mat X\beta_j}_2^2.
\end{align*}
Somit reduziert sich (\ref{spca_criterion_restatement}) für fixes $\mat A$ auf das Lösen von $k$ Elastic Net Problemen
\begin{align}
\label{sub_problem_enet}
\hat{\beta}_j = \argmin_{\beta_j} \norm{Y_j^* - \mat X \beta_j}_2^2 + \lambda_2 \norm{\beta_j}_2^2 + \lambda_{1,j}\norm{\beta_j}_1,
\end{align}
wobei $Y_j^* = \mat X \alpha_j$ für alle $1 \leq j \leq k$. In Kapitel \ref{fundamentals} haben wir uns ausführlich mit Elastic Nets beschäftigt und ein effizientes Koordinatenabstiegsverfahren zur Lösung dieser präsentiert.

\subsubsection{$\mat A$ gegeben $\mat B$:}

Fixieren wir die Matrix $\mat B$, können wir uns auf das Minimieren der Verlustfunktion $\norm{\mat X - \mat X \mat B \mat A^T}_F^{2}$ beschränken, da die Bedingungen an $\beta_j$ nicht von Relevanz beim Optimieren über $\mat A$ sind. Somit reduziert sich (\ref{spca_criterion_restatement}) für fixes $\mat B$ auf das Lösen von
\begin{align}
\label{sub_problem_procrustes}
\begin{split}
\hat{\mat A} = \argmin_{\mat A}\norm{\mat X - \mat X \mat B \mat A^T}_F^2\\
\text{unter der Nebenbedingung, dass } \mat A \mat A^T = \mat I_{k \times k}.
\end{split}
\end{align}
Für dieses Optimierungsproblem lässt sich eine explizite Lösung angeben. Es ist eine Form von Procrustes Rotationsproblem, welches wir ebenfalls in Kapitel \ref{fundamentals} beschrieben haben. Die Matrix $\mat X \mat B$ soll durch Multiplikation mit einer orthogonalen Matrix $\mat A$ in $\mat X$ überführt werden. Berechnen wir eine Singulärwertzerlegung von $(\mat X^T \mat X) \mat B = \mat U \mat D \mat V^T,$ ist die Lösung für (\ref{sub_problem_procrustes}) gegeben durch
\begin{align}
\hat{\mat A} = \mat U \mat V^T.
\end{align}

\section{Algorithmus}

Durch die Vorarbeit im vorangegangenem Abschnitt können wir einen effizienten Algorithmus zur Lösung des Sparse PCA Kriteriums angeben. Zuerst initialisieren wir $\mat A$ mit den ersten $k$ Hauptachsen. Anschließend minimieren wir abwechselnd über die Matrizen $\mat A$ und $\mat B$ bis ein geeignetes Konvergenzkriterium erfüllt ist oder wir eine maximale Anzahl an Iterationen erreicht haben. Durch abschließende Normalisierung der Spalten von $\mat B$ erhalten wir die dünnbesetzten Hauptachsen. Eine Übersicht haben wir in Algorithmus \ref{spca_algorithm} erstellt.

\begin{algorithm}[tbh]
    \caption{Sparse Principal Component Analysis}
    \label{spca_algorithm}
    \begin{algorithmic}[1]
        \Procedure{SPCA}{$\mat A, \mat B, \lambda_2, \lambda_{1,j}$}
        	\State $\mat A \gets \mat V[,1 \colon k]$, die ersten $k$ Hauptachsen
            \While{nicht konvergiert}
                \State Gegeben festes $\mat A = [\alpha_1, \ldots, \alpha_k]$, löse das elastic net Problem
                $$\hat{\beta}_j = \argmin_{\beta_j} \norm{\mat X \alpha_j - \mat X \beta_j}^{2} + \lambda_2 \norm{\beta_j}^2 + \lambda_{1,j}\norm{\beta_j}_{1} \quad \text{für } j = 1, \ldots, k$$
                \State Gegeben festes $\mat B = [\beta_1, \ldots, \beta_k]$, berechne die Singulärwerzerlegung von $$\mat X^T \mat X \mat B = \mat U \mat D \mat V^T$$
                $$\mat A \gets \mat U \mat V^T$$
            \EndWhile
            \State $\hat{V}_j \gets \frac{\beta_j}{\norm{\beta_j}}$ for $j = 1, \ldots, k$
        \EndProcedure
    \end{algorithmic}
\end{algorithm} 

Es stellt sich nun die Frage nach einem passendem Abbruchkriterium. Da für uns am Schluss des Algorithmus nur die dünnbesetzten Hauptachsen relevant sind, liegt es nahe ein Konvergenzkriterium für $\mat B$ zu wählen. Zou et al. brechen die Iteration in ihrer Implementierung ab, falls
$$\max_{\substack{1 \leq i \leq p \\ 1 \leq j \leq k}} \left| \frac{\beta_{ij}^{(l+1)}}{\norm{\beta_i}} - \frac{\beta_{ij}^{(l)}}{\norm{\beta_i}} \right| < \epsilon ,$$
wobei $\beta_{ij}^{(l)}$ der $j$-te Eintrag der dünnbesetzten Hauptachse $\beta_i$ in der $l$-ten Iteration ist. Sobald also die Änderung in $\mat B$ klein genug ist, kann die while-Schleife beendet werden. Um die Laufzeit des Algorithmus zu beschränken, ist es sinnvoll ein zusätzliches Abbruchkriterium zu definieren. So werden wir bei Anwendung des Algorithmus eine maximale Anzahl an Iterationen $l_{max}$ festlegen, die nicht überschritten werden darf.



\section{Komplexität}
\label{complexity}

Wir werden uns nun mit der Komplexität von Algorithmus \ref{spca_algorithm} beschäftigen. Dabei unterscheiden wir zwischen den beiden Fällen $n > p$ und $p \gg n$, in welcher (\ref{sub_problem_enet}) auf eine andere Weise gelöst werden kann. Falls $p$ nur geringfügig größer ist als $n$ wenden wir weiterhin die beschriebene Methode an.\\

\textbf{Fall: } $\mathbf{n > p}$

Für die Berechnung von (\ref{sub_problem_enet}) lässt sich in ein Trick anwenden. Indem wir
\begin{align}
\label{sub_problem_enet_covariance}
\hat{\beta}_j & = \argmin_{\beta_j} \norm{Y_j^* - \mat X \beta_j}_2^2 + \lambda_2 \norm{\beta_j}_2^2 + \lambda_{1,j}\norm{\beta_j}_1 \nonumber\\
& = \argmin_{\beta_j} (\alpha_j - \beta_j)^T \mat X^T \mat X (\alpha_j - \beta_j) + \lambda_2 \norm{\beta_j}_2^2 + \lambda_{1,j}\norm{\beta_j}_1
\end{align}
umformen, sehen wir, dass sowohl (\ref{sub_problem_enet}) als auch (\ref{sub_problem_procrustes}) nur von der Kovarianzmatrix $\mat X^T \mat X$ abhängen. Daher ist es sinnvoll, diese vorab zu berechnen, um die Anzahl an Multiplikation je Iteration zu verringern. Ersetzen wir $\mat X^T\mat X$ durch die Kovarianzmatrix $\mat \Sigma$, erhalten wir eine Populationsversion der dünnbesetzten Hauptkomponentenanalyse. Auch wenn (\ref{sub_problem_enet_covariance}) mit $\mat \Sigma$ nicht direkt ein Elastic Net Problem ist, kann man dies mithilfe von $Y^{**} = \mat{\Sigma}^{\frac{1}{2}} \alpha_j$ und $\mat X^{**} = \mat{\Sigma}^{\frac{1}{2}}$ in ein Elastic Net Problem transformieren
$$\hat{\beta}_j = \argmin_{\beta} \norm{Y^{**} - \mat X^{**} \beta}_2^2 + \lambda_2 \norm{\beta}_2^2 + \lambda_{1,j}\norm{\beta}_1$$
Dadurch ergibt sich die folgende Laufzeit für die einzelnen Komponenten in Algorithmus \ref{spca_algorithm}.

\begin{table}
\centering
\begin{tabular}{ll}
Berechnung & Komplexität\\\hline\hspace{0.2cm}
$\mat X^T\mat X$ & $\mathcal{O}(np^2)$\\
$\mat X^T\mat X \mat B$ & $\mathcal{O}(p^2k)$\\
Singulärwertzerlegung von $\mat X^T\mat X \mat B$ & $\mathcal{O}(pk^2)$\\
Lösung des Elastic Net Problems (\ref{sub_problem_enet_covariance}) & $\mathcal{O}(p^3)$
\end{tabular}
\end{table}
Insgesamt ergibt sich aufgrund von $k \leq p$ eine Laufzeit von $(np^2) + m\mathcal{O}(p^3)$, wobei $m$ die Anzahl an Iterationen darstellt. Diese Schranke ist sehr gut im Hinblick auf eine hohe Beobachtungszahl, wird aber für hochdimensionale Daten (z.B. $p > 100$) ineffizient.\\

\textbf{Fall: } $\mathbf{p \gg n}$

Wir bezeichnen mit $J$ die Anzahl von Null verschiedener Einträge der Hauptachsen. Die meiste Zeit wird zur Lösung des Elastic Net Problems benötigt, welches in $\mathcal{O}(pnJ + J^3)$.

%----------------------------------------------------------------------------------------
%	Numerische Lösung im Fall p >> n
%----------------------------------------------------------------------------------------

\section{Numerische Lösung im Fall $p \gg n$}
\label{numerical_solution_p_greater_n}

Für viele Anwendungen kann die Anzahl an Variablen die Anzahl an Beobachtungen deutlich übersteigen. In diesem Fall ist sehr rechenintensiv.

Wir beobachten, dass (\ref{pca_regression_formulation_ridge}) für alle $\lambda_2$ gilt. Es stellt sich heraus, dass für $\lambda_2 \to \infty$ eine explizite Lösung des Minimierungsproblem existiert.
\begin{thm} \label{spca_p_greater_n}
Seien $\hat{\mat B}_j(\lambda_2) = \frac{\hat{\beta}_j}{\norm{\hat{\beta}_j}}$ die dünnbesetzten Hauptachsen aus (\ref{spca_criterion}).
und $(\tilde{\mat A}, \tilde{\mat B})$ die Lösung des Optimierungsproblems
\begin{align}
\label{spca_p_greater_n_problem}
\begin{split}
(\tilde{\mat A}, \tilde{\mat B}) = \argmin_{\mat A, \mat B} -2\spur{\mat A^T\mat X^T\mat X \mat B} + \sum_{j=1}^k \norm{\beta_j}_2^2 + \sum_{j=1}^k \lambda_{1,j}\norm{\beta_j}_1\\
\text{unter der Nebenbedingung, dass } \mat A^T \mat A = \mat I_{k \times k}.
\end{split}
\end{align}
Für $\lambda_2 \to \infty$ konvergieren die dünnbesetzten Hauptachsen $\hat{\mat B}_j(\lambda_2) \to \frac{\tilde{\beta}_j}{\norm{\tilde{\beta}_j}}.$
\end{thm}

Daher können wir das vereinfachte Optimierungsproblem (\ref{spca_p_greater_n_problem}) benutzen, um die Elastic Net Probleme für den Spezialfall $\lambda_2 = \infty$ zu lösen. Fixieren wir wie in Algorithmus \ref{spca_algorithm} die Matrix $\mat A$, verbleiben wir mit dem Problem
\begin{align}
\label{spca_p_greater_n_problem_fixed_A}
\hat{\beta}_j = \argmin_{\beta_j} -2 \alpha_j^T(\mat X^T\mat X)\beta_j + \norm{\beta_j}_{2}^{2} + \lambda_{1,j}\norm{\beta_j}_1.
\end{align}
Für (\ref{spca_p_greater_n_problem_fixed_A}) können wir eine explizite Lösung angeben
\begin{align}
\label{spca_p_greater_n_solution}
\hat{\beta}_j = \operatorname{soft}_{\frac{\lambda_{1,j}}{2}}(\alpha_j^T\mat X^T\mat X) = \left(|\alpha_j^T\mat X^T\mat X| - \frac{\lambda_{1,j}}{2}\right)_+ \operatorname{Sign}(\alpha_j^T\mat X^T\mat X)
\end{align}
wobei $\operatorname{soft}_{\delta}(x)$ der soft-thresholding Operator aus Abschnitt \ref{generalized_linear_models} ist. Daher ersetzen wir für den Fall $p \gg n$ die Berechnung in Schritt 4 des Algorithmus \ref{spca_algorithm} durch den Spezialfall (\ref{spca_p_greater_n_solution}). Für den hochdimensionalen Fall kann somit ein effizientes Verfahren gewährleistet werden. (Dann dauert jedes elastic net nur noch $\mathcal{O}(pn)$, wieso ist $\lambda_2 \to \infty$ sinnvoll? Und warum wählen wir den Weg nicht für große $p$?)

UST totally ignores the dependence between predictors and treats them as independent variables. Although this may be considered illegitimate, UST and its variants are used in other methods such as significance analysis of microarrays (Tusher et al., 2001) and the nearest shrunken
centroids classifier (Tibshirani et al., 2002), and have shown good empirical performance


%----------------------------------------------------------------------------------------
%	Eigene Implementierung in Python
%----------------------------------------------------------------------------------------


\section{Eigene Implementierung in Python}

Momentan existieren Implementierungen der dünnbesetzten Hauptkomponentenanalyse in R und Python. Zou et al. stellen das elasticnet package mit einer spca-Funktion, welche auf ihrem Ansatz beruht, in der Programmiersprache R zur Verfügung. Für die Lösung des Subproblems (\ref{sub_problem_enet}) wird der LARS-EN Algorithmus gewählt, welche eine Erweiterung des LARS-Algorithmus für Elastic Nets ist \cite{zou_elasticnet}. Dagegen bietet scikit-learn eine SparsePCA-Variante in Python, welche auf \cite{jenatton} zurückgeht und einen leicht anderen Ansatz verfolgt.

Um ein genaues Verständnis der Ergebnisse zu garantieren, haben wir uns dazu entschieden, eine eigene Implementierung in Python vorzunehmen, die auf dem Ansatz von Zou et al. beruht. Es wurde kritisch überprüft, dass der von uns implementierte Code dieselben Ergebnissen wie der spca-Algorithmus aus dem elasticnet package liefert. Dazu haben wir den Pitprops Datensatz aus \cite{zou_sparsepca}, welcher oft als Benchmark genutzt wird, und zusätzlich den eigenen Datensatz, welchen wir in Kapitel \ref{application} beschreiben, verwendet. Gegenüber der Implementierung im elasticnet package haben wir zwei entscheidende Änderungen vorgenommen, welche die Laufzeit in der Praxis verkürzen. Statt das Subproblem (\ref{sub_problem_enet}) mit LARS-EN zu lösen, wählen wir ein Koordinaten-Abstiegsverfahren, welches wir in Abschnitt \ref{elastic_net} beschrieben haben. Des Weiteren wird in der Implementierung von Zou et al. die Gram-Matrix $\mat X^T \mat X$ vorab berechnet, um für das Subproblem (\ref{sub_problem_procrustes}) nur eine Multiplikation pro Iteration $(\mat X^T \mat X) \mat B$ durchführen zu müssen. Da die Gram-Matrix aber in $\mathbb{R}^{p \times p}$ liegt, ist es für den Fall $p \gg n$ sinnvoller, $\mat X^T (\mat X \mat B)$ in jeder Iteration naiv zu berechnen, damit keine $p \times p$-Matrix zwischengespeichert werden muss. Dies ermöglicht für unseren hochdimensionalen Datensatz eine Laufzeit, die etwa um den Faktor $4$ besser ist.

Bezüglich der Aufrufstruktur der Methode haben wir eine Reparametrisierung vorgenommen, um die Notation mit der ElasticNet-Klasse in scikit-learn zu vereinheitlichen. Ähnlich wie in Abschnitt \ref{comparison_linear_models} definieren wir
\begin{align}
\lambda = \frac{2\lambda_2 + \lambda_1}{2n} \quad \text{und} \quad \alpha = \frac{\lambda_1}{2\lambda_2 + \lambda_1}
\end{align}
wobei $\alpha$ die Stärke der Bestrafung und $\gamma$ das Verhältnis des $\ell_1$ zum $\ell_2$-Strafterm beschreibt.