% Main chapter title
\chapter{Implementierung}

% Chapter label
\label{implementation}

In diesem Kapitel werden wir einen Algorithmus beschreiben, der das Sparse PCA Kriterium (\ref{spca_criterion}) minimiert. Dabei sehen wir uns mit einem nicht-konvexem Optimierungsproblem konfrontiert. Allerdings können wir ausnutzen, dass (\ref{spca_criterion}) konvex bezüglich der beiden Matrizen $\mat A$ und $\mat B$ ist, falls wir die jeweils andere fixieren. Im Folgenden werden wir zunächst eine allgemeine numerische Lösung präsentieren und die Komplexität des assoziierten Algorithmus bestimmen. In einer HDLSS-Situation werden wir diesen in \ref{numerical_solution_p_greater_n} leicht abändern, um eine effiziente Berechnung zu garantieren. Zum Schluss dieses Kapitels diskutieren wir Details einer eigenen Implementierung in Python.


%----------------------------------------------------------------------------------------
%	Numerische Lösung
%----------------------------------------------------------------------------------------


\section{Numerische Lösung}
\label{spca_numerical_solution}

Für eine einfache Übersicht werden wir das Sparse PCA Kriterium hier wiederholen.
\begin{align}
\label{spca_criterion_restatement}
\begin{gathered}
(\widehat{\mat{A}}, \widehat{\mat{B}}) = \argmin_{\mat{A}, \mat{B}} \sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^{\top}x_i}_2^2 + \lambda_2 \sum_{j=1}^{k}\norm{\beta_j}_2^2 + \sum_{j=1}^k \lambda_{1,j} \norm{\beta_j}_1\\
\text{unter der Nebenbedingung, dass } \mat{A}^{\top}\mat{A} = \mat{\mathbb{1}}_{k \times k}.
\end{gathered}
\end{align}
Wie im Einstieg erwähnt handelt es sich bei (\ref{spca_criterion_restatement}) um ein bikonvexes Optimierungsproblem. Daher liegt es nahe einen alternierenden Ansatz zu wählen, um das Problem numerisch zu lösen. Somit fixieren wir im Folgenden eine der beiden Matrizen.

\subsubsection{$\mat B$ gegeben $\mat A$:}

Wir wenden uns zunächst der Verlustfunktion zu. Hierfür sei $\mat A_{\perp} \in \mathbb{R}^{p \times (p-k)}$ eine orthonormale Matrix, so dass $[\mat A ; \mat A_{\perp}]$ $p \times p$ orthonormal ist. Dann gilt

\begin{align*}
\sum_{i=1}^{n} \norm{x_i - \mat{A}\mat{B}^{\top}x_i}_2^2 & = \norm{\mat X - \mat X \mat B \mat A^{\top}}_F^{2}\\
& = \norm{\mat X \mat A_{\perp}}_F^2 + \norm{\mat X \mat A - \mat X \mat B}_F^2\\
& = \norm{\mat X \mat A_{\perp}}_F^2 + \sum_{j=1}^{k}\norm{\mat X \alpha_j - \mat X\beta_j}_2^2.
\end{align*}
Somit reduziert sich (\ref{spca_criterion_restatement}) für fixes $\mat A$ auf das Lösen von $k$ Elastic Net Problemen
\begin{align}
\label{sub_problem_enet}
\widehat{\beta}_j = \argmin_{\beta_j} \norm{Y_j^* - \mat X \beta_j}_2^2 + \lambda_2 \norm{\beta_j}_2^2 + \lambda_{1,j}\norm{\beta_j}_1,
\end{align}
wobei $Y_j^* = \mat X \alpha_j$ für alle $1 \leq j \leq k$. In Abschnitt \ref{elastic_net} haben wir uns ausführlich mit Elastic Nets beschäftigt und ein effizientes Koordinaten-Abstiegsverfahren zur Lösung dieser präsentiert.

\subsubsection{$\mat A$ gegeben $\mat B$:}

Fixieren wir die Matrix $\mat B$, können wir uns auf das Minimieren von $\norm{\mat X - \mat X \mat B \mat A^{\top}}_F^{2}$ beschränken, da die Bedingungen an $\beta_j$ nicht von Relevanz beim Optimieren über $\mat A$ sind. Somit reduziert sich (\ref{spca_criterion_restatement}) für fixes $\mat B$ auf das Lösen von
\begin{align}
\label{sub_problem_procrustes}
\begin{gathered}
\widehat{\mat A} = \argmin_{\mat A}\norm{\mat X - \mat X \mat B \mat A^{\top}}_F^2\\
\text{unter der Nebenbedingung, dass } \mat A^{\top} \mat A = \mat{\mathbb{1}}_{k \times k}.
\end{gathered}
\end{align}
Für dieses Optimierungsproblem lässt sich eine explizite Lösung angeben. Es ist eine Form von Procrustes Rotationsproblem, welches wir in Abschnitt \ref{matrix_approximation} beschrieben haben. Die Matrix $\mat X \mat B$ soll durch Multiplikation mit einer orthogonalen Matrix $\mat A$ in $\mat X$ überführt werden. Berechnen wir eine Singulärwertzerlegung von $(\mat X^{\top} \mat X) \mat B = \mat U \mat D \mat V^{\top},$ ist die Lösung von (\ref{sub_problem_procrustes}) gegeben durch
\begin{align}
\widehat{\mat A} = \mat U \mat V^{\top}.
\end{align}


%----------------------------------------------------------------------------------------
%	Algorithmus
%----------------------------------------------------------------------------------------


\section{Algorithmus}

Durch die Vorarbeit im vorangegangenem Abschnitt können wir einen effizienten Algorithmus zur Lösung des Sparse PCA Kriteriums angeben. Zuerst initialisieren wir $\mat A$ mit den ersten $k$ gewöhnlichen Hauptachsen. Anschließend minimieren wir abwechselnd über die Matrizen $\mat A$ und $\mat B$ bis ein geeignetes Konvergenzkriterium erfüllt ist oder wir eine maximale Anzahl an Iterationen erreicht haben. Durch abschließende Normalisierung der Spalten von $\mat B$ erhalten wir die dünnbesetzten Hauptachsen. Eine Übersicht haben wir in Algorithmus \ref{spca_algorithm} erstellt.

\begin{algorithm}[tbh]
    \caption{Sparse Principal Component Analysis}
    \label{spca_algorithm}
    \begin{algorithmic}[1]
        \Procedure{SPCA}{$\mat A, \mat B, k, \lambda_2, \lambda_{1,j}$}
        	\State $\mat A \gets \mat V[,1 \colon k]$, die ersten $k$ Hauptachsen
            \While{nicht konvergiert}
                \State Gegeben festes $\mat A = [\alpha_1, \ldots, \alpha_k]$, löse das elastic net Problem
                $$\widehat{\beta}_j = \argmin_{\beta_j} \norm{\mat X \alpha_j - \mat X \beta_j}^{2} + \lambda_2 \norm{\beta_j}^2 + \lambda_{1,j}\norm{\beta_j}_{1} \quad \text{für } j = 1, \ldots, k$$
                \State Gegeben festes $\mat B = [\beta_1, \ldots, \beta_k]$, berechne die Singulärwerzerlegung von $$\mat X^{\top} \mat X \mat B = \mat U \mat D \mat V^{\top}$$
                $$\mat A \gets \mat U \mat V^{\top}$$
            \EndWhile
            \State $\widehat{V}_j \gets \frac{\beta_j}{\norm{\beta_j}}$ for $j = 1, \ldots, k$
        \EndProcedure
    \end{algorithmic}
\end{algorithm} 

Es stellt sich nun die Frage nach einem passendem Abbruchkriterium. Da für uns am Schluss des Algorithmus nur die dünnbesetzten Hauptachsen relevant sind, liegt es nahe ein Konvergenzkriterium für $\mat B$ zu wählen. Zou et al. brechen die Iteration in ihrer Implementierung ab, falls
$$\max_{\substack{1 \leq i \leq p \\ 1 \leq j \leq k}} \left| \frac{\beta_{ij}^{(l+1)}}{\norm{\beta_i}} - \frac{\beta_{ij}^{(l)}}{\norm{\beta_i}} \right| < \epsilon ,$$
wobei $\beta_{ij}^{(l)}$ der $j$-te Eintrag der dünnbesetzten Hauptachse $\beta_i$ in der $l$-ten Iteration ist. Sobald also die Änderung in $\mat B$ klein genug ist, kann die while-Schleife beendet werden. Um die Laufzeit des Algorithmus zu beschränken, ist es sinnvoll ein zusätzliches Abbruchkriterium zu definieren. So werden wir bei Anwendung des Algorithmus eine maximale Anzahl an Iterationen $l_{max}$ festlegen, die nicht überschritten werden darf.


%----------------------------------------------------------------------------------------
%	Komplexität
%----------------------------------------------------------------------------------------


\section{Komplexität}
\label{complexity}

Wir werden uns nun mit der Komplexität von Algorithmus \ref{spca_algorithm} beschäftigen. Dabei werden wir wieder einmal zwischen den Fällen $n > p$ und $p \gg n$ unterscheiden.

\subsubsection{Fall: $\mathbf{n > p}$}

In diesem Fall lässt sich ein Trick für die Berechnung von (\ref{sub_problem_enet}) anwenden. Indem wir
\begin{align}
\label{sub_problem_enet_covariance}
\widehat{\beta}_j & = \argmin_{\beta_j} \norm{Y_j^* - \mat X \beta_j}_2^2 + \lambda_2 \norm{\beta_j}_2^2 + \lambda_{1,j}\norm{\beta_j}_1 \nonumber\\
& = \argmin_{\beta_j} (\alpha_j - \beta_j)^{\top} \mat X^{\top} \mat X (\alpha_j - \beta_j) + \lambda_2 \norm{\beta_j}_2^2 + \lambda_{1,j}\norm{\beta_j}_1
\end{align}
umformen, hängen beide Subprobleme (\ref{sub_problem_enet}) und (\ref{sub_problem_procrustes}) nur von der Kovarianzmatrix $\mat X^{\top} \mat X$ ab. Daher ist es sinnvoll, diese vorab zu berechnen, um die Anzahl an Multiplikationen je Iteration zu verringern. Allerdings ist (\ref{sub_problem_enet_covariance}) mit $\mat X^{\top} \mat X$ nicht direkt ein Elastic Net Problem. Definieren wir $Y^{**} = (\mat{X}^{\top}\mat X)^{\frac{1}{2}} \alpha_j$ und $\mat X^{**} = (\mat{X}^{\top}\mat X)^{\frac{1}{2}}$ können wir (\ref{sub_problem_enet_covariance}) aber in ein Elastic Net Problem transformieren
$$\widehat{\beta}_j = \argmin_{\beta} \norm{Y^{**} - \mat X^{**} \beta}_2^2 + \lambda_2 \norm{\beta}_2^2 + \lambda_{1,j}\norm{\beta}_1.$$
Um die Laufzeit des Algorithmus zu bestimmen, wenden wir uns Tabelle \ref{complexity_calculation} zu, welche den Rechenaufwand für die verschiedenen Berechnungsschritte zeigt. Die Initialisierung von $\mat A$ durch die ersten $k$ gewöhnlichen Hauptachsen wird durch eine abgeschnittene Singulärwertzerlegung von $\mat X$ berechnet. Zudem berechnen wir vorab die Kovarianzmatrix $\mat X^{\top} \mat X$. Pro Iteration lösen wir $k$ Elastic Net Probleme und ein Procrustes Rotationsproblem. Insgesamt ergibt sich daher aufgrund $k \leq \min \{n,p\}$ und $j \leq p$ eine Laufzeit von $np^2 + mk\cdot\mathcal{O}(p^3)$, wobei $m$ die Anzahl an Iterationen und $j$ die Anzahl von Null verschiedener Einträge ist.

\begin{table}
\centering
\begin{tabular}{ll}
Berechnung & Komplexität\\\hline\addlinespace
Singulärwertzerlegung von $\mat X$ & $\mathcal{O}(npk)$\\
$\mat X^{\top}\mat X$ & $\mathcal{O}(np^2)$\\
$(\mat X^{\top}\mat X) \mat B$ & $\mathcal{O}(p^2k)$\\
$\mat X^{\top} (\mat X \mat B)$ & $\mathcal{O}(npk)$\\
Singulärwertzerlegung von $\mat X^{\top}\mat X \mat B$ & $\mathcal{O}(pk^2)$\\
Elastic Net Problem & $\mathcal{O}(pnj + j^3)$
\end{tabular}
\caption{Die Tabelle zeigt die Komplexität für die in Algorithmus \ref{spca_algorithm} vorkommenden Berechnungsschritte. Dabei ist $j$ die Anzahl von Null verschiedener Koeffizienten.}
\label{complexity_calculation}
\end{table}

\subsubsection{Fall: $\mathbf{p \gg n}$}

Für diesen Fall ist eine Berechnung der Kovarianzmatrix $\mat X^{\top}\mat X$ nicht mehr sinnvoll, da dies eine $p \times p$-Matrix ist. Daher werden wir $\mat X^{\top}(\mat X \mat B)$ in jeder Iteration naiv berechnen. Die Laufzeit des Algorithmus wird in diesem Fall von der Lösung der $k$ Elastic Net Probleme dominiert, besonders wenn wir viele von Null verschiedene Einträge zulassen. Dies schlägt sich auch in der Komplexität des Algorithmus nieder, welche in diesem Fall durch $mk\cdot\mathcal{O}(pnj + j^3)$ gegeben ist. Für große $j$ bzw. $p$ sind die Berechnungskosten somit sehr hoch, weshalb wir im folgenden Abschnitt eine spezielle Form der dünnbesetzten Hauptkomponentenanalyse kennenlernen werden.


%----------------------------------------------------------------------------------------
%	Numerische Lösung im Fall p >> n
%----------------------------------------------------------------------------------------

\section{Numerische Lösung in hochdimensionalen Fällen}
\label{numerical_solution_p_greater_n}

Für viele Anwendungen kann die Menge an Variablen die Anzahl an Beobachtungen deutlich übersteigen. Um auch in diesem Fall eine effiziente Berechnung zu ermöglichen, formulieren wir einen Spezialfall von Algorithmus \ref{spca_algorithm}. Dazu beobachten wir, dass Theorem \ref{pca_regression_formulation_ridge} für alle $\lambda_2 > 0$ gilt. Für $\lambda_2 \to \infty$ erhalten wir eine interessante Charakterisierung.
\begin{thm} \label{spca_p_greater_n}
Seien $\frac{\widehat{\beta}_j(\lambda_2)}{\norm{\widehat{\beta}_j(\lambda_2)}}$ die dünnbesetzten Hauptachsen aus (\ref{spca_criterion}) in Abhängigkeit von $\lambda_2$
und $(\tilde{\mat A}, \tilde{\mat B})$ die Lösung des Optimierungsproblems
\begin{align}
\label{spca_p_greater_n_problem}
\begin{gathered}
(\tilde{\mat A}, \tilde{\mat B}) = \argmin_{\mat A, \mat B} -2\spur{\mat A^{\top}\mat X^{\top}\mat X \mat B} + \sum_{j=1}^k \norm{\beta_j}_2^2 + \sum_{j=1}^k \lambda_{1,j}\norm{\beta_j}_1\\
\text{unter der Nebenbedingung, dass } \mat A^{\top} \mat A = \mat{\mathbb{1}}_{k \times k}.
\end{gathered}
\end{align}
Für $\lambda_2 \to \infty$ konvergieren die dünnbesetzten Hauptachsen $\frac{\widehat{\beta}_j(\lambda_2)}{\norm{\widehat{\beta}_j(\lambda_2)}} \to \frac{\tilde{\beta}_j}{\norm{\tilde{\beta}_j}}$ gegen die Lösung von (\ref{spca_p_greater_n_problem}). 
\end{thm}
Daher können wir das vereinfachte Optimierungsproblem (\ref{spca_p_greater_n_problem}) benutzen, um das Sparse PCA Kriterium für den Fall $\lambda_2 = \infty$ zu lösen. Fixieren wir wie in Algorithmus \ref{spca_algorithm} die Matrix $\mat A$, verbleiben wir mit dem Problem
\begin{align}
\label{spca_p_greater_n_problem_fixed_A}
\widehat{\beta}_j = \argmin_{\beta_j} -2 \alpha_j^{\top}(\mat X^{\top}\mat X)\beta_j + \norm{\beta_j}_{2}^{2} + \lambda_{1,j}\norm{\beta_j}_1.
\end{align}
Für (\ref{spca_p_greater_n_problem_fixed_A}) existiert aufgrund des Wegfalls von $\lambda_2$ eine explizite Lösung
\begin{align}
\label{spca_p_greater_n_solution}
\widehat{\beta}_j = \operatorname{soft}_{\frac{\lambda_{1,j}}{2}}(\alpha_j^{\top}\mat X^{\top}\mat X) = \left(|\alpha_j^{\top}\mat X^{\top}\mat X| - \frac{\lambda_{1,j}}{2}\right)_+ \operatorname{Sign}(\alpha_j^{\top}\mat X^{\top}\mat X).
\end{align}
Somit können wir die Berechnung in Schritt 4 von Algorithmus \ref{spca_algorithm} durch (\ref{spca_p_greater_n_solution}) ersetzen. Folglich müssen keine Elastic Net Probleme berechnet werden, weshalb wir ein effizientes Verfahren im hochdimensionalen Fall gewährleisten können. Diese Modifikation werden wir im Folgenden mit Algorithmus \ref{spca_algorithm}$^*$ bezeichnen.

Bei einer derartigen Ersetzung ist aber besondere Vorsicht zu genießen. Wir möchten kurz erklären, was die Modifikation in Anwendung bewirkt. Mit (\ref{spca_p_greater_n_solution}) wird im Wesentlichen der soft-thresholding-Operator auf die $j$-te Hauptachse angewendet. Alle Einträge in der Hauptachse, welche im Betrag kleiner als $\frac{\lambda_{1,j}}{2}$ sind, werden dadurch auf Null gesetzt. Mit $\lambda_{1,j}$ kontrollieren wir also weiterhin den Grad der Dünnbesetzung. Allerdings werden Abhängigkeiten zwischen Variablen dabei völlig außer Acht gelassen und die Variablen als unabhängig angenommen. Auch wenn dies zunächst unzulässig scheinen mag, findet man ein derartiges Vorgehen auch in anderen Verfahren wieder \cite{tibshirani_diagnosis}. Eine Rechtfertigung dieses Ansatzes beruht vor allem auf guten empirischen Ergebnissen. Daher muss im Anwendungsfall entschieden werden, ob eine Verwendung von Algorithmus \ref{spca_algorithm}$^*$ sinnvoll ist.


%----------------------------------------------------------------------------------------
%	Eigene Implementierung in Python
%----------------------------------------------------------------------------------------


\section{Eigene Implementierung in Python}

Momentan existieren Implementierungen der dünnbesetzten Hauptkomponentenanalyse in R und Python. Zou et al. stellen das \texttt{elasticnet} package mit einer \texttt{spca}-Funktion, welche auf ihrem Ansatz beruht, in der Programmiersprache R zur Verfügung. Für die Lösung des Subproblems (\ref{sub_problem_enet}) wird der \texttt{LARS-EN} Algorithmus gewählt, welche eine Erweiterung des \texttt{LARS}-Algorithmus für das Elastic Net ist \cite{zou_elasticnet}. Dagegen bietet scikit-learn eine\texttt{ SparsePCA}-Variante in Python, welche auf \cite{jenatton} zurückgeht und einen anderen Ansatz verfolgt.

Um ein genaues Verständnis der Ergebnisse zu garantieren, haben wir uns dazu entschieden, eine eigene Implementierung in Python vorzunehmen, die auf dem Ansatz von Zou et al. beruht. Es wurde kritisch überprüft, dass der von uns implementierte Code korrekte Ergebnisse erzielt. Dazu haben  wir den Pitprops Datensatz aus \cite{zou_sparsepca}, welcher oft als Benchmark genutzt wird, und zusätzlich den eigenen Datensatz, welchen wir in Kapitel \ref{application} beschreiben, verwendet. Gegenüber der Implementierung im elasticnet package haben wir zwei entscheidende Änderungen vorgenommen, welche die Laufzeit in der Praxis verkürzen. Statt das Subproblem (\ref{sub_problem_enet}) mit LARS-EN zu lösen, wählen wir ein randomisiertes Koordinaten-Abstiegsverfahren, welches wir in Abschnitt \ref{elastic_net} beschrieben haben. Des Weiteren wird in der Implementierung von Zou et al. die Gram-Matrix $\mat X^{\top} \mat X$ immer vorab berechnet, um für das Subproblem (\ref{sub_problem_procrustes}) nur eine Multiplikation pro Iteration $(\mat X^{\top} \mat X) \mat B$ durchführen zu müssen. Da die Gram-Matrix aber in $\mathbb{R}^{p \times p}$ liegt, ist es für den Fall $p \gg n$ sinnvoller, $\mat X^{\top} (\mat X \mat B)$ in jeder Iteration naiv zu berechnen, damit keine $p \times p$-Matrix zwischengespeichert werden muss. Dies ermöglicht für unseren hochdimensionalen Datensatz eine Laufzeit, die etwa um den Faktor $4$ besser ist.

Bezüglich der Aufrufstruktur der Methode haben wir eine Reparametrisierung vorgenommen, um die Notation mit der ElasticNet-Klasse in scikit-learn zu vereinheitlichen. Ähnlich wie in Abschnitt \ref{comparison_linear_models} definieren wir
\begin{align}
\lambda = \frac{2\lambda_2 + \lambda_1}{2n} \quad \text{und} \quad \alpha = \frac{\lambda_1}{2\lambda_2 + \lambda_1}
\end{align}
wobei $\lambda$ die Stärke der Bestrafung und $\alpha$ das Verhältnis des $\ell_1$ zum $\ell_2$-Strafterm beschreibt.